# AnyLoom Docker Environment Configuration
# Copy this file to .env and customize as needed

# ============================================================================
# LLM Configuration (llama.cpp server)
# ============================================================================
LLM_PORT=8008
LLM_CONTEXT_SIZE=131072
LLM_MODEL_DIR=./models
# LLM_IMAGE=local/llama.cpp:server-cuda-blackwell  # Build: bash scripts/build_llm_image.sh
# LLM_IMAGE=ghcr.io/ggml-org/llama.cpp:server-cuda  # Fallback (no Blackwell, ~10x slower on RTX 5090)

# GPU requirements:
# Qwen3-30B-A3B Q4_K_M weights are ~18.6 GiB.
# With K:Q8_0/V:Q4_0 KV cache, 131K context uses ~5 GiB.
# Total VRAM: ~24.6 GiB â€” fits comfortably on 32GB GPUs.
# For 24GB GPUs, reduce LLM_CONTEXT_SIZE to 65536.

# ============================================================================
# Embedding Configuration (llama.cpp server, GPU)
# ============================================================================
EMBEDDING_PORT=8009
# BGE-M3 Q8_0 GGUF (~635 MB, runs on GPU)
# Model file is set in docker-compose.yml command

# ============================================================================
# Qdrant Configuration
# ============================================================================
QDRANT_PORT=6333
QDRANT_GRPC_PORT=6334

# ============================================================================
# AnythingLLM Configuration
# ============================================================================
ANYTHINGLLM_PORT=3001
ANYTHINGLLM_UID=1000
ANYTHINGLLM_GID=1000
# ANYTHINGLLM_API_KEY=your_key_here    # Generate at http://localhost:3001/settings/developer
# ANYTHINGLLM_WORKSPACE=anyloom        # Workspace slug (created by configure_anythingllm.py)

# ============================================================================
# GPU Configuration
# ============================================================================
CUDA_VISIBLE_DEVICES=0

# ============================================================================
# HuggingFace (Optional)
# ============================================================================
# HF_TOKEN=your_token_here  # Only needed for gated models
