# AnyLoom Docker Environment Configuration
# Laptop Profile: RTX 2070 Max-Q (8GB VRAM), i7-10750H, 32GB RAM
# Copy this file to .env and customize as needed

# ============================================================================
# LLM Configuration (llama.cpp server)
# ============================================================================
LLM_PORT=8008
LLM_CONTEXT_SIZE=8192
LLM_MODEL_DIR=./models
LLM_IMAGE=ghcr.io/ggml-org/llama.cpp:server-cuda

# GPU requirements (RTX 2070 Max-Q, 8GB VRAM):
# Qwen2.5-Coder-7B Q4_K_M weights are ~4.5 GiB.
# With K:Q4_0/V:Q4_0 KV cache, 8K context uses ~0.3 GiB.
# Total VRAM: ~5.1 GiB — fits comfortably on 8GB GPUs.

# ============================================================================
# Embedding Configuration (llama.cpp server, CPU)
# ============================================================================
EMBEDDING_PORT=8009
# BGE-M3 Q8_0 GGUF (~635 MB, runs on CPU — GPU reserved for LLM)

# ============================================================================
# Qdrant Configuration
# ============================================================================
QDRANT_PORT=6333
QDRANT_GRPC_PORT=6334

# ============================================================================
# AnythingLLM Configuration
# ============================================================================
ANYTHINGLLM_PORT=3001
ANYTHINGLLM_UID=1000
ANYTHINGLLM_GID=1000
# ANYTHINGLLM_API_KEY=your_key_here    # Generate at http://localhost:3001/settings/developer
# ANYTHINGLLM_WORKSPACE=anyloom        # Workspace slug (created by configure_anythingllm.py)

# ============================================================================
# GPU Configuration
# ============================================================================
CUDA_VISIBLE_DEVICES=0

# ============================================================================
# HuggingFace (Optional)
# ============================================================================
# HF_TOKEN=your_token_here  # Only needed for gated models
