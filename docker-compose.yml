services:
  # ============================================================================
  # Qdrant Vector Database
  # ============================================================================
  qdrant:
    image: qdrant/qdrant:latest
    container_name: anyloom-qdrant
    restart: unless-stopped
    ports:
      - "${QDRANT_PORT:-6333}:6333"
      - "${QDRANT_GRPC_PORT:-6334}:6334"  # gRPC port
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    deploy:
      resources:
        limits:
          memory: 2g
    networks:
      - anyloom
    healthcheck:
      test: ["CMD-SHELL", "bash -c 'echo > /dev/tcp/localhost/6333'"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s

  # ============================================================================
  # LLM Inference Engine — llama.cpp server (GPU Required)
  # Laptop profile: Qwen2.5-Coder-7B Q4_K_M (~4.5 GB VRAM)
  # Single slot, 16K context, Q8_0 KV cache
  # ============================================================================
  llm:
    image: ${LLM_IMAGE:-ghcr.io/ggml-org/llama.cpp:server-cuda}
    container_name: anyloom-llm
    restart: "no"  # Use "on-failure" when stable
    ports:
      - "${LLM_PORT:-8008}:8080"
    volumes:
      - ${LLM_MODEL_DIR:-./models}:/models:ro
    environment:
      - GGML_CUDA_GRAPH_OPT=0  # Turing (sm_75) has limited CUDA graph support
    command: >
      --model /models/qwen2.5-coder-7b-instruct-q4_k_m.gguf
      --alias gpt-4
      --ctx-size ${LLM_CONTEXT_SIZE:-8192}
      --n-gpu-layers 99
      --flash-attn on
      --cache-type-k q8_0
      --cache-type-v q8_0
      --batch-size 2048
      --ubatch-size 512
      --threads 4
      --threads-batch 6
      --jinja
      --slots
      -np 1
      --host 0.0.0.0
      --port 8080
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 8g
    networks:
      - anyloom
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/health || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 60s

  # ============================================================================
  # Embedding Engine — llama.cpp server (CPU-only, BGE-M3 for AnythingLLM)
  # Laptop profile: moved to CPU to free GPU VRAM for LLM.
  # BGE-M3 Q8_0 produces 1024-dim dense vectors, same as desktop profile.
  # ============================================================================
  embedding:
    image: ghcr.io/ggml-org/llama.cpp:server  # CPU-only image
    container_name: anyloom-embedding
    restart: unless-stopped
    ports:
      - "${EMBEDDING_PORT:-8009}:8080"
    volumes:
      - ${LLM_MODEL_DIR:-./models}:/models:ro
    command: >
      --model /models/bge-m3-q8_0.gguf
      --embeddings
      --ctx-size 8192
      --batch-size 4096
      --ubatch-size 4096
      --n-gpu-layers 0
      --parallel 1
      --threads 4
      --host 0.0.0.0
      --port 8080
    deploy:
      resources:
        limits:
          memory: 4g
    networks:
      - anyloom
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/health || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ============================================================================
  # AnythingLLM UI (Optional)
  # ============================================================================
  anythingllm:
    image: mintplexlabs/anythingllm:latest
    container_name: anyloom-anythingllm
    restart: unless-stopped
    shm_size: "512m"  # Chrome uses /dev/shm for IPC; reduced from 1g for laptop
    init: true      # Reaps zombie Chrome processes (Node.js as PID 1 won't)
    ports:
      - "${ANYTHINGLLM_PORT:-3001}:3001"
    volumes:
      - anythingllm_storage:/app/server/storage
      - anythingllm_hotdir:/app/collector/hotdir
    environment:
      - STORAGE_DIR=/app/server/storage
      - UID=${ANYTHINGLLM_UID:-1000}
      - GID=${ANYTHINGLLM_GID:-1000}
      - NODE_OPTIONS=--dns-result-order=ipv4first  # Qdrant binds IPv4 only
      - ANYTHINGLLM_CHROMIUM_ARGS=--no-sandbox,--disable-setuid-sandbox,--disable-dev-shm-usage,--disable-gpu,--no-zygote,--disable-software-rasterizer
      - MCP_NO_COOLDOWN=1  # Allow agent to call the same MCP tool multiple times with different args
    networks:
      - anyloom
    depends_on:
      qdrant:
        condition: service_healthy
      llm:
        condition: service_started  # Don't block UI on LLM model load
      embedding:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:3001/api/v1/system || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

# ============================================================================
# Networks
# ============================================================================
networks:
  anyloom:
    name: anyloom
    driver: bridge

# ============================================================================
# Volumes (external — pre-existing from initial setup)
# ============================================================================
volumes:
  qdrant_storage:
    name: anyloom_qdrant_storage
    external: true
  anythingllm_storage:
    name: anyloom_anythingllm_storage
    external: true
  anythingllm_hotdir:
    name: anyloom_anythingllm_hotdir
    external: true
