[
  {
    "id": "E1",
    "query": "How do the two RAG pipelines differ?",
    "mode": "query",
    "response": "The two RAG pipelines differ fundamentally in their retrieval mechanism, architecture, and operational model:\n\n1.  **Retrieval Mechanism:**\n    *   **AnythingLLM (port 6333):** Uses **passive, dense-only retrieval**. Document chunks are automatically injected into the system message on every relevant query via cosine similarity search against BGE-M3 Q8_0 GGUF embeddings. This happens without any explicit tool call from the agent.\n    *   **LM Studio (port 6334):** Uses **active, hybrid retrieval**. The agent must explicitly call the `rag_search` tool to retrieve documents. This search combines dense semantic matching (BGE-M3) with sparse lexical matching (keyword weights) using Reciprocal Rank Fusion (RRF) for a more comprehensive result.\n\n2.  **Architecture and Data:**\n    *   **AnythingLLM:** Operates on a dedicated Qdrant instance (port 6333) with independent data volumes and collection configuration. It uses 6,600-character chunks with 1,000-character overlap, processed by a recursive text splitter.\n    *   **LM Studio:** Operates on a separate Qdrant instance (port 6334) with its own data volumes and collection (`lmstudio_docs`). It uses deterministic section-header-based chunking with zero overlap, which allows for precise source filtering.\n\n3.  **Operational Model:**\n    *   **AnythingLLM:** The retrieval is automatic and requires no agent action. The agent's response is built from the pre-injected context.\n    *   **LM Studio:** The retrieval is a deliberate, explicit tool call. The agent must initiate the search, specify parameters like result limits or source filters, and wait for the results before proceeding.\n\nThis asymmetry means the same information often exists in different formats: the LM Studio pipeline tends to produce terser results that catch exact identifiers via sparse matching, while the AnythingLLM pipeline provides richer, more contextually relevant prose based on semantic similarity.",
    "word_count": 263,
    "grade": "FAIL",
    "reason": "Over word limit: 263 words (max 150)",
    "has_headers": false,
    "has_bullets": true
  },
  {
    "id": "E2",
    "query": "How does DyTopo route messages between agents?",
    "mode": "query",
    "response": "DyTopo routes messages between agents using a dynamic, semantically-routed graph based on the MiniLM-L6-v2 sentence embedding model. Here's how it works:\n\n1.  **Descriptor Embedding:** At the start of each round (after round 1, which uses broadcast), each agent's output (or \"descriptor\") is embedded into a 384-dimensional vector using the MiniLM-L6-v2 model. This model runs on CPU and consumes approximately 80 MB of RAM.\n\n2.  **Graph Construction:** The system constructs a directed acyclic graph (DAG) by calculating the cosine similarity between all pairs of agent descriptors. A connection (edge) is created from one agent to another only if the similarity score exceeds a threshold determined by the `tau` parameter.\n\n3.  **Routing Threshold (`tau`):**\n    *   **Lower `tau` (e.g., 0.1-0.2):** Creates a denser graph with more connections. More agents receive messages from each other, leading to richer collaboration but higher token cost.\n    *   **Higher `tau` (e.g., 0.4-0.6):** Creates a sparser graph with fewer connections. Agents work more independently, leading to faster convergence but potentially missing useful cross-agent insights.\n\n4.  **Execution Order:** The graph is processed in topological order. Agents are executed only after all their incoming messages have been received, ensuring that each agent's work is based on the most relevant, semantically connected inputs.\n\nThis system replaces the simple broadcast mode of round 1 with a more efficient, context-aware routing mechanism that ensures agents only communicate with those whose work is most relevant to their current task.",
    "word_count": 234,
    "grade": "FAIL",
    "reason": "Over word limit: 234 words (max 150)",
    "has_headers": false,
    "has_bullets": true
  },
  {
    "id": "E3",
    "query": "What is the difference between chat mode and query mode in AnythingLLM?",
    "mode": "query",
    "response": "In AnythingLLM, **chat mode** and **query mode** are two distinct interaction paradigms with different capabilities and access levels:\n\n*   **Chat Mode:**\n    *   **Purpose:** Designed for natural, conversational interaction with the AI agent.\n    *   **Tool Access:** **No tool access.** The agent can only respond using the information available in the workspace's RAG context (the \"Context:\" section) and its internal training knowledge.\n    *   **Functionality:** The agent can answer questions, provide explanations, summarize documents, and engage in dialogue, but it cannot perform actions like checking system status, reading files, or accessing live data.\n    *   **Use Case:** Ideal for general inquiries, brainstorming, and tasks that don't require external actions or real-time information.\n\n*   **Query Mode:**\n    *   **Purpose:** Designed for specific, actionable requests that require the agent to perform tasks.\n    *   **Tool Access:** **Full tool access.** The agent can call any of its configured Agent Skills (e.g., Web Scraper, Filesystem, Tavily, Fetch) to execute commands, retrieve data, or modify the system.\n    *   **Functionality:** The agent can perform complex, multi-step tasks like debugging system issues, analyzing file contents, searching the web for current information, or managing files.\n    *   **Use Case:** Ideal for tasks that require external actions, real-time data, or system-level operations.\n\nIn essence, **chat mode is for talking** (using context and knowledge), while **query mode is for doing** (using tools and actions). To perform any task that requires a tool call, the user must prefix their request with `@agent` to activate query mode.",
    "word_count": 238,
    "grade": "FAIL",
    "reason": "Over word limit: 238 words (max 150)",
    "has_headers": false,
    "has_bullets": true
  },
  {
    "id": "E4",
    "query": "How does hybrid search with RRF work compared to dense-only search?",
    "mode": "query",
    "response": "Hybrid search with Reciprocal Rank Fusion (RRF) combines two distinct retrieval methods—dense semantic search and sparse keyword search—into a single, more robust result list. Here's how it differs from dense-only search:\n\n### **Dense-Only Search (e.g., AnythingLLM, port 6333)**\n*   **Mechanism:** Relies solely on semantic similarity. Documents are embedded into high-dimensional vectors (e.g., 1024D BGE-M3) using a neural model. When a query is submitted, the system computes the cosine similarity between the query vector and all document vectors.\n*   **Strengths:** Excels at finding documents that are semantically similar, even if they don’t contain the exact keywords. It understands context and meaning.\n*   **Weaknesses:** Can miss documents that contain exact keywords but are not semantically close. It may return results that are \"close in meaning\" but not the most relevant for a precise query.\n\n### **Hybrid Search with RRF (e.g., LM Studio, port 6334)**\n*   **Mechanism:** Performs two independent searches in parallel:\n    1.  **Dense Search:** Uses the BGE-M3 model to find semantically similar documents (same as dense-only).\n    2.  **Sparse Search:** Uses a keyword-based approach (e.g., TF-IDF or learned lexical weights) to find documents that contain the exact terms from the query.\n*   **Fusion with RRF:** The results from both searches are ranked independently. RRF then combines these two ranked lists into a single unified ranking. The key insight of RRF is that it gives higher priority to documents that appear early in *either* list, but especially those that appear early in both. This prevents a document from being ranked too high just because it scored well in one method but poorly in the other.\n*   **Strengths:** Combines the best of both worlds. It catches documents with exact keyword matches (like port numbers, tool names, or specific phrases) *and* semantically relevant documents. This leads to more comprehensive and accurate results.\n*   **Weaknesses:** Requires more computational resources (both models must run) and is more complex to implement.\n\n### **Key Difference**\n*   **Dense-only search** is like asking, \"What's the most similar thing to this idea?\"\n*   **Hybrid search with RRF** is like asking, \"What's the most similar thing *and* what contains the exact keywords I'm looking for?\" RRF ensures that the final answer list is balanced and prioritizes documents that are strong in either dimension, not just one.",
    "word_count": 373,
    "grade": "FAIL",
    "reason": "Contains ### headers (not allowed for explanation tier); Over word limit: 373 words (max 150)",
    "has_headers": true,
    "has_bullets": true
  },
  {
    "id": "E5",
    "query": "How does AnythingLLM inject RAG context into a query?",
    "mode": "query",
    "response": "AnythingLLM injects RAG context into a query through an automatic, passive retrieval mechanism tied to its Qdrant instance on port 6333. Here's how it works:\n\n1.  **Automatic Retrieval:** When a query is received, the system automatically performs a dense-only semantic search against the Qdrant vector database (port 6333) using BGE-M3 embeddings.\n2.  **Chunk Injection:** The top 16 relevant document chunks (each 6,600 characters with 1,000-character overlap) are retrieved based on a low similarity threshold.\n3.  **System Message Injection:** These retrieved chunks are injected into the system message *after* a \"Context:\" separator, before the user's query is processed. This happens with zero explicit tool call from the agent.\n4.  **Primary Source of Truth:** The agent treats this automatically injected context as its primary source of truth for answering the query. It cites specific values directly from the retrieved chunks, such as \"Per the architecture reference: port 6333 serves dense-only workspace RAG.\"\n\nThis process is entirely passive and automatic, meaning the agent does not need to initiate the search; it happens as a standard part of the query processing pipeline.",
    "word_count": 178,
    "grade": "FAIL",
    "reason": "Over word limit: 178 words (max 150)",
    "has_headers": false,
    "has_bullets": false
  }
]