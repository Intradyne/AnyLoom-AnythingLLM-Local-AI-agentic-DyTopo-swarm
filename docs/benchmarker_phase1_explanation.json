[
  {
    "id": "E1",
    "query": "How do the two RAG pipelines differ?",
    "response": "The two RAG pipelines differ fundamentally in **retrieval mechanism, architecture, and integration**:\n\n### **1. Retrieval Mechanism**\n- **AnythingLLM (Port 6333)**: Uses **passive, dense-only retrieval** with BGE-M3 Q8_0 GGUF embeddings. Document chunks are automatically injected into the system message after a \"Context:\" separator on every relevant query, requiring zero tool calls.\n- **qdrant-rag server (Port 6334)**: Uses **active, hybrid dense-plus-sparse retrieval** via Reciprocal Rank Fusion (RRF). It combines dense semantic search (BGE-M3) with sparse lexical matching (keyword-based term frequency) and requires explicit `rag_search` tool calls.\n\n### **2. Chunking Strategy**\n- **AnythingLLM**: Uses **6,600-character chunks with 1,000-character overlap**, split recursively by paragraph boundaries.\n- **qdrant-rag server**: Uses **deterministic section-header-based chunking** with zero overlap, splitting documents at every `##` header.\n\n### **3. Search Type**\n- **AnythingLLM**: **Dense-only** — relies solely on cosine similarity between query and document embeddings.\n- **qdrant-rag server**: **Hybrid** — fuses results from dense semantic and sparse lexical searches using RRF.\n\n### **4. Integration and Access**\n- **AnythingLLM**: Fully automated — no user or agent action required. The RAG pipeline is integrated into the system prompt and runs passively.\n- **qdrant-rag server**: Requires explicit tool calls (`rag_search`, `rag_status`, etc.) from the LM Studio agent. It is not accessible to AnythingLLM.\n\n### **5. Use Case**\n- **AnythingLLM**: Best for natural language queries where semantic similarity is key (e.g., \"Explain how hybrid search works\").\n- **qdrant-rag server**: Best for technical queries requiring exact term matching (e.g., \"What is the port for the MCP server?\") or when the user needs to scope results by source.\n\n### **6. Data Isolation**\n- The two pipelines operate on **completely independent Qdrant instances** (port 6333 and port 6334) with separate data volumes, chunking strategies, and indexing pipelines.\n\nThis asymmetry means the same information often exists in format-divergent versions: terser in the LM Studio pipeline (where sparse matching catches exact identifiers) and richer prose in the AnythingLLM pipeline (where dense-only retrieval depends entirely on semantic similarity) (per architecture reference).",
    "word_count": 321,
    "grade": "FAIL",
    "reason": "Contains ### headers (not allowed for 2-item comparison); Over word limit: 321 words (max 150)"
  },
  {
    "id": "E2",
    "query": "How does DyTopo route messages between agents?",
    "response": "DyTopo routes messages between agents using **semantic routing based on descriptor embeddings**. It embeds each agent's role and current task description using the MiniLM-L6-v2 sentence embedding model (80 MB RAM on CPU), computes cosine similarity between descriptors, and constructs a directed acyclic graph (DAG) where edges represent relevant communication paths. Only messages with similarity scores above a threshold (controlled by the `tau` parameter) are routed to specific agents. This replaces broadcast mode after round 1, enabling efficient, targeted collaboration—lower `tau` values create denser communication graphs, while higher values create sparser, more independent agent work (per tool-reference.md and architecture reference).",
    "word_count": 99,
    "grade": "PASS",
    "reason": "Good explanation (99 words)"
  },
  {
    "id": "E3",
    "query": "What's the difference between chat mode and query mode in AnythingLLM?",
    "response": "In AnythingLLM, **chat mode** and **query mode** are not distinct operational modes. Instead, they describe the same interaction pattern with different access levels:\n\n- **Chat mode** is the default state where the agent responds using only **workspace RAG context** (from port 6333) and **training knowledge**. It has **no tool access**—it cannot call Tavily, Desktop Commander, Memory, or any other tool. If a user asks for live data (e.g., current prices, exchange rates, sports scores), the agent must respond with: \"This requires a live tool call — use @agent mode to get current data.\"\n\n- **Query mode** is not a standalone mode in AnythingLLM. It is a **subset of chat mode** where the user issues a direct factual query (e.g., \"What is the current price of gold?\"). In this case, the agent still cannot access tools and must either answer from RAG context or training knowledge (with a caveat if outdated) or refuse with the standard live-data disclaimer.\n\nThe **only way to access tools** (Tavily, Desktop Commander, Filesystem, etc.) is by prefixing the message with `@agent` (agent mode). This activates the AIbitat framework, enabling live tool execution and access to external data, system state, and file operations.\n\nThus, the key difference is **tool access**: chat mode (and query mode) have none; agent mode (activated by `@agent`) has full tool access.",
    "word_count": 219,
    "grade": "FAIL",
    "reason": "Over word limit: 219 words (max 150); Missing history/no-history distinction"
  },
  {
    "id": "E4",
    "query": "How does hybrid search with RRF work compared to dense-only search?",
    "response": "Hybrid search with Reciprocal Rank Fusion (RRF) and dense-only search differ fundamentally in **retrieval strategy, coverage, and use case**:\n\n### **1. Dense-Only Search (AnythingLLM, Port 6333)**\n- **Mechanism**: Relies solely on **cosine similarity** between query and document embeddings (BGE-M3 Q8_0 GGUF).\n- **Strengths**:\n  - Excels at capturing **semantic meaning** (e.g., \"climate change\" matches \"global warming\").\n  - Works well for natural language queries and open-ended topics.\n- **Weaknesses**:\n  - Fails to catch **exact term matches** (e.g., \"port 6334\" or \"QdrantClient\").\n  - Prone to retrieving semantically similar but irrelevant results when query phrasing is ambiguous.\n\n### **2. Hybrid Search with RRF (LM Studio, Port 6334)**\n- **Mechanism**: Combines two independent search pathways:\n  - **Dense search**: BGE-M3 embeddings (semantic similarity).\n  - **Sparse search**: Keyword-based term frequency weights (exact term matching).\n  - **Fusion**: Uses **Reciprocal Rank Fusion (RRF)** to merge results:\n    $$\n    \\text{Score}(d) = \\sum_{i=1}^{n} \\frac{1}{k + \\text{rank}_i(d)}\n    $$\n    where $k = 60$ and $\\text{rank}_i(d)$ is the document’s position in the $i$-th list.\n- **Strengths**:\n  - **Balances semantics and precision**: Captures both meaning and exact terms.\n  - **Robust to query phrasing**: A query like \"What port is the MCP server on?\" retrieves \"port 6334\" via sparse matching, even if the query is phrased as a question.\n  - **Reduces false positives**: Documents ranking poorly in either list are penalized.\n- **Weaknesses**:\n  - Requires **explicit tool calls** (`rag_search`) — not automatic.\n  - Slightly slower due to dual computation (dense + sparse).\n\n### **Key Difference**\n- **Dense-only** is **passive and automatic** — ideal for natural language understanding.\n- **Hybrid with RRF** is **active and precise** — ideal for technical queries requiring exact identifiers or configuration details.\n\nThis asymmetry means the same information often exists in **format-divergent versions**: terser in the LM Studio pipeline (where sparse matching catches exact identifiers) and richer prose in the AnythingLLM pipeline (where dense-only retrieval depends entirely on semantic similarity) (per architecture reference).",
    "word_count": 310,
    "grade": "FAIL",
    "reason": "Contains ### headers (not allowed for 2-item comparison); Over word limit: 310 words (max 150)"
  }
]