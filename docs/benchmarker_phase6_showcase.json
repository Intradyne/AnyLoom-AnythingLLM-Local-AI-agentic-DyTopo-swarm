[
  {
    "id": "S1",
    "label": "Perfect Lookup",
    "query": "What is BGE-M3?",
    "mode": "query",
    "response": "BGE-M3 is a multi-granularity embedding model from BAAI that produces both dense and sparse vectors for hybrid search. In this stack, BGE-M3 embeddings power both Qdrant instances — GGUF Q8_0 on GPU for port 6333 and FlagEmbedding on CPU for port 6334 (per architecture reference).",
    "word_count": 45
  },
  {
    "id": "S2",
    "label": "Price Fabrication Guard",
    "query": "What's gold trading at right now?",
    "mode": "chat",
    "response": "This requires a live tool call — use @agent mode to get current data.",
    "word_count": 14
  },
  {
    "id": "S3",
    "label": "Tool Boundary Awareness",
    "query": "Search Memory for all entities related to Qdrant",
    "mode": "chat",
    "response": "search_nodes(\"Qdrant\")",
    "word_count": 1
  },
  {
    "id": "S4",
    "label": "Deep Architecture Knowledge",
    "query": "Walk me through the full architecture of this stack, from user query to Qdrant retrieval and back",
    "mode": "query",
    "response": "Here is the full architecture of the Loom stack, from user query to Qdrant retrieval and back, as implemented in this deployment:\n\n---\n\n### **1. User Query Entry**\n- The user submits a query via one of two frontends:\n  - **AnythingLLM** (web interface at `localhost:3001`) — for workspace-specific RAG and chat.\n  - **LM Studio** (web interface at `localhost:1234`) — for general chat, tool use, and hybrid RAG.\n\n---\n\n### **2. Query Routing and Inference**\n- Both frontends route all **text generation and embedding requests** to the **same LM Studio inference endpoint** at `http://localhost:1234/v1`.\n- This endpoint serves the **Qwen3-30B-A3B-Instruct-2507** model (Q6_K quantization) and the **co-loaded BGE-M3 GGUF embedding model**.\n- **Inference is sequential**: All requests (from both frontends and DyTopo swarms) queue at port 1234. Active DyTopo swarms or high-volume usage can delay responses.\n\n---\n\n### **3. RAG Retrieval Pathways (Two Independent Pipelines)**\n\n#### **A. AnythingLLM Workspace RAG (Port 6333) – Passive, Dense-Only**\n- **Trigger**: Automatic on every relevant query in chat mode.\n- **Mechanism**:\n  - The workspace agent retrieves up to 16 document snippets from the **Qdrant instance on port 6333** using **dense-only cosine similarity**.\n  - Documents are chunked (6,600 characters with 1,000-character overlap), embedded via BGE-M3 GGUF through the `/v1/embeddings` endpoint, and stored as 1024-dimensional vectors.\n- **Retrieval**:\n  - No explicit tool call required.\n  - Snippets are injected into the system message after a `Context:` separator.\n- **Use Case**: Natural language queries where semantic similarity is key.\n\n#### **B. LM Studio Hybrid RAG (Port 6334) – Active, Dense+Sparse**\n- **Trigger**: Explicit tool call via `rag_search` from the LM Studio agent.\n- **Mechanism**:\n  - The **qdrant-rag MCP server** (running natively on the host) connects to the Qdrant instance on port 6334.\n  - Performs **hybrid search** using **Reciprocal Rank Fusion (RRF)**:\n    - **Dense search**: BGE-M3 embeddings (computed on CPU via FlagEmbedding).\n    - **Sparse search**: Keyword-based term frequency weights (inverted index).\n  - Results are fused and returned with source filtering (`source_dir`).\n- **Use Case**: Technical queries requiring exact term matching (e.g., \"port 6334\", \"QdrantClient\") or when the user needs to scope results.\n\n---\n\n### **4. Qdrant Instances (Two Independent Containers)**\n- **Port 6333 (AnythingLLM Qdrant)**:\n  - Docker container: `anythingllm-qdrant`\n  - Data volume: `qdrant_anythingllm`\n  - Configuration: Dense-only, cosine similarity, 4 GB memory limit, always-restart policy.\n- **Port 6334 (LM Studio Qdrant)**:\n  - Docker container: `lmstudio-qdrant`\n  - Data volume: `qdrant_lmstudio`\n  - Configuration: Hybrid search with RRF, source_dir filtering, 2.3 GB RAM (CPU-only), no GPU VRAM used.\n\n> **Note**: The two Qdrant instances are completely isolated. Documents and configurations are independent.\n\n---\n\n### **5. Tool Access and Cross-Agent Coordination**\n- **Shared Resource**: The **Memory knowledge graph** (local, private) is accessible to both agents for storing stable facts (e.g., port mappings, resolved errors).\n- **Exclusive Tools**:\n  - **AnythingLLM**: Only has built-in Agent Skills (Web Scraper, RAG, Save File, etc.).\n  - **LM Studio**: Has full access to MCP tools (Desktop Commander, Filesystem, Tavily, Fetch, Playwright, Sequential Thinking, DyTopo, and `rag_search` tools).\n\n---\n\n### **6. Output and Response**\n- The final response is generated by the **Qwen3-30B-A3B model** at port 1234, incorporating:\n  - The **Context:** section (from AnythingLLM’s RAG).\n  - Or the **rag_search** results (from LM Studio’s hybrid RAG).\n  - Or both (if the user switches between frontends or the agent synthesizes findings).\n- Responses are returned to the user via the respective frontend.\n\n---\n\n### **7. Key Architectural Asymmetry**\n- **AnythingLLM**: Passive RAG — automatic, dense-only, no tool calls.\n- **LM Studio**: Active RAG — explicit tool calls, hybrid search, source filtering.\n\nThis design allows for both seamless chat (AnythingLLM) and precise, controlled retrieval (LM Studio), with full separation of concerns and data isolation between the two pipelines.\n\n---\n\n### **Summary Diagram (Textual)**\n```\nUser Query\n    ↓\n[AnythingLLM] ↔ [LM Studio API (port 1234)] ← Qwen3-30B-A3B (GPU)\n    ↓\n[Qdrant 6333 (dense-only)] ← AnythingLLM Workspace RAG (auto-injected)\n    ↓\n[Qdrant 6334 (hybrid RRF)] ← LM Studio qdrant-rag MCP server (explicit call)\n    ↓\nResponse → User\n```\n\n> **Note**: The Memory graph is shared for persistent facts, but all other data paths are agent-specific.",
    "word_count": 665
  },
  {
    "id": "S5",
    "label": "Explanation-Tier Comparison",
    "query": "How does AnythingLLM's RAG pipeline differ from the MCP qdrant-rag server?",
    "mode": "query",
    "response": "AnythingLLM's RAG pipeline and the MCP qdrant-rag server differ fundamentally in **retrieval mechanism, architecture, and integration**:\n\n### **1. Retrieval Mechanism**\n- **AnythingLLM (Port 6333)**: Uses **passive, dense-only retrieval** with BGE-M3 Q8_0 GGUF embeddings. Document chunks are automatically injected into the system message after a \"Context:\" separator on every relevant query, requiring zero tool calls.\n- **qdrant-rag server (Port 6334)**: Uses **active, hybrid dense-plus-sparse retrieval** via Reciprocal Rank Fusion (RRF). It combines dense semantic search (BGE-M3) with sparse lexical matching (keyword-based term frequency) and requires explicit `rag_search` tool calls.\n\n### **2. Chunking Strategy**\n- **AnythingLLM**: Uses **6,600-character chunks with 1,000-character overlap**, split recursively by paragraph boundaries.\n- **qdrant-rag server**: Uses **deterministic section-header-based chunking** with zero overlap, splitting documents at every `##` header.\n\n### **3. Search Type**\n- **AnythingLLM**: **Dense-only** — relies solely on cosine similarity between query and document embeddings.\n- **qdrant-rag server**: **Hybrid** — fuses results from dense semantic and sparse lexical searches using RRF.\n\n### **4. Integration and Access**\n- **AnythingLLM**: Fully automated — no user or agent action required. The RAG pipeline is integrated into the system prompt and runs passively.\n- **qdrant-rag server**: Requires explicit tool calls (`rag_search`, `rag_status`, etc.) from the LM Studio agent. It is not accessible to AnythingLLM.\n\n### **5. Use Case**\n- **AnythingLLM**: Best for natural language queries where semantic similarity is key (e.g., \"Explain how hybrid search works\").\n- **qdrant-rag server**: Best for technical queries requiring exact term matching (e.g., \"What is the port for the MCP server?\") or when the user needs to scope results by source.\n\n### **6. Data Isolation**\n- The two pipelines operate on **completely independent Qdrant instances** (port 6333 and port 6334) with separate data volumes, chunking strategies, and indexing pipelines.\n\nThis asymmetry means the same information often exists in format-divergent versions: terser in the LM Studio pipeline (where sparse matching catches exact identifiers) and richer prose in the AnythingLLM pipeline (where dense-only retrieval depends entirely on semantic similarity) (per architecture reference).",
    "word_count": 325
  },
  {
    "id": "S6",
    "label": "RAG-Grounded Citation",
    "query": "What chunking strategy does this workspace use?",
    "mode": "query",
    "response": "This workspace uses **recursive text splitting with paragraph boundary preservation** for document chunking in the AnythingLLM RAG pipeline (port 6333). The chunk size is set to **6,600 characters** with a **1,000-character overlap** between consecutive chunks. This strategy ensures that semantic units (such as paragraphs or sections) are preserved within a single chunk, minimizing the risk of splitting content mid-sentence or mid-idea. The recursive splitter respects natural document structure, making it ideal for dense semantic retrieval via BGE-M3 embeddings. This configuration is managed through AnythingLLM's document ingestion pipeline and is distinct from the section-header-based chunking used in the LM Studio qdrant-rag server (port 6334), which splits documents at every `##` header with zero overlap.",
    "word_count": 113
  }
]