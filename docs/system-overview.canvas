{
	"nodes":[
		{"id":"anythingllm","type":"text","text":"## AnythingLLM\n\nUser interface + working memory\nText / Voice to Text / Voice\nBuilt-in web scraper (LLM + manual)\nAuto-loads Qdrant RAG (port 6333)\nWorkspace: `AnyLoom` (auto-configured)\n\n**Agent Skills:** asset-price, smart-web-reader\n**MCP:** fetch (uvx mcp-server-fetch)","x":-200,"y":-700,"width":340,"height":280,"color":"4"},
		{"id":"user_input","type":"text","text":"## User Input\n\nText or voice query\nObsidian notes, to-dos\nFile / web / automation requests","x":-180,"y":-960,"width":280,"height":160,"color":"1"},
		{"id":"obsidian","type":"text","text":"## Obsidian\n\nPersonal knowledge base\nNotes, to-dos, project planning\nCanvas visualizations","x":280,"y":-960,"width":280,"height":160,"color":"6"},
		{"id":"8a45810e2796ae10","type":"text","text":"api calls","x":834,"y":-530,"width":116,"height":50},
		{"id":"n8n","type":"text","text":"## n8n Docker Container\n\nScheduled tasks\nComplex automation\nWorkflows\nn8n can call LLMs or other AI\nAlso has its own Docker MCP server","x":461,"y":-625,"width":284,"height":240,"color":"5"},
		{"id":"lm_studio","type":"text","text":"## llama.cpp\n\nInference backend (Docker)\nPort 8008 (host) / 8080 (container)\nOpenAI-compatible: /v1/chat/completions\nGPU-accelerated with 131K context\nParallel agent execution (semaphore=8)\n","x":-200,"y":-385,"width":340,"height":225,"color":"4"},
		{"id":"docker_mcp","type":"text","text":"## Docker MCP Gateway\n\nShared MCP infrastructure for\nAnythingLLM + llama.cpp\n\n**Tools:**\n‚Ä¢ Desktop Commander ‚Äî shell, file ops, processes\n‚Ä¢ Filesystem ‚Äî read/write/list/move/delete\n‚Ä¢ Memory ‚Äî persistent knowledge graph\n‚Ä¢ Context7 ‚Äî always-current API docs\n‚Ä¢ Tavily ‚Äî web search (API credits)\n‚Ä¢ Fetch ‚Äî URL ‚Üí clean markdown (free)\n‚Ä¢ Sequential Thinking ‚Äî structured reasoning\n\nQdrant is called directly by port, not through gateway","x":410,"y":-361,"width":385,"height":420,"color":"5"},
		{"id":"standalone","type":"text","text":"## Standalone Tools\n\nResponsive bots\nPython smart web scraper\n2D / video / 3D generation models","x":892,"y":-225,"width":243,"height":170,"color":"2"},
		{"id":"qdrant","type":"text","text":"## Qdrant ‚Äî Port 6333\n\nSingle hybrid instance (dense+sparse RAG)\nServes both AnythingLLM and MCP server\nBGE-M3 via ONNX INT8 on CPU (MCP RAG)\nDense 1024-dim + sparse lexical (RRF fusion)\n~0.6 GB RAM, 0 VRAM\n\nContainer: anyloom-qdrant (Docker Compose)","x":-780,"y":-585,"width":360,"height":285,"color":"5"},
		{"id":"mcp_server","type":"text","text":"## MCP Server (qdrant-rag)\n\n`src/qdrant_mcp_server.py` (~1,030 lines)\nFastMCP, stdio JSON-RPC transport\n\n**RAG tools (5):** rag_search, rag_status,\nrag_reindex, rag_sources, rag_file_info\n\n**DyTopo tools (3):** swarm_start,\nswarm_status, swarm_result\n\nBGE-M3 ONNX INT8 on CPU (~0.6 GB RAM)\nHybrid dense+sparse search with RRF","x":-200,"y":-101,"width":340,"height":365,"color":"4"},
		{"id":"dytopo","type":"text","text":"## DyTopo Swarm\n\n`src/dytopo/` ‚Äî 8 core modules + 6 sub-packages\nAsync parallel orchestration (asyncio.gather)\nSemaphore-controlled concurrency (llama.cpp default)\nSemantic routing: MiniLM-L6-v2, cosine sim, threshold œÑ\nTopological tiers for parallel-within-tier execution\nDomains: code ¬∑ math ¬∑ general\nOptional RAG context pre-fetch ‚Üí Manager (1 retrieval, 0 per-agent)\n\nSee [swarm-overview.canvas](swarm-overview.canvas) for module details","x":-780,"y":21,"width":360,"height":419,"color":"3"},
		{"id":"rag_sources","type":"text","text":"## RAG Document Sources\n\n**Multi-source indexing ‚Üí Qdrant 6333:**\n6 docs in `rag-docs/`\n- anythingllm/ (6 docs)\nHybrid dense+sparse indexing\nSHA-256 content-hash dedup\nsource_dir payload filtering","x":-1140,"y":-300,"width":360,"height":260,"color":"6"},
		{"id":"embedding","type":"text","text":"## llama.cpp Embedding\n\nBGE-M3 Q8_0 GGUF (GPU, AnythingLLM)\nPort 8009 (host) / 8080 (container)\nOpenAI-compatible: /v1/embeddings\n1024-dim dense vectors, 8192 context\n~605 MB model, ~635 MB VRAM\n\nContainer: anyloom-embedding","x":-780,"y":-920,"width":360,"height":240,"color":"4"},
		{"id":"agent_skills","type":"text","text":"## Agent Skills (Custom)\n\nRun inside AnythingLLM container\nLLM selects tool via function calling\n\n**asset-price** ‚Äî yahoo-finance2 v3\nStocks, crypto, commodities, indices, ETFs\nAlias table + search fallback\n\n**smart-web-reader** ‚Äî 3-tier extraction\nDefuddle ‚Üí Readability+Turndown ‚Üí regex\n8K char truncation, 15s timeout\n\nüìç [skills/](../skills/)","x":795,"y":-960,"width":310,"height":410,"color":"2"}
	],
	"edges":[
		{"id":"e1","fromNode":"user_input","fromSide":"bottom","toNode":"anythingllm","toSide":"top","color":"1","label":"chat / query"},
		{"id":"e2","fromNode":"user_input","fromSide":"right","toNode":"obsidian","toSide":"left","color":"1","label":"notes"},
		{"id":"e3","fromNode":"anythingllm","fromSide":"bottom","toNode":"lm_studio","toSide":"top","color":"4","label":"LLM inference"},
		{"id":"e4","fromNode":"anythingllm","fromSide":"left","toNode":"qdrant","toSide":"right","color":"4","label":"hybrid RAG"},
		{"id":"e5","fromNode":"lm_studio","fromSide":"bottom","toNode":"mcp_server","toSide":"top","color":"4","label":"MCP tools"},
		{"id":"e8","fromNode":"mcp_server","fromSide":"left","toNode":"qdrant","toSide":"right","color":"5","label":"search"},
		{"id":"e9","fromNode":"mcp_server","fromSide":"bottom","toNode":"dytopo","toSide":"right","color":"3","label":"swarm tasks"},
		{"id":"e11","fromNode":"rag_sources","fromSide":"right","toNode":"qdrant","toSide":"bottom","color":"6"},
		{"id":"e12","fromNode":"docker_mcp","fromSide":"right","toNode":"standalone","toSide":"left"},
		{"id":"e15","fromNode":"obsidian","fromSide":"bottom","toNode":"anythingllm","toSide":"top","color":"6","label":"context"},
		{"id":"e16","fromNode":"dytopo","fromSide":"top","toNode":"lm_studio","toSide":"left","color":"3","label":"LLM calls"},
		{"id":"b1352f454ef94003","fromNode":"n8n","fromSide":"left","toNode":"anythingllm","toSide":"right","color":"1","label":"chat/query"},
		{"id":"ff8262542ace6b7a","fromNode":"n8n","fromSide":"left","toNode":"lm_studio","toSide":"right","color":"1","label":"chat/query"},
		{"id":"36d3a83a99ed569e","fromNode":"lm_studio","fromSide":"right","toNode":"docker_mcp","toSide":"left","color":"4"},
		{"id":"35c1b2400e8af29d","fromNode":"n8n","fromSide":"right","toNode":"8a45810e2796ae10","toSide":"left"},
		{"id":"79a5b601d269adb7","fromNode":"8a45810e2796ae10","fromSide":"left","toNode":"n8n","toSide":"right"},
		{"id":"9166f6f1838afc97","fromNode":"n8n","fromSide":"right","toNode":"standalone","toSide":"left"},
		{"id":"a85278928ec47069","fromNode":"docker_mcp","fromSide":"top","toNode":"n8n","toSide":"bottom","color":"5"},
		{"id":"e_allm_embed","fromNode":"anythingllm","fromSide":"left","toNode":"embedding","toSide":"right","color":"4","label":"embed chunks"},
		{"id":"e_embed_qdrant","fromNode":"embedding","fromSide":"bottom","toNode":"qdrant","toSide":"top","color":"4","label":"embeddings"},
		{"id":"e_allm_skills","fromNode":"anythingllm","fromSide":"right","toNode":"agent_skills","toSide":"left","color":"2","label":"@agent tools"}
	]
}