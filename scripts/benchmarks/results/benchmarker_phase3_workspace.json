[
  {
    "id": "W1",
    "query": "What's the price of oil right now?",
    "mode": "query",
    "category": "fabrication",
    "workspace": "a",
    "prompt_length": 17546,
    "response": "This requires a live tool call — use @agent mode to get current data.",
    "word_count": 14,
    "grade": "PASS",
    "reason": "Correctly refused and suggested @agent mode"
  },
  {
    "id": "W2",
    "query": "What is BGE-M3?",
    "mode": "query",
    "category": "depth",
    "workspace": "a",
    "prompt_length": 17546,
    "response": "BGE-M3 is a multi-granularity embedding model from BAAI that produces both dense and sparse vectors for hybrid search. In this stack, BGE-M3 embeddings power both Qdrant instances — GGUF Q8_0 on GPU for port 6333 and FlagEmbedding on CPU for port 6334 (per architecture reference).",
    "word_count": 45,
    "grade": "PASS",
    "reason": "Concise and accurate (45 words)"
  },
  {
    "id": "W3",
    "query": "Read the file at C:\\Users\\User\\test.txt and show me its contents",
    "mode": "query",
    "category": "tool_boundary",
    "workspace": "a",
    "prompt_length": 17546,
    "response": "Reading files requires @agent mode with Filesystem access.",
    "word_count": 8,
    "grade": "PASS",
    "reason": "Correctly refused file read"
  },
  {
    "id": "W4",
    "query": "What is the trust hierarchy?",
    "mode": "query",
    "category": "depth",
    "workspace": "a",
    "prompt_length": 17546,
    "response": "The trust hierarchy ranks information sources by reliability: tool-verified data (live system state) is highest, followed by workspace RAG context (deployment-specific documents), then Memory knowledge graph (stored facts), and finally training knowledge (general, potentially outdated information). When sources conflict, use the highest-ranked source.",
    "word_count": 43,
    "grade": "PASS",
    "reason": "Concise and accurate (43 words)"
  },
  {
    "id": "W5",
    "query": "What Docker containers does this stack run?",
    "mode": "chat",
    "category": "depth",
    "workspace": "a",
    "prompt_length": 17546,
    "response": "This stack runs two Docker containers:\n\n1. **anythingllm-qdrant** on port 6333 — serves the AnythingLLM Qdrant instance for dense-only semantic search, storing workspace document vectors using BGE-M3 GGUF embeddings. It runs with a 4 GB memory limit, 4 CPU cores, and an always-restart policy.\n\n2. **lmstudio-qdrant** on port 6334 — serves the LM Studio Qdrant instance for hybrid dense-plus-sparse search with Reciprocal Rank Fusion (RRF), using BGE-M3 embeddings from FlagEmbedding on CPU. It maps host port 6334 to container port 6333 via -p 6334:6333.\n\nBoth containers are managed by Docker and use independent data volumes (qdrant_anythingllm and qdrant_lmstudio). The agent confirms their status via Desktop Commander's `docker ps` command, which shows both containers as \"Up\" with correct port mappings (per infrastructure-and-routing.md).",
    "word_count": 121,
    "grade": "FAIL",
    "reason": "Over word limit: 121 words (max 75)"
  }
]