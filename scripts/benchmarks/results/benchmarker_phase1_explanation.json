[
  {
    "id": "E1",
    "query": "How do the two RAG pipelines differ?",
    "mode": "query",
    "response": "The two RAG pipelines differ fundamentally in their retrieval mechanism and integration with the agent workflow. AnythingLLM uses passive, dense-only retrieval: document chunks from the `anyloom_docs` collection are automatically injected into the system message on every relevant query via a hybrid search pipeline on port 6333, requiring no agent action. In contrast, the qdrant-rag MCP server uses active, hybrid dense-plus-sparse search: the agent must explicitly call the `rag_search` tool to retrieve documents, which performs Reciprocal Rank Fusion (RRF) of dense semantic matches (from BGE-M3 ONNX INT8 on CPU) and sparse keyword matches (term frequency weights), enabling precise identification of exact identifiers like port numbers and tool names. This results in format-divergent outputs—terser, keyword-optimized results from the MCP pipeline and richer, prose-based results from AnythingLLM’s dense-only approach—while both query the same Qdrant instance (port 6333) with different collections and strategies (per infrastructure-and-routing.md).",
    "word_count": 141,
    "grade": "FAIL",
    "reason": "Missing port numbers",
    "has_headers": false,
    "has_bullets": false
  },
  {
    "id": "E2",
    "query": "How does DyTopo route messages between agents?",
    "mode": "query",
    "response": "DyTopo routes messages between agents using semantic routing based on the MiniLM-L6-v2 sentence embedding model. After the first round (which uses broadcast mode), the system computes embeddings for each agent's descriptor (e.g., \"Developer\", \"Researcher\") and uses cosine similarity to construct a directed acyclic graph (DAG) where edges represent semantic relevance. Agents only receive messages from others whose descriptors are above a configurable threshold (tau), ensuring that each agent only processes inputs relevant to its role. This prevents information overload and enables efficient, collaborative reasoning across multiple inference rounds.",
    "word_count": 88,
    "grade": "PASS",
    "reason": "Good explanation (88 words)",
    "has_headers": false,
    "has_bullets": false
  },
  {
    "id": "E3",
    "query": "What's the difference between chat mode and query mode in AnythingLLM?",
    "mode": "query",
    "response": "In AnythingLLM, chat mode and query mode are functionally identical — both use the same system prompt and operate under the same constraints. The distinction lies solely in the user interface and interaction pattern:  \n- **Chat mode** is the default interface for conversational interaction, where the model responds to natural language queries using workspace context and training knowledge.  \n- **Query mode** is a streamlined variant of chat mode, designed for single, focused questions (e.g., \"What is the price of gold?\"), and follows the same rules: no tool access without the `@agent` prefix.  \n\nBoth modes lack tool access in the absence of `@agent`, and any query requiring live data (prices, scores, weather, etc.) must be prefixed with `@agent` to activate the AIbitat tool-calling framework. The system prompt explicitly states that only agent mode can call tools, and chat mode cannot execute Tavily, Desktop Commander, or any other external tool.",
    "word_count": 148,
    "grade": "PASS",
    "reason": "Good explanation (148 words)",
    "has_headers": false,
    "has_bullets": true
  },
  {
    "id": "E4",
    "query": "How does hybrid search with RRF work compared to dense-only search?",
    "mode": "query",
    "response": "Hybrid search with Reciprocal Rank Fusion (RRF) combines both dense semantic matching and sparse keyword matching to improve retrieval accuracy. In this stack, dense vectors from BGE-M3 (1024-dimensional, ONNX INT8 on CPU) capture semantic meaning, while sparse vectors use TF-weighted hash-based term frequency to catch exact keyword matches like port numbers and tool names. RRF fuses the ranked results from both pathways by scoring each document as the reciprocal of its rank in each list, reducing the impact of poor rankings in one mode. This results in better recall for queries mixing natural language with specific identifiers. In contrast, dense-only search (used by AnythingLLM) relies solely on semantic similarity, which can miss exact matches or fail on queries with precise technical terms. The hybrid approach ensures both meaning and precision are preserved, making it more effective for technical and factual queries.",
    "word_count": 141,
    "grade": "PASS",
    "reason": "Good explanation (141 words)",
    "has_headers": false,
    "has_bullets": false
  },
  {
    "id": "E5",
    "query": "How does AnythingLLM inject RAG context into a query?",
    "mode": "query",
    "response": "AnythingLLM injects RAG context into a query automatically through its workspace Qdrant instance on port 6333. On every relevant message, the system performs a dense-only retrieval using BGE-M3 embeddings (GGUF Q8_0 format) via the llama.cpp backend. It retrieves up to 16 document snippets from the `anyloom_docs` collection, using 6,600-character chunks with 1,000-character overlap and a low similarity threshold. These chunks are injected into the system message after a \"Context:\" separator, serving as the agent's primary source of truth. The retrieval is passive—no explicit tool call is required—and is triggered by semantic similarity between the query and stored content. This ensures that the agent always has access to the most current, context-specific information from the workspace, even when the query is phrased in natural language.",
    "word_count": 124,
    "grade": "PASS",
    "reason": "Good explanation (124 words)",
    "has_headers": false,
    "has_bullets": false
  }
]