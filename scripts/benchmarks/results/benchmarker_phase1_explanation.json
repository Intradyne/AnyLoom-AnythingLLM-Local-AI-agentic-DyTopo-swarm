[
  {
    "id": "E1",
    "query": "How do the two RAG pipelines differ?",
    "mode": "query",
    "response": "The two RAG pipelines in this stack—**AnythingLLM's passive RAG** and the **qdrant-rag MCP server's active RAG**—differ fundamentally in retrieval mechanism, execution model, and architectural design, despite sharing the same Qdrant instance (port 6333) and BGE-M3 embedding model.\n\n1. **Retrieval Mechanism**:\n   - **AnythingLLM**: Uses **dense-only semantic search** via cosine similarity on BGE-M3 embeddings. It retrieves document chunks automatically, without any agent action, based on semantic relevance to the query.\n   - **qdrant-rag MCP server**: Uses **hybrid dense+sparse search** with Reciprocal Rank Fusion (RRF). It combines semantic similarity (dense vectors) with exact keyword matching (sparse vectors, e.g., port numbers, tool names), improving recall for queries containing specific identifiers.\n\n2. **Execution Model**:\n   - **AnythingLLM**: **Passive retrieval**—chunks from the `anyloom_docs` collection are injected into the system message automatically on every relevant query, requiring no tool call.\n   - **qdrant-rag MCP server**: **Active retrieval**—the agent must explicitly call `rag_search` with parameters (query, limit, source filter), making retrieval a deliberate, on-demand operation.\n\n3. **Chunking and Content**:\n   - **AnythingLLM**: Uses **6,600-character chunks with 1,000-character overlap**, optimized for semantic continuity and natural language understanding.\n   - **qdrant-rag MCP server**: Uses **deterministic section-header-based chunking with zero overlap**, preserving exact structural boundaries (e.g., code blocks, configuration sections), which improves precision for technical queries.\n\n4. **Collection and Access**:\n   - **AnythingLLM**: Queries the `anyloom_docs` collection via the llama.cpp `/v1/embeddings` endpoint.\n   - **qdrant-rag MCP server**: Queries a separate collection via the `rag_search` tool, which accesses the same Qdrant instance but uses different indexing and filtering logic.\n\n5. **Use Case and Performance**:\n   - **AnythingLLM**: Best for natural language queries and general knowledge retrieval where semantic meaning is primary.\n   - **qdrant-rag MCP server**: Better for precise, technical queries involving exact terms (e.g., \"what port is Qdrant on?\") due to sparse matching and explicit control.\n\nIn summary, the two pipelines are **format-divergent** and **behaviorally asymmetric**: AnythingLLM offers seamless, automatic access to rich prose, while the MCP pipeline enables precise, controlled retrieval of structured technical content.",
    "word_count": 316,
    "grade": "FAIL",
    "reason": "Over word limit: 316 words (max 150); Missing port numbers",
    "has_headers": false,
    "has_bullets": true
  },
  {
    "id": "E2",
    "query": "How does DyTopo route messages between agents?",
    "mode": "query",
    "response": "DyTopo routes messages between agents using **semantic routing via MiniLM-L6-v2 sentence embeddings**. After the first round (which uses broadcast mode), DyTopo computes a **cosine similarity threshold** between agent descriptors (e.g., \"Developer\", \"Tester\", \"Analyst\") and the current task or message content. Agents are connected in a **Directed Acyclic Graph (DAG)** based on this similarity, where only messages relevant to a given agent’s role are delivered.\n\nThe process works as follows:\n1. **Descriptor Embedding**: Each agent’s role (e.g., \"Researcher\", \"Verifier\") is embedded using the MiniLM-L6-v2 model (a 22M-parameter model consuming ~80 MB RAM on CPU).\n2. **Similarity Thresholding**: DyTopo computes cosine similarity between the message content and each agent’s descriptor. Only agents with similarity above a configurable threshold (called `tau`) receive the message.\n3. **Graph Construction**: The system builds a DAG where nodes are agents and edges represent semantic relevance. The graph is topologically sorted so agents execute in an order that respects dependency flow.\n4. **Message Passing**: During each round, only the agents with high semantic similarity to the current task receive the message. This prevents information overload and ensures that each agent receives only the information relevant to its role.\n\nThis approach enables **collaborative reasoning with minimal noise**, allowing agents to build on each other’s work in a structured, efficient way. For example, in a code review swarm, the Tester only receives the Developer’s implementation if it’s semantically relevant, enabling targeted feedback. The `tau` parameter controls the threshold: lower values (e.g., 0.1–0.2) increase communication density, while higher values (e.g., 0.4–0.6) promote independence.\n\nThis system is implemented in the `router.py` module of the DyTopo core, which uses NetworkX to construct and manage the DAG dynamically per task.",
    "word_count": 276,
    "grade": "FAIL",
    "reason": "Over word limit: 276 words (max 150)",
    "has_headers": false,
    "has_bullets": false
  },
  {
    "id": "E3",
    "query": "What's the difference between chat mode and query mode in AnythingLLM?",
    "mode": "query",
    "response": "In AnythingLLM, **chat mode** and **query mode** are two distinct interaction paradigms that differ in tool access, response behavior, and underlying system architecture:\n\n1. **Chat Mode**:\n   - **Tool Access**: No live tool access. The agent can only use **workspace context** (auto-injected RAG from Qdrant port 6333) and **training knowledge**.\n   - **Response Behavior**: Answers are generated from pre-existing document chunks or internal knowledge. For time-sensitive queries (e.g., prices, weather, news), the agent must explicitly state: *\"This requires a live tool call — use @agent mode to get current data.\"*\n   - **Use Case**: Best for general knowledge, document retrieval, and non-real-time tasks where the answer is already in the workspace or in training data.\n\n2. **Query Mode**:\n   - **Tool Access**: Enables **live tool execution** via the AIbitat framework when prefixed with `@agent`. This allows calls to Tavily, Fetch, Desktop Commander, Filesystem, and other MCP tools.\n   - **Response Behavior**: The agent can execute tools first, then generate a response based on real-time data. For example, it can fetch current stock prices, check system status, or read file contents.\n   - **Use Case**: Designed for dynamic, real-time tasks such as price lookups, system diagnostics, web scraping, and file operations.\n\n**Key Difference**:  \nChat mode is **read-only and static**—it cannot access live data or perform actions. Query mode is **action-oriented and dynamic**, allowing the agent to execute tools and retrieve up-to-the-minute information. The system prompt explicitly enforces this separation: in chat mode, the agent must refuse to generate fabricated prices or rates; in query mode, it must call the appropriate tool first.\n\nThis distinction ensures that users are always aware of whether the response is based on current data or static knowledge.",
    "word_count": 275,
    "grade": "FAIL",
    "reason": "Over word limit: 275 words (max 150); Missing history/no-history distinction",
    "has_headers": false,
    "has_bullets": true
  },
  {
    "id": "E4",
    "query": "How does hybrid search with RRF work compared to dense-only search?",
    "mode": "query",
    "response": "Hybrid search with Reciprocal Rank Fusion (RRF) combines **dense semantic search** and **sparse lexical search** to improve retrieval accuracy, while dense-only search relies solely on semantic similarity.\n\n### **Dense-Only Search**:\n- Uses only **dense vectors** (e.g., BGE-M3 embeddings) to compute cosine similarity between query and documents.\n- Excels at capturing **semantic meaning** — e.g., \"What is the price of gold?\" retrieves documents discussing gold prices, even if the word \"price\" isn’t used.\n- **Fails on exact keyword matches** — e.g., \"What port is Qdrant on?\" may miss a document stating \"port 6333\" if the semantic similarity is low.\n- **No keyword precision** — relies entirely on embedding quality and similarity threshold.\n\n### **Hybrid Search with RRF**:\n- Combines **two independent search paths**:\n  1. **Dense search**: Semantic matching using BGE-M3 embeddings (1024-dim, ONNX INT8 on CPU).\n  2. **Sparse search**: Keyword-based matching using TF-weighted hash-based vectors (e.g., term frequency in document).\n- **RRF fusion** merges the two ranked result lists:\n  - Each document’s final score is computed as the sum of the reciprocals of its rank in each list:  \n    `Final Score = 1/rank_dense + 1/rank_sparse`\n  - This gives higher weight to documents that rank well in both paths.\n- **Advantages**:\n  - **Better recall**: Captures both semantic meaning and exact terms (e.g., \"port 6333\", \"BGE-M3\", \"ONNX INT8\").\n  - **Robust to query phrasing**: Works well for both natural language and keyword-heavy queries.\n  - **No reliance on perfect embedding similarity** — a document with a correct keyword but low semantic score can still rank highly.\n\n### **Comparison**:\n| Feature | Dense-Only Search | Hybrid Search with RRF |\n|--------|-------------------|--------------------------|\n| Uses | Only dense vectors | Dense + sparse vectors |\n| Matches | Semantic meaning | Semantic + exact keywords |\n| Handles | Natural language | Natural language + technical terms |\n| Fails on | Exact term matches | N/A |\n| Performance | Faster (single vector) | Slightly slower (two searches) |\n| Use Case | General knowledge | Technical, precise, or hybrid queries |\n\nIn this stack, the **qdrant-rag MCP server** uses hybrid RRF for active retrieval, while **AnythingLLM** uses dense-only for passive retrieval. This creates a **format-divergent** system: the MCP pipeline excels at finding exact identifiers (e.g., port numbers, model names), while AnythingLLM excels at semantic understanding.",
    "word_count": 378,
    "grade": "FAIL",
    "reason": "Contains ### headers (not allowed for explanation tier); Over word limit: 378 words (max 150)",
    "has_headers": true,
    "has_bullets": true
  },
  {
    "id": "E5",
    "query": "How does AnythingLLM inject RAG context into a query?",
    "mode": "query",
    "response": "AnythingLLM injects RAG context into a query automatically through a **passive, dense-only retrieval pipeline** that runs on every relevant message in **chat mode**. The process is fully transparent and requires no explicit tool call from the agent:\n\n1. **Trigger**: When a user sends a query, AnythingLLM checks if it is \"relevant\" — i.e., if it could benefit from document retrieval (e.g., \"What port is Qdrant on?\", \"How does hybrid search work?\").\n\n2. **RAG Pipeline Activation**: The system queries the **Qdrant vector database** on port 6333 using **BGE-M3 dense embeddings** (1024-dimensional, ONNX INT8 on CPU). The search uses **cosine similarity** to find the most semantically similar document chunks.\n\n3. **Chunk Retrieval**: The system retrieves up to **16 document snippets** from the `anyloom_docs` collection. Each chunk is **6,600 characters long** with **1,000-character overlap** to preserve context continuity.\n\n4. **Context Injection**: The retrieved chunks are inserted into the system message **after a `Context:` separator**, forming a **pre-prompt** that the model sees before generating a response. This is done **automatically** and **without any user or agent action**.\n\n5. **Retrieval Settings**:\n   - **Similarity threshold**: Low (to maximize recall).\n   - **Chunk size**: 6,600 characters (~1,650 tokens).\n   - **Overlap**: 1,000 characters.\n   - **History**: Up to 30 messages of conversation context are included.\n\n6. **Citation and Priority**:\n   - The agent is instructed to **treat this injected context as its primary source of truth**.\n   - If the context conflicts with training data, the context wins.\n   - The agent **must cite specific values** from the chunks (e.g., \"Per the architecture reference: port 6333 serves workspace RAG\").\n\nThis system is **fully passive** — no `rag_search` call is needed. It is optimized for **semantic recall** and **natural language understanding**, making it ideal for general knowledge queries. However, it **cannot** retrieve exact keyword matches (e.g., \"What is the port number?\") as reliably as hybrid search, which is why the **qdrant-rag MCP server** uses active, hybrid RRF retrieval for technical precision.",
    "word_count": 317,
    "grade": "FAIL",
    "reason": "Over word limit: 317 words (max 150)",
    "has_headers": false,
    "has_bullets": true
  }
]