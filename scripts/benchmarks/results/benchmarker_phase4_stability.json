{
  "runs": {
    "D3": [
      {
        "run": 1,
        "word_count": 548,
        "text": "The **decision cascade** is a tiered, first-match system that determines how the agent selects the most appropriate tool or information source for a given query, progressing from the cheapest and fastest to the most expensive and specialized. It ensures efficient, cost-effective responses by prioritizing lower-cost options before escalating to higher-cost ones.\n\nThe cascade operates in **agent mode** (activated by `@agent`) and applies to all queries requiring external information. It consists o"
      },
      {
        "run": 2,
        "word_count": 548,
        "text": "The **decision cascade** is a tiered, first-match system that determines how the agent selects the most appropriate tool or information source for a given query, progressing from the cheapest and fastest to the most expensive and specialized. It ensures efficient, cost-effective responses by prioritizing lower-cost options before escalating to higher-cost ones.\n\nThe cascade operates in **agent mode** (activated by `@agent`) and applies to all queries requiring external information. It consists o"
      },
      {
        "run": 3,
        "word_count": 548,
        "text": "The **decision cascade** is a tiered, first-match system that determines how the agent selects the most appropriate tool or information source for a given query, progressing from the cheapest and fastest to the most expensive and specialized. It ensures efficient, cost-effective responses by prioritizing lower-cost options before escalating to higher-cost ones.\n\nThe cascade operates in **agent mode** (activated by `@agent`) and applies to all queries requiring external information. It consists o"
      }
    ],
    "D4": [
      {
        "run": 1,
        "word_count": 262,
        "text": "RRF stands for **Reciprocal Rank Fusion**, a technique used to combine multiple ranked result lists into a single unified ranking. It is employed in the hybrid search pipeline of the qdrant-rag MCP server to merge results from both dense semantic search and sparse keyword search.\n\n### How RRF Works:\n- For each document in the result lists, RRF computes a **final score** as the sum of the reciprocals of its rank in each list:\n  $$\n  \\text{Final Score} = \\frac{1}{\\text{rank}_{\\text{dense}}} + \\fra"
      },
      {
        "run": 2,
        "word_count": 262,
        "text": "RRF stands for **Reciprocal Rank Fusion**, a technique used to combine multiple ranked result lists into a single unified ranking. It is employed in the hybrid search pipeline of the qdrant-rag MCP server to merge results from both dense semantic search and sparse keyword search.\n\n### How RRF Works:\n- For each document in the result lists, RRF computes a **final score** as the sum of the reciprocals of its rank in each list:\n  $$\n  \\text{Final Score} = \\frac{1}{\\text{rank}_{\\text{dense}}} + \\fra"
      },
      {
        "run": 3,
        "word_count": 262,
        "text": "RRF stands for **Reciprocal Rank Fusion**, a technique used to combine multiple ranked result lists into a single unified ranking. It is employed in the hybrid search pipeline of the qdrant-rag MCP server to merge results from both dense semantic search and sparse keyword search.\n\n### How RRF Works:\n- For each document in the result lists, RRF computes a **final score** as the sum of the reciprocals of its rank in each list:\n  $$\n  \\text{Final Score} = \\frac{1}{\\text{rank}_{\\text{dense}}} + \\fra"
      }
    ],
    "D5": [
      {
        "run": 1,
        "word_count": 90,
        "text": "This stack uses the BGE-M3 embedding model for both dense and sparse vector generation in its hybrid search pipeline. The dense embeddings are computed via the BGE-M3 ONNX INT8 model running on CPU, which serves the /v1/embeddings endpoint at port 8008. The same model is used by both the AnythingLLM workspace agent (for dense-only retrieval) and the qdrant-rag MCP server (for hybrid dense-plus-sparse search with Reciprocal Rank Fusion). The model consumes approximately 0.6 GB of RAM with zero VR"
      },
      {
        "run": 2,
        "word_count": 90,
        "text": "This stack uses the BGE-M3 embedding model for both dense and sparse vector generation in its hybrid search pipeline. The dense embeddings are computed via the BGE-M3 ONNX INT8 model running on CPU, which serves the /v1/embeddings endpoint at port 8008. The same model is used by both the AnythingLLM workspace agent (for dense-only retrieval) and the qdrant-rag MCP server (for hybrid dense-plus-sparse search with Reciprocal Rank Fusion). The model consumes approximately 0.6 GB of RAM with zero VR"
      },
      {
        "run": 3,
        "word_count": 90,
        "text": "This stack uses the BGE-M3 embedding model for both dense and sparse vector generation in its hybrid search pipeline. The dense embeddings are computed via the BGE-M3 ONNX INT8 model running on CPU, which serves the /v1/embeddings endpoint at port 8008. The same model is used by both the AnythingLLM workspace agent (for dense-only retrieval) and the qdrant-rag MCP server (for hybrid dense-plus-sparse search with Reciprocal Rank Fusion). The model consumes approximately 0.6 GB of RAM with zero VR"
      }
    ],
    "D6": [
      {
        "run": 1,
        "word_count": 48,
        "text": "AnythingLLM's RAG pipeline uses **6,600-character chunks** with **1,000-character overlap**. These chunks are generated using a recursive text splitter that respects paragraph boundaries, ensuring semantic coherence while maintaining context continuity across adjacent segments. This configuration supports dense-only semantic retrieval via BGE-M3 embeddings and is optimized for natural language understanding."
      },
      {
        "run": 2,
        "word_count": 48,
        "text": "AnythingLLM's RAG pipeline uses **6,600-character chunks** with **1,000-character overlap**. These chunks are generated using a recursive text splitter that respects paragraph boundaries, ensuring semantic coherence while maintaining context continuity across adjacent segments. This configuration supports dense-only semantic retrieval via BGE-M3 embeddings and is optimized for natural language understanding."
      },
      {
        "run": 3,
        "word_count": 48,
        "text": "AnythingLLM's RAG pipeline uses **6,600-character chunks** with **1,000-character overlap**. These chunks are generated using a recursive text splitter that respects paragraph boundaries, ensuring semantic coherence while maintaining context continuity across adjacent segments. This configuration supports dense-only semantic retrieval via BGE-M3 embeddings and is optimized for natural language understanding."
      }
    ],
    "D7": [
      {
        "run": 1,
        "word_count": 375,
        "text": "The Memory knowledge graph is a core component of the **decision cascade**, serving as **Tier 1** — the second-fastest and most reliable source after automatic RAG context (Tier 0). It is prioritized for stable, structured facts that are critical to system operation and architecture.\n\n### Role in the Decision Cascade:\n1. **Tier 1: Memory Knowledge Graph**  \n   - **When Checked**: After Tier 0 (automatic RAG context) fails to provide a sufficient answer, the agent performs a `search_nodes` query "
      },
      {
        "run": 2,
        "word_count": 375,
        "text": "The Memory knowledge graph is a core component of the **decision cascade**, serving as **Tier 1** — the second-fastest and most reliable source after automatic RAG context (Tier 0). It is prioritized for stable, structured facts that are critical to system operation and architecture.\n\n### Role in the Decision Cascade:\n1. **Tier 1: Memory Knowledge Graph**  \n   - **When Checked**: After Tier 0 (automatic RAG context) fails to provide a sufficient answer, the agent performs a `search_nodes` query "
      },
      {
        "run": 3,
        "word_count": 375,
        "text": "The Memory knowledge graph is a core component of the **decision cascade**, serving as **Tier 1** — the second-fastest and most reliable source after automatic RAG context (Tier 0). It is prioritized for stable, structured facts that are critical to system operation and architecture.\n\n### Role in the Decision Cascade:\n1. **Tier 1: Memory Knowledge Graph**  \n   - **When Checked**: After Tier 0 (automatic RAG context) fails to provide a sufficient answer, the agent performs a `search_nodes` query "
      }
    ],
    "D8": [
      {
        "run": 1,
        "word_count": 246,
        "text": "The llama.cpp agent (running in @agent mode) has access to the **nine containerized MCP tools** provided by the Docker MCP Gateway. These tools are **not** part of AnythingLLM’s native Agent Skills and are exclusively available through the MCP interface. They are:\n\n1. **Desktop Commander** – Executes operating system commands (e.g., `docker ps`, `nvidia-smi`, `systemctl status`) and manages running processes, containers, and system health.\n2. **Filesystem** – Reads, writes, searches, and lists f"
      },
      {
        "run": 2,
        "word_count": 246,
        "text": "The llama.cpp agent (running in @agent mode) has access to the **nine containerized MCP tools** provided by the Docker MCP Gateway. These tools are **not** part of AnythingLLM’s native Agent Skills and are exclusively available through the MCP interface. They are:\n\n1. **Desktop Commander** – Executes operating system commands (e.g., `docker ps`, `nvidia-smi`, `systemctl status`) and manages running processes, containers, and system health.\n2. **Filesystem** – Reads, writes, searches, and lists f"
      },
      {
        "run": 3,
        "word_count": 246,
        "text": "The llama.cpp agent (running in @agent mode) has access to the **nine containerized MCP tools** provided by the Docker MCP Gateway. These tools are **not** part of AnythingLLM’s native Agent Skills and are exclusively available through the MCP interface. They are:\n\n1. **Desktop Commander** – Executes operating system commands (e.g., `docker ps`, `nvidia-smi`, `systemctl status`) and manages running processes, containers, and system health.\n2. **Filesystem** – Reads, writes, searches, and lists f"
      }
    ],
    "D9": [
      {
        "run": 1,
        "word_count": 459,
        "text": "Here is a side-by-side comparison of the **two RAG pipelines** in this stack: the **AnythingLLM workspace agent** (passive, dense-only) and the **qdrant-rag MCP server** (active, hybrid RRF), based on chunking strategy, embedding format, and retrieval mechanism.\n\n| Feature | **AnythingLLM Workspace Agent** | **qdrant-rag MCP Server** |\n|--------|-------------------------------|---------------------------|\n| **Chunking Strategy** | Recursive text splitting with **6,600-character chunks** and **1,"
      },
      {
        "run": 2,
        "word_count": 459,
        "text": "Here is a side-by-side comparison of the **two RAG pipelines** in this stack: the **AnythingLLM workspace agent** (passive, dense-only) and the **qdrant-rag MCP server** (active, hybrid RRF), based on chunking strategy, embedding format, and retrieval mechanism.\n\n| Feature | **AnythingLLM Workspace Agent** | **qdrant-rag MCP Server** |\n|--------|-------------------------------|---------------------------|\n| **Chunking Strategy** | Recursive text splitting with **6,600-character chunks** and **1,"
      },
      {
        "run": 3,
        "word_count": 459,
        "text": "Here is a side-by-side comparison of the **two RAG pipelines** in this stack: the **AnythingLLM workspace agent** (passive, dense-only) and the **qdrant-rag MCP server** (active, hybrid RRF), based on chunking strategy, embedding format, and retrieval mechanism.\n\n| Feature | **AnythingLLM Workspace Agent** | **qdrant-rag MCP Server** |\n|--------|-------------------------------|---------------------------|\n| **Chunking Strategy** | Recursive text splitting with **6,600-character chunks** and **1,"
      }
    ],
    "D10": [
      {
        "run": 1,
        "word_count": 337,
        "text": "If `rag_search` returns zero results on port 6334, the agent should **not** proceed to the next tier of the decision cascade — instead, it must **diagnose the root cause** before escalating.\n\n### Why?\nThe decision cascade assumes that **each tier is a valid, working source**. If `rag_search` fails (returns zero results), it does **not** mean the answer is missing — it means the **system is misconfigured or degraded**. Escalating to Tavily or Fetch without diagnosing the failure would be a **fail"
      },
      {
        "run": 2,
        "word_count": 337,
        "text": "If `rag_search` returns zero results on port 6334, the agent should **not** proceed to the next tier of the decision cascade — instead, it must **diagnose the root cause** before escalating.\n\n### Why?\nThe decision cascade assumes that **each tier is a valid, working source**. If `rag_search` fails (returns zero results), it does **not** mean the answer is missing — it means the **system is misconfigured or degraded**. Escalating to Tavily or Fetch without diagnosing the failure would be a **fail"
      },
      {
        "run": 3,
        "word_count": 337,
        "text": "If `rag_search` returns zero results on port 6334, the agent should **not** proceed to the next tier of the decision cascade — instead, it must **diagnose the root cause** before escalating.\n\n### Why?\nThe decision cascade assumes that **each tier is a valid, working source**. If `rag_search` fails (returns zero results), it does **not** mean the answer is missing — it means the **system is misconfigured or degraded**. Escalating to Tavily or Fetch without diagnosing the failure would be a **fail"
      }
    ]
  },
  "analysis": {
    "D3": {
      "min": 548,
      "max": 548,
      "avg": 548.0,
      "spread": 0,
      "verdict": "Deterministic"
    },
    "D4": {
      "min": 262,
      "max": 262,
      "avg": 262.0,
      "spread": 0,
      "verdict": "Deterministic"
    },
    "D5": {
      "min": 90,
      "max": 90,
      "avg": 90.0,
      "spread": 0,
      "verdict": "Deterministic"
    },
    "D6": {
      "min": 48,
      "max": 48,
      "avg": 48.0,
      "spread": 0,
      "verdict": "Deterministic"
    },
    "D7": {
      "min": 375,
      "max": 375,
      "avg": 375.0,
      "spread": 0,
      "verdict": "Deterministic"
    },
    "D8": {
      "min": 246,
      "max": 246,
      "avg": 246.0,
      "spread": 0,
      "verdict": "Deterministic"
    },
    "D9": {
      "min": 459,
      "max": 459,
      "avg": 459.0,
      "spread": 0,
      "verdict": "Deterministic"
    },
    "D10": {
      "min": 337,
      "max": 337,
      "avg": 337.0,
      "spread": 0,
      "verdict": "Deterministic"
    }
  }
}