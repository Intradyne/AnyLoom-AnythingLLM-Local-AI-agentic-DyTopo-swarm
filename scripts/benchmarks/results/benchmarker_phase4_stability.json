{
  "runs": {
    "D3": [
      {
        "run": 1,
        "word_count": 209,
        "text": "The decision cascade is a tiered system that routes queries from the cheapest to the most expensive information source, ensuring efficient and cost-effective responses. It operates as a first-match system: the agent uses the highest tier that can answer the question and only descends to lower tiers when higher tiers produce insufficient results.  \n\n- **Tier 0**: Automatic RAG context — the Qdrant instance (`anyloom-qdrant` on port 6333) retrieves relevant document chunks and injects them into th"
      },
      {
        "run": 2,
        "word_count": 209,
        "text": "The decision cascade is a tiered system that routes queries from the cheapest to the most expensive information source, ensuring efficient and cost-effective responses. It operates as a first-match system: the agent uses the highest tier that can answer the question and only descends to lower tiers when higher tiers produce insufficient results.  \n\n- **Tier 0**: Automatic RAG context — the Qdrant instance (`anyloom-qdrant` on port 6333) retrieves relevant document chunks and injects them into th"
      },
      {
        "run": 3,
        "word_count": 209,
        "text": "The decision cascade is a tiered system that routes queries from the cheapest to the most expensive information source, ensuring efficient and cost-effective responses. It operates as a first-match system: the agent uses the highest tier that can answer the question and only descends to lower tiers when higher tiers produce insufficient results.  \n\n- **Tier 0**: Automatic RAG context — the Qdrant instance (`anyloom-qdrant` on port 6333) retrieves relevant document chunks and injects them into th"
      }
    ],
    "D4": [
      {
        "run": 1,
        "word_count": 100,
        "text": "RRF (Reciprocal Rank Fusion) is a technique that fuses multiple ranked result lists into a single unified ranking by scoring each document as the reciprocal of its rank in each list. In this stack, RRF combines dense semantic search results (from BGE-M3 embeddings) and sparse keyword search results (from TF-weighted term frequency) to improve retrieval accuracy. This hybrid approach ensures that documents with high semantic relevance or exact keyword matches are prioritized, enhancing recall for"
      },
      {
        "run": 2,
        "word_count": 100,
        "text": "RRF (Reciprocal Rank Fusion) is a technique that fuses multiple ranked result lists into a single unified ranking by scoring each document as the reciprocal of its rank in each list. In this stack, RRF combines dense semantic search results (from BGE-M3 embeddings) and sparse keyword search results (from TF-weighted term frequency) to improve retrieval accuracy. This hybrid approach ensures that documents with high semantic relevance or exact keyword matches are prioritized, enhancing recall for"
      },
      {
        "run": 3,
        "word_count": 100,
        "text": "RRF (Reciprocal Rank Fusion) is a technique that fuses multiple ranked result lists into a single unified ranking by scoring each document as the reciprocal of its rank in each list. In this stack, RRF combines dense semantic search results (from BGE-M3 embeddings) and sparse keyword search results (from TF-weighted term frequency) to improve retrieval accuracy. This hybrid approach ensures that documents with high semantic relevance or exact keyword matches are prioritized, enhancing recall for"
      }
    ],
    "D5": [
      {
        "run": 1,
        "word_count": 88,
        "text": "This stack uses the BGE-M3 embedding model for both dense and sparse vector generation in hybrid search. The dense vectors are computed via the BGE-M3 ONNX INT8 model on CPU (0.6 GB RAM, 0 VRAM) for the qdrant-rag MCP server's hybrid RRF pipeline. AnythingLLM uses the same BGE-M3 model in GGUF Q8_0 format through the llama.cpp /v1/embeddings endpoint for its dense-only retrieval. Both implementations serve the same Qdrant instance on port 6333, with the ONNX version handling CPU-based embeddings"
      },
      {
        "run": 2,
        "word_count": 88,
        "text": "This stack uses the BGE-M3 embedding model for both dense and sparse vector generation in hybrid search. The dense vectors are computed via the BGE-M3 ONNX INT8 model on CPU (0.6 GB RAM, 0 VRAM) for the qdrant-rag MCP server's hybrid RRF pipeline. AnythingLLM uses the same BGE-M3 model in GGUF Q8_0 format through the llama.cpp /v1/embeddings endpoint for its dense-only retrieval. Both implementations serve the same Qdrant instance on port 6333, with the ONNX version handling CPU-based embeddings"
      },
      {
        "run": 3,
        "word_count": 88,
        "text": "This stack uses the BGE-M3 embedding model for both dense and sparse vector generation in hybrid search. The dense vectors are computed via the BGE-M3 ONNX INT8 model on CPU (0.6 GB RAM, 0 VRAM) for the qdrant-rag MCP server's hybrid RRF pipeline. AnythingLLM uses the same BGE-M3 model in GGUF Q8_0 format through the llama.cpp /v1/embeddings endpoint for its dense-only retrieval. Both implementations serve the same Qdrant instance on port 6333, with the ONNX version handling CPU-based embeddings"
      }
    ],
    "D6": [
      {
        "run": 1,
        "word_count": 63,
        "text": "AnythingLLM's RAG pipeline uses 6,600-character chunks with 1,000-character overlap, split using a recursive text splitter that respects paragraph boundaries. This configuration ensures that document content is broken into manageable segments while preserving contextual continuity across chunk boundaries. The chunks are embedded via the BGE-M3 model in GGUF Q8_0 format through the llama.cpp /v1/embeddings endpoint and stored in the `anyloom_docs` collection on port 6333."
      },
      {
        "run": 2,
        "word_count": 63,
        "text": "AnythingLLM's RAG pipeline uses 6,600-character chunks with 1,000-character overlap, split using a recursive text splitter that respects paragraph boundaries. This configuration ensures that document content is broken into manageable segments while preserving contextual continuity across chunk boundaries. The chunks are embedded via the BGE-M3 model in GGUF Q8_0 format through the llama.cpp /v1/embeddings endpoint and stored in the `anyloom_docs` collection on port 6333."
      },
      {
        "run": 3,
        "word_count": 63,
        "text": "AnythingLLM's RAG pipeline uses 6,600-character chunks with 1,000-character overlap, split using a recursive text splitter that respects paragraph boundaries. This configuration ensures that document content is broken into manageable segments while preserving contextual continuity across chunk boundaries. The chunks are embedded via the BGE-M3 model in GGUF Q8_0 format through the llama.cpp /v1/embeddings endpoint and stored in the `anyloom_docs` collection on port 6333."
      }
    ],
    "D7": [
      {
        "run": 1,
        "word_count": 145,
        "text": "The Memory knowledge graph is the second tier in the decision cascade, positioned after automatic RAG context (Tier 0). It serves as the primary source for stable, long-term facts—such as port mappings (port 8008 for llama.cpp inference, port 6333 for Qdrant), configuration values (RAG_GPU_THREADS=8, EMBED_BATCH_SIZE=16), architecture decisions (why hybrid search is used for the MCP pipeline and dense-only for AnythingLLM), and resolved error patterns. The agent checks Memory first for any query"
      },
      {
        "run": 2,
        "word_count": 145,
        "text": "The Memory knowledge graph is the second tier in the decision cascade, positioned after automatic RAG context (Tier 0). It serves as the primary source for stable, long-term facts—such as port mappings (port 8008 for llama.cpp inference, port 6333 for Qdrant), configuration values (RAG_GPU_THREADS=8, EMBED_BATCH_SIZE=16), architecture decisions (why hybrid search is used for the MCP pipeline and dense-only for AnythingLLM), and resolved error patterns. The agent checks Memory first for any query"
      },
      {
        "run": 3,
        "word_count": 145,
        "text": "The Memory knowledge graph is the second tier in the decision cascade, positioned after automatic RAG context (Tier 0). It serves as the primary source for stable, long-term facts—such as port mappings (port 8008 for llama.cpp inference, port 6333 for Qdrant), configuration values (RAG_GPU_THREADS=8, EMBED_BATCH_SIZE=16), architecture decisions (why hybrid search is used for the MCP pipeline and dense-only for AnythingLLM), and resolved error patterns. The agent checks Memory first for any query"
      }
    ],
    "D8": [
      {
        "run": 1,
        "word_count": 213,
        "text": "The llama.cpp agent (via the Docker MCP Gateway) has access to nine containerized MCP tools, which are exclusive to the MCP system and not available to AnythingLLM. These tools are:\n\n1. **Desktop Commander** – executes OS-level commands (e.g., `docker ps`, `nvidia-smi`, `systemctl`) and manages processes, containers, and system health.\n2. **Filesystem** – reads, writes, searches, and lists files and directories (e.g., `read file`, `find file`, `list directory`).\n3. **Memory** – reads and writes "
      },
      {
        "run": 2,
        "word_count": 213,
        "text": "The llama.cpp agent (via the Docker MCP Gateway) has access to nine containerized MCP tools, which are exclusive to the MCP system and not available to AnythingLLM. These tools are:\n\n1. **Desktop Commander** – executes OS-level commands (e.g., `docker ps`, `nvidia-smi`, `systemctl`) and manages processes, containers, and system health.\n2. **Filesystem** – reads, writes, searches, and lists files and directories (e.g., `read file`, `find file`, `list directory`).\n3. **Memory** – reads and writes "
      },
      {
        "run": 3,
        "word_count": 213,
        "text": "The llama.cpp agent (via the Docker MCP Gateway) has access to nine containerized MCP tools, which are exclusive to the MCP system and not available to AnythingLLM. These tools are:\n\n1. **Desktop Commander** – executes OS-level commands (e.g., `docker ps`, `nvidia-smi`, `systemctl`) and manages processes, containers, and system health.\n2. **Filesystem** – reads, writes, searches, and lists files and directories (e.g., `read file`, `find file`, `list directory`).\n3. **Memory** – reads and writes "
      }
    ],
    "D9": [
      {
        "run": 1,
        "word_count": 337,
        "text": "| Feature | AnythingLLM RAG Pipeline | qdrant-rag MCP Server Pipeline |\n|--------|----------------------------|-------------------------------|\n| **Chunking Strategy** | Recursive text splitting with 6,600-character chunks and 1,000-character overlap, preserving paragraph boundaries. Chunks are not aligned to section headers. | Deterministic section-header-based splitting (e.g., `##` headers) with zero overlap. Chunks are aligned to structural document elements, ensuring precise semantic boundar"
      },
      {
        "run": 2,
        "word_count": 337,
        "text": "| Feature | AnythingLLM RAG Pipeline | qdrant-rag MCP Server Pipeline |\n|--------|----------------------------|-------------------------------|\n| **Chunking Strategy** | Recursive text splitting with 6,600-character chunks and 1,000-character overlap, preserving paragraph boundaries. Chunks are not aligned to section headers. | Deterministic section-header-based splitting (e.g., `##` headers) with zero overlap. Chunks are aligned to structural document elements, ensuring precise semantic boundar"
      },
      {
        "run": 3,
        "word_count": 337,
        "text": "| Feature | AnythingLLM RAG Pipeline | qdrant-rag MCP Server Pipeline |\n|--------|----------------------------|-------------------------------|\n| **Chunking Strategy** | Recursive text splitting with 6,600-character chunks and 1,000-character overlap, preserving paragraph boundaries. Chunks are not aligned to section headers. | Deterministic section-header-based splitting (e.g., `##` headers) with zero overlap. Chunks are aligned to structural document elements, ensuring precise semantic boundar"
      }
    ],
    "D10": [
      {
        "run": 1,
        "word_count": 249,
        "text": "The agent should not proceed to port 6334 — it is not a valid endpoint for rag_search. The qdrant-rag MCP server uses port 6333 (not 6334) for all RAG operations. Port 6334 is the gRPC port for the Qdrant container (`anyloom-qdrant`) and is not accessible via HTTP or the MCP server's API.\n\nIf `rag_search` returns zero results on port 6333, the agent follows the decision cascade by descending to Tier 2: **Context7** (via `resolve-library-id` → `get-library-docs`), then Tier 3: **Tavily**, and fin"
      },
      {
        "run": 2,
        "word_count": 249,
        "text": "The agent should not proceed to port 6334 — it is not a valid endpoint for `rag_search`. The qdrant-rag MCP server uses port 6333 (not 6334) for all RAG operations. Port 6334 is the gRPC port for the Qdrant container (`anyloom-qdrant`) and is not accessible via HTTP or the MCP server's API.\n\nIf `rag_search` returns zero results on port 6333, the agent follows the decision cascade by descending to Tier 2: **Context7** (via `resolve-library-id` → `get-library-docs`), then Tier 3: **Tavily**, and f"
      },
      {
        "run": 3,
        "word_count": 249,
        "text": "The agent should not proceed to port 6334 — it is not a valid endpoint for `rag_search`. The qdrant-rag MCP server uses port 6333 (not 6334) for all RAG operations. Port 6334 is the gRPC port for the Qdrant container (`anyloom-qdrant`) and is not accessible via HTTP or the MCP server's API.\n\nIf `rag_search` returns zero results on port 6333, the agent follows the decision cascade by descending to Tier 2: **Context7** (via `resolve-library-id` → `get-library-docs`), then Tier 3: **Tavily**, and f"
      }
    ]
  },
  "analysis": {
    "D3": {
      "min": 209,
      "max": 209,
      "avg": 209.0,
      "spread": 0,
      "verdict": "Deterministic"
    },
    "D4": {
      "min": 100,
      "max": 100,
      "avg": 100.0,
      "spread": 0,
      "verdict": "Deterministic"
    },
    "D5": {
      "min": 88,
      "max": 88,
      "avg": 88.0,
      "spread": 0,
      "verdict": "Deterministic"
    },
    "D6": {
      "min": 63,
      "max": 63,
      "avg": 63.0,
      "spread": 0,
      "verdict": "Deterministic"
    },
    "D7": {
      "min": 145,
      "max": 145,
      "avg": 145.0,
      "spread": 0,
      "verdict": "Deterministic"
    },
    "D8": {
      "min": 213,
      "max": 213,
      "avg": 213.0,
      "spread": 0,
      "verdict": "Deterministic"
    },
    "D9": {
      "min": 337,
      "max": 337,
      "avg": 337.0,
      "spread": 0,
      "verdict": "Deterministic"
    },
    "D10": {
      "min": 249,
      "max": 249,
      "avg": 249.0,
      "spread": 0,
      "verdict": "Deterministic"
    }
  }
}