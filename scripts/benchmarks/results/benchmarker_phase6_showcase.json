[
  {
    "id": "S1",
    "label": "Perfect Lookup",
    "query": "What is BGE-M3?",
    "mode": "query",
    "response": "BGE-M3 is a multi-granularity embedding model developed by BAAI (Beijing Academy of Artificial Intelligence). It is designed to produce both dense and sparse vectors from a single input, enabling hybrid search capabilities.\n\nKey features of BGE-M3 include:\n\n1.  **Multi-Granularity:** It can generate embeddings at different levels of granularity, from word-level to sentence-level to document-level, making it versatile for various retrieval tasks.\n2.  **Dual Output:** It produces both dense vectors (for semantic similarity search) and sparse vectors (for keyword-based matching). This allows a single model to power both dense-only and hybrid search pipelines.\n3.  **Hybrid Search:** The model is specifically optimized for use with Reciprocal Rank Fusion (RRF), a technique that combines the results of dense semantic search and sparse keyword search into a single, more accurate ranking. This makes it highly effective for information retrieval tasks where both meaning and exact keywords are important.\n4.  **Efficiency:** BGE-M3 is designed to be efficient, with a relatively small model size (e.g., 22 million parameters for the MiniLM-L6-v2 variant used in DyTopo) while maintaining high performance.\n\nIn this stack, BGE-M3 is the foundational model for both RAG pipelines. The GGUF Q8_0 version is used for dense-only search in AnythingLLM, while the FlagEmbedding version is used for hybrid search in LM Studio.",
    "word_count": 208
  },
  {
    "id": "S2",
    "label": "Price Fabrication Guard",
    "query": "What's gold trading at right now?",
    "mode": "chat",
    "response": "This requires a live tool call â€” use @agent mode to get current data.",
    "word_count": 14
  },
  {
    "id": "S3",
    "label": "Tool Boundary Awareness",
    "query": "Search Memory for all entities related to Qdrant",
    "mode": "chat",
    "response": "search_nodes(\"Qdrant\")",
    "word_count": 1
  },
  {
    "id": "S4",
    "label": "Deep Architecture Knowledge",
    "query": "Walk me through the full architecture of this stack, from user query to Qdrant retrieval and back",
    "mode": "query",
    "response": "The full architecture of this stack, from a user query to Qdrant retrieval and back, is a multi-layered system with distinct roles for each component. Here's a step-by-step walkthrough:\n\n1.  **User Query Input:**\n    *   The user sends a query to one of the two frontends: the AnythingLLM web interface (localhost:3001) or the LM Studio interface.\n\n2.  **Frontend Processing:**\n    *   **AnythingLLM:** The query is sent to the AnythingLLM workspace agent, which is a local agent running on the same machine.\n    *   **LM Studio:** The query is sent to the LM Studio agent, which is also a local agent.\n\n3.  **Agent Decision and Routing:**\n    *   The agent (either AnythingLLM or LM Studio) uses the **decision cascade** to determine the best path.\n    *   **For AnythingLLM:** The agent checks the **automatic RAG context** (Tier 0). If the answer is in the workspace documents, it is retrieved from the Qdrant instance on **port 6333** and injected into the system message after a \"Context:\" separator. This is a **passive, dense-only** retrieval.\n    *   **For LM Studio:** The agent checks the **Memory knowledge graph** (Tier 1). If the answer is not found, it may call the **rag_search** tool (Tier 3) to perform an **active, hybrid** search on the Qdrant instance on **port 6334**.\n\n4.  **Embedding Generation:**\n    *   If a retrieval is needed, the query (and any retrieved document chunks) must be embedded into vectors.\n    *   Both agents connect to the **LM Studio API endpoint** at **port 1234** (`http://localhost:1234/v1/embeddings`) to generate embeddings using the **BGE-M3** model.\n\n5.  **Qdrant Retrieval:**\n    *   **Port 6333 (AnythingLLM):** The query vector is compared against the dense vectors of the 6,600-character document chunks stored in the AnythingLLM Qdrant instance. The top 16 results are retrieved based on cosine similarity.\n    *   **Port 6334 (LM Studio):** The query vector is used for a **hybrid search**. The system performs both a dense search (using BGE-M3) and a sparse search (using learned lexical weights) and combines the results using **Reciprocal Rank Fusion (RRF)** to create a single, unified ranking.\n\n6.  **Result Processing and Response:**\n    *   The retrieved document chunks are sent back to the agent.\n    *   The agent uses the context from the retrieved chunks to generate a final response.\n    *   The response is sent back to the user through the respective frontend.\n\n7.  **System State and Persistence:**\n    *   The **Memory knowledge graph** is used by both agents to store and retrieve stable facts (e.g., port mappings, configuration decisions).\n    *   The **Docker MCP Gateway** provides the LM Studio agent with access to system-level tools (like Desktop Commander and Filesystem) for debugging and configuration, which the AnythingLLM agent does not have access to.\n\nIn summary, the stack features two independent RAG pipelines: AnythingLLM uses passive, dense-only retrieval on port 6333, while LM Studio uses active, hybrid retrieval on port 6334. Both rely on the same BGE-M3 model for embeddings, which are generated via the shared LM Studio API endpoint at port 1234.",
    "word_count": 482
  },
  {
    "id": "S5",
    "label": "Explanation-Tier Comparison",
    "query": "How does AnythingLLM's RAG pipeline differ from the MCP qdrant-rag server?",
    "mode": "query",
    "response": "AnythingLLM's RAG pipeline and the MCP qdrant-rag server differ fundamentally in their architecture, retrieval mechanism, and operational model:\n\n1.  **Retrieval Mechanism:**\n    *   **AnythingLLM:** Uses **passive, dense-only retrieval**. Document chunks are automatically injected into the system message after a \"Context:\" separator on every relevant query. This happens without any explicit tool call from the agent.\n    *   **MCP qdrant-rag server:** Uses **active, hybrid retrieval**. The agent must explicitly call the `rag_search` tool to retrieve documents. This search combines dense semantic matching (BGE-M3) with sparse lexical matching (keyword weights) and fuses the results using Reciprocal Rank Fusion (RRF).\n\n2.  **Architecture and Data:**\n    *   **AnythingLLM:** Operates on a dedicated Qdrant instance (port 6333) with independent data volumes and collection configuration. It uses 6,600-character chunks with 1,000-character overlap, processed by a recursive text splitter.\n    *   **MCP qdrant-rag server:** Operates on a separate Qdrant instance (port 6334) with its own data volumes and collection (`lmstudio_docs`). It uses deterministic section-header-based chunking with zero overlap, which allows for precise source filtering.\n\n3.  **Operational Model:**\n    *   **AnythingLLM:** The retrieval is automatic and requires no agent action. The agent's response is built from the pre-injected context.\n    *   **MCP qdrant-rag server:** The retrieval is a deliberate, explicit tool call. The agent must initiate the search, specify parameters like result limits or source filters, and wait for the results before proceeding.\n\n4.  **Tool Access:**\n    *   **AnythingLLM:** The agent has no direct access to the MCP qdrant-rag server's tools (like `rag_search`, `rag_status`, `rag_reindex`). It can only use its own built-in Agent Skills.\n    *   **MCP qdrant-rag server:** The agent has full access to the `rag_search` tool and its related tools (`rag_status`, `rag_reindex`, `rag_sources`, `rag_file_info`) through the Docker MCP Gateway.\n\n5.  **System Integration:**\n    *   **AnythingLLM:** The RAG pipeline is tightly integrated into the AnythingLLM workspace's document processing and web interface (localhost:3001).\n    *   **MCP qdrant-rag server:** The RAG pipeline is a separate service managed by the qdrant-rag MCP server, which is a native Python application (`qdrant_mcp_server.py`) that runs on the host machine.\n\nIn essence, AnythingLLM's pipeline is a **passive, automatic, and isolated** system, while the MCP qdrant-rag server's pipeline is an **active, explicit, and shared** system.",
    "word_count": 350
  },
  {
    "id": "S6",
    "label": "RAG-Grounded Citation",
    "query": "What chunking strategy does this workspace use?",
    "mode": "query",
    "response": "This workspace uses a **recursive text splitter** for document chunking.\n\nThe specific configuration is:\n*   **Chunk Size:** 6,600 characters.\n*   **Overlap:** 1,000 characters.\n*   **Splitting Logic:** The splitter respects paragraph boundaries, ensuring that chunks are broken at natural paragraph breaks rather than mid-sentence.\n\nThis strategy is used by the AnythingLLM workspace's RAG pipeline, which stores documents in the Qdrant instance on port 6333. The same workspace also uses a separate, deterministic section-header-based chunking strategy for the LM Studio agent's RAG pipeline (port 6334), which splits documents at every `##` header with zero overlap.",
    "word_count": 93
  },
  {
    "id": "S7",
    "label": "Multi-Fact Numeric Recall",
    "query": "What are the memory limits for each Qdrant container?",
    "mode": "query",
    "response": "The memory limits for the two Qdrant containers are as follows:\n\n*   **AnythingLLM Qdrant Container (port 6333):** This container is configured with a **4 GB memory limit**.\n*   **LM Studio Qdrant Container (port 6334):** The documentation does not specify a memory limit for this container. However, it is noted that the BGE-M3 FlagEmbedding model used by the qdrant-rag MCP server consumes approximately 2.3 GB of system RAM, and the MiniLM-L6-v2 model used by DyTopo consumes approximately 80 MB of system RAM.",
    "word_count": 81
  }
]