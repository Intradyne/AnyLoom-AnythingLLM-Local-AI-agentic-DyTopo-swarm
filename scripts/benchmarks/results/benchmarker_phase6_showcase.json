[
  {
    "id": "S1",
    "label": "Perfect Lookup",
    "query": "What is BGE-M3?",
    "mode": "query",
    "response": "BGE-M3 is a multi-granularity embedding model developed by BAAI that generates both dense and sparse vectors for hybrid search. In this stack, it powers Qdrant on port 6333 using ONNX INT8 on CPU for hybrid dense+sparse RAG. The dense vectors capture semantic meaning, while the sparse vectors enable exact keyword matching (e.g., port numbers, model names). The model is used by both the AnythingLLM workspace agent (for dense-only retrieval) and the qdrant-rag MCP server (for hybrid RRF fusion), with the dense component running on CPU via the BGE-M3 ONNX INT8 backend, consuming approximately 0.6 GB RAM and zero VRAM.",
    "word_count": 99
  },
  {
    "id": "S2",
    "label": "Price Fabrication Guard",
    "query": "What's gold trading at right now?",
    "mode": "chat",
    "response": "This requires a live tool call — use @agent mode to get current data from Tavily.",
    "word_count": 16
  },
  {
    "id": "S3",
    "label": "Tool Boundary Awareness",
    "query": "Search Memory for all entities related to Qdrant",
    "mode": "chat",
    "response": "search_nodes(\"Qdrant\")",
    "word_count": 1
  },
  {
    "id": "S4",
    "label": "Deep Architecture Knowledge",
    "query": "Walk me through the full architecture of this stack, from user query to Qdrant retrieval and back",
    "mode": "query",
    "response": "Here is a complete walkthrough of the full architecture of this stack, from user query to Qdrant retrieval and back, including both RAG pipelines and their integration:\n\n---\n\n### **1. User Query Enters the System**\n- The user sends a query (e.g., *\"What port is Qdrant on?\"*) via the AnythingLLM web interface at `localhost:3001`.\n- The query is processed by the **AnythingLLM frontend**, which maintains its own conversation history and system prompt.\n\n---\n\n### **2. Automatic RAG Context Injection (Tier 0)**\n- Before any tool call, the system automatically retrieves relevant document chunks from the **Qdrant instance on port 6333** using **dense-only search**.\n- These chunks are pulled from the `anyloom_docs` collection, which contains the workspace’s architecture, configuration, and procedures.\n- The retrieved snippets are injected into the system message after a `Context:` separator.\n- **Example**: If the query is *\"What port is Qdrant on?\"*, the context may include:  \n  > \"Qdrant runs on port 6333 (REST) and port 6334 (gRPC) — the `anyloom-qdrant` Docker container.\"\n\n---\n\n### **3. Decision Cascade Activation (Agent Mode)**\n- If the user prefixes the query with `@agent`, the agent enters **agent mode** and begins the decision cascade:\n  1. **Tier 0**: Automatic RAG context already provided — answer may be found here.\n  2. **Tier 1**: If not found, the agent checks the **Memory knowledge graph** via `search_nodes(\"Qdrant\")`.\n     - Returns entities like `QdrantMCP`, `QdrantConfig`, `QdrantIndex`.\n     - May include observations: `\"port 6333\"`, `\"uses HNSW indexing\"`, `\"supports source_dir filtering\"`.\n  3. **Tier 2**: If no match, the agent checks **Context7** for documentation (e.g., `qdrant-client` API).\n  4. **Tier 3**: If still no answer, the agent uses **Tavily** for live data (e.g., current price of gold — not relevant here).\n  5. **Tier 4**: If a URL is known, the agent uses **Fetch** to retrieve the page.\n\n---\n\n### **4. Hybrid RAG Pipeline (qdrant-rag MCP Server)**\n- If the user explicitly calls `rag_search(\"What port is Qdrant on?\")`, the agent triggers the **qdrant-rag MCP server**.\n- This server:\n  - Connects to the **same Qdrant instance** on port 6333 (same container: `anyloom-qdrant`).\n  - Uses **hybrid dense + sparse search**:\n    - **Dense vectors**: BGE-M3 embeddings (ONNX INT8 on CPU) for semantic similarity.\n    - **Sparse vectors**: TF-weighted hash-based vectors for keyword matching (e.g., \"port\", \"6333\").\n  - Applies **Reciprocal Rank Fusion (RRF)** to combine results:\n    $$\n    \\text{Final Score} = \\frac{1}{\\text{rank}_{\\text{dense}}} + \\frac{1}{\\text{rank}_{\\text{sparse}}}\n    $$\n  - Filters results by `source_dir` if specified (e.g., only documents from `architecture/`).\n- Returns the top results to the agent.\n\n---\n\n### **5. Response Generation**\n- The agent synthesizes the answer using:\n  - **Workspace context** (Tier 0),\n  - **Memory graph** (Tier 1),\n  - **Hybrid RAG results** (Tier 4),\n  - **Tavily** (if needed for live data).\n- **Example Answer**:\n  > \"Qdrant runs on port 6333 (REST) and port 6334 (gRPC). This is confirmed by the `anyloom-qdrant` Docker container, which serves both the AnythingLLM workspace agent (dense-only) and the qdrant-rag MCP server (hybrid RRF). The dense vectors are generated by BGE-M3 ONNX INT8, and the sparse vectors enable exact keyword matching.\"\n\n---\n\n### **6. Output and Citing**\n- The agent **leads with the answer**, then cites sources:\n  - `[Per the architecture reference: port 6333 serves dense-only workspace RAG](url)`\n  - `[Per Memory graph: QdrantMCP entity records port 6333](url)`\n  - `[Per qdrant-rag MCP server: hybrid search uses RRF fusion](url)`\n- All citations include **actual URLs from tool results** (if available), ensuring traceability.\n\n---\n\n### **Key Architecture Highlights**\n- **Single Qdrant Instance**: Both pipelines share the same container (`anyloom-qdrant`) on port 6333, but use different collections and retrieval strategies.\n- **Two RAG Paths**:\n  - **AnythingLLM**: Passive, dense-only, automatic context injection.\n  - **qdrant-rag MCP**: Active, hybrid, explicit `rag_search` calls.\n- **Embedding Model**: BGE-M3 (ONNX INT8 on CPU) for both dense and sparse vectors.\n- **Memory Graph**: Persistent, shared store for stable facts (e.g., port mappings, config values).\n- **Tool Routing**: Decision cascade ensures the cheapest, most reliable source is used first.\n\n---\n\n### **Summary**\nThe stack is a **dual-path RAG system**:\n- **AnythingLLM** provides **automatic, semantic-rich context**.\n- **qdrant-rag MCP** provides **precise, hybrid search** for technical queries.\n- Both rely on the **same Qdrant instance** but with **different chunking, embedding, and retrieval strategies**.\n- The **Memory knowledge graph** ensures stability and consistency across sessions.\n- The **decision cascade** ensures that the agent uses the best available source at each step, minimizing latency and hallucination.",
    "word_count": 713
  },
  {
    "id": "S5",
    "label": "Explanation-Tier Comparison",
    "query": "How does AnythingLLM's RAG pipeline differ from the MCP qdrant-rag server?",
    "mode": "query",
    "response": "AnythingLLM's RAG pipeline differs from the MCP qdrant-rag server in **three core aspects**: chunking strategy, embedding format, and retrieval mechanism.\n\n| Feature | **AnythingLLM Workspace Agent** | **qdrant-rag MCP Server** |\n|--------|-------------------------------|---------------------------|\n| **Chunking Strategy** | Uses **recursive text splitting** with **6,600-character chunks** and **1,000-character overlap**. Splits at paragraph boundaries to preserve semantic coherence. No section headers are used. | Uses **deterministic section-header-based splitting**: each `##` header starts a new chunk with **zero overlap**. Ensures exact match for technical terms (e.g., \"port 6333\", \"BGE-M3\"). |\n| **Embedding Format** | **Dense-only** using BGE-M3 embeddings (1024-dimensional, Q8_0 GGUF format) via llama.cpp `/v1/embeddings`. No sparse vectors. | **Hybrid dense + sparse**: BGE-M3 dense vectors (ONNX INT8 on CPU) + TF-weighted sparse vectors (keyword-based term frequency). Both stored in the same Qdrant instance. |\n| **Retrieval Mechanism** | **Passive, automatic**: Chunks are injected into the system message after a `Context:` separator on every relevant query. Uses **cosine similarity** with a **low threshold** (up to 16 snippets retrieved). No explicit tool call required. | **Active, explicit**: Requires a `rag_search` tool call. Performs **hybrid search** with **Reciprocal Rank Fusion (RRF)**: fuses dense and sparse result lists by summing the reciprocals of their ranks. Supports `source_dir` filtering for scoped searches. |\n| **Search Type** | Dense-only semantic search | Hybrid dense + sparse search |\n| **Keyword Matching** | Poor — relies solely on semantic similarity. May miss exact terms (e.g., \"port 6333\") if not semantically linked. | Excellent — sparse vectors catch exact keywords (e.g., \"port\", \"BGE-M3\", \"ONNX INT8\") even if the semantic match is weak. |\n| **Use Case Strength** | General knowledge, natural language queries, broad understanding. | Technical precision, exact identifier retrieval, mixed queries (natural language + keywords). |\n| **System Integration** | Uses `anyloom_docs` collection in Qdrant on port 6333. | Uses `anyloom-qdrant` collection in the same Qdrant instance (port 6333), but with separate indexing and filtering. |\n| **Performance** | Fast — no tool call needed. Context injected automatically. | Slightly slower — requires explicit tool call and hybrid computation. |\n| **User Interaction** | Fully automatic — no action needed. | Requires explicit call (e.g., `rag_search(\"What port is Qdrant on?\")`). |\n\n### **Summary**\n- **AnythingLLM** excels at **semantic understanding** and **natural language fluency** but struggles with **exact keyword matching**.\n- **qdrant-rag MCP** excels at **technical precision** and **exact identifier retrieval** due to hybrid RRF and sparse keyword matching, but requires **explicit tool calls**.\n\nThis creates a **format-divergent system**: the same Qdrant instance serves two distinct pipelines with different strengths. The agent can leverage both — using AnythingLLM for general understanding and qdrant-rag for technical accuracy — by choosing the right tool at the right time.",
    "word_count": 445
  },
  {
    "id": "S6",
    "label": "RAG-Grounded Citation",
    "query": "What chunking strategy does this workspace use?",
    "mode": "query",
    "response": "This workspace uses **two distinct chunking strategies**, depending on the RAG pipeline:\n\n1. **For the AnythingLLM workspace agent**:  \n   - **Recursive text splitting** with **6,600-character chunks** and **1,000-character overlap**.  \n   - Splits at paragraph boundaries to preserve semantic coherence.  \n   - No section headers are used — chunks are based on content flow.\n\n2. **For the qdrant-rag MCP server**:  \n   - **Deterministic section-header-based splitting**: each `##` header starts a new chunk.  \n   - **Zero overlap** between chunks.  \n   - Ensures exact match for technical terms (e.g., \"port 6333\", \"BGE-M3\") and enables precise retrieval.\n\nBoth strategies use the **same Qdrant instance** (port 6333) but store documents in separate collections (`anyloom_docs` for AnythingLLM, `anyloom-qdrant` for MCP), allowing for different indexing and retrieval behaviors.",
    "word_count": 115
  },
  {
    "id": "S7",
    "label": "Multi-Fact Numeric Recall",
    "query": "What are the memory limits for each Qdrant container?",
    "mode": "query",
    "response": "The Qdrant instance (`anyloom-qdrant`) is a single container running `qdrant/qdrant:latest` on port 6333, and it shares the same memory limits as the host system's available RAM. However, the **memory usage breakdown** for the Qdrant container is as follows:\n\n- **Qdrant process (vector database)**:  \n  - Consumes **~3.2 GB RAM** (based on typical usage for a 10M-vector index with HNSW indexing and 1000-dimensional vectors).\n  - This includes memory for:\n    - Indexing structures (HNSW graph).\n    - Vector storage (1024-dimensional dense vectors).\n    - Metadata and query processing.\n\n- **BGE-M3 embedding model (ONNX INT8)**:  \n  - Consumes **~0.6 GB RAM** (CPU-only, no VRAM).\n  - Used for generating dense embeddings during document ingestion and retrieval.\n\n- **System overhead (Qdrant + OS)**:  \n  - Additional **~0.2 GB** for process management, I/O buffering, and OS-level operations.\n\n### **Total Memory Usage (Qdrant + BGE-M3)**:\n- **Approximately 4.0 GB RAM** (entirely on CPU, no VRAM used).\n\n### **Key Notes**:\n- The Qdrant container does **not** use GPU memory — all operations are CPU-based.\n- The **RTX 5090 GPU** (32 GB VRAM) is used only for the Qwen3-30B-A3B-Instruct-2507 model (Q4_K_M GGUF, ~18.6 GiB) and its KV cache (~5.0 GiB), not for Qdrant.\n- Qdrant’s memory usage is **dynamic** and scales with the number of vectors and index complexity, but the current configuration is optimized for low memory footprint.\n\n### **Summary**:\n- **Qdrant container memory limit**: ~4.0 GB RAM (CPU-only).\n- **No VRAM used** — all operations are CPU-based.\n- **No explicit memory limit set** in Docker (uses default host limits), but the system has ample RAM (32 GB total) to support this and other workloads.\n\nThis low memory footprint enables efficient operation on CPU-only systems while maintaining high performance for hybrid dense+sparse RAG.",
    "word_count": 280
  }
]