[
  {
    "id": "S1",
    "label": "Perfect Lookup",
    "query": "What is BGE-M3?",
    "mode": "query",
    "response": "BGE-M3 is a multi-granularity embedding model from BAAI that produces both dense and sparse vectors for hybrid search. In this stack, BGE-M3 embeddings power Qdrant on port 6333 using ONNX INT8 on CPU for hybrid dense+sparse RAG (per architecture reference).",
    "word_count": 40
  },
  {
    "id": "S2",
    "label": "Price Fabrication Guard",
    "query": "What's gold trading at right now?",
    "mode": "chat",
    "response": "I don't have reliable price data — use @agent mode to get current data from Tavily.",
    "word_count": 16
  },
  {
    "id": "S3",
    "label": "Tool Boundary Awareness",
    "query": "Search Memory for all entities related to Qdrant",
    "mode": "chat",
    "response": "search_nodes(\"Qdrant\")",
    "word_count": 1
  },
  {
    "id": "S4",
    "label": "Deep Architecture Knowledge",
    "query": "Walk me through the full architecture of this stack, from user query to Qdrant retrieval and back",
    "mode": "query",
    "response": "The full architecture of this stack operates as a dual-path RAG system, with two distinct pipelines — one passive (AnythingLLM) and one active (qdrant-rag MCP server) — both leveraging the same Qdrant instance on port 6333. Here's the end-to-end flow:\n\n1. **User Query Input**  \n   The user sends a query via the AnythingLLM web interface at `localhost:3001`. This triggers the system to begin processing.\n\n2. **Automatic RAG Context Injection (AnythingLLM Pipeline)**  \n   - The query is routed to the **AnythingLLM workspace agent**, which automatically retrieves relevant document chunks from the `anyloom_docs` collection in Qdrant on port 6333 using **dense-only cosine similarity search**.  \n   - This retrieval is passive: no tool call is required. The system injects up to 16 chunks (each ~500 tokens) into the system message after a \"Context:\" separator, using BGE-M3 embeddings in GGUF Q8_0 format.  \n   - The chunks are 6,600-character segments with 1,000-character overlap, split recursively while preserving paragraph boundaries.\n\n3. **Active RAG via MCP Server (qdrant-rag MCP Server Pipeline)**  \n   - If the query requires deeper or more precise retrieval (e.g., technical, structured, or keyword-based), the agent may call `rag_search` explicitly in agent mode.  \n   - This triggers the **qdrant-rag MCP server** (running natively on the host), which connects to the same Qdrant instance on port 6333 but uses a different collection (`anyloom-qdrant`) with **hybrid dense+sparse search**.  \n   - The query is processed using **BGE-M3 embeddings in ONNX INT8 format** for dense vectors and **TF-weighted hash-based sparse vectors** for keyword matching.  \n   - Results are fused via **Reciprocal Rank Fusion (RRF)**, combining rankings from both components to improve recall for queries mixing natural language and exact identifiers.  \n   - Source filtering via `source_dir` is supported (e.g., `source=\"architecture.md\"`), enabling scoped searches.\n\n4. **Response Generation**  \n   - The agent synthesizes evidence from both pipelines:  \n     - AnythingLLM’s passive RAG provides broad semantic context.  \n     - The MCP server’s active RAG provides precision, structure, and keyword-level accuracy.  \n   - The final response is generated using the llama.cpp model (Qwen3-30B-A3B-Instruct-2507) via the `/v1/chat/completions` endpoint on port 8008, with the full context from both RAG pipelines and conversation history.\n\n5. **Output and Feedback Loop**  \n   - The response is returned to the user via the AnythingLLM UI at `localhost:3001`.  \n   - The agent may update the **Memory knowledge graph** with new facts (e.g., configuration decisions, resolved issues) for future use.\n\n**Key Infrastructure**:\n- **Port 6333**: Single Qdrant container (`anyloom-qdrant`) serving both pipelines via different collections and retrieval strategies.\n- **Port 8008**: llama.cpp inference backend (Qwen3-30B-A3B-Instruct-2507) for LLM inference.\n- **Port 3001**: AnythingLLM web UI for user interaction.\n- **MCP Tools**: 9 containerized tools (e.g., Desktop Commander, Filesystem, Tavily) accessible only in agent mode.\n\nThis architecture enables a balance between conversational fluency (AnythingLLM) and technical precision (MCP server), with all data stored in a shared, persistent Qdrant instance.",
    "word_count": 452
  },
  {
    "id": "S5",
    "label": "Explanation-Tier Comparison",
    "query": "How does AnythingLLM's RAG pipeline differ from the MCP qdrant-rag server?",
    "mode": "query",
    "response": "AnythingLLM's RAG pipeline and the MCP qdrant-rag server differ in three core aspects: **retrieval mechanism, chunking strategy, and access model**.\n\n1. **Retrieval Mechanism**  \n   - **AnythingLLM**: Uses **dense-only semantic search** with BGE-M3 embeddings (GGUF Q8_0) via cosine similarity. It is **passive** — chunks from the `anyloom_docs` collection are injected automatically into the system message on every relevant query, requiring no tool call.  \n   - **qdrant-rag MCP server**: Uses **hybrid dense-plus-sparse search** with Reciprocal Rank Fusion (RRF). It combines BGE-M3 dense embeddings (ONNX INT8 on CPU) with TF-weighted sparse vectors for keyword matching. It is **active** — requires an explicit `rag_search` tool call to retrieve results.\n\n2. **Chunking Strategy**  \n   - **AnythingLLM**: Employs **recursive text splitting** with 6,600-character chunks and 1,000-character overlap, preserving paragraph boundaries. Chunks are not aligned to section headers.  \n   - **qdrant-rag MCP server**: Uses **deterministic section-header-based splitting** (e.g., `##` headers) with zero overlap. Chunks are aligned to structural document elements, ensuring precise semantic boundaries.\n\n3. **Access and Use Case**  \n   - **AnythingLLM**: Designed for **conversational fluency** and broad semantic recall. Ideal for natural language queries where context is more important than precision.  \n   - **qdrant-rag MCP server**: Designed for **technical precision** and exact term matching. Ideal for queries involving specific identifiers (e.g., port numbers, tool names) or structured document sections, especially when using `source_dir` filtering.\n\nBoth pipelines use the same Qdrant instance on port 6333 but operate on separate collections (`anyloom_docs` vs. `anyloom-qdrant`) with different indexing strategies. AnythingLLM prioritizes ease of use and natural interaction, while the MCP server emphasizes accuracy and structure.",
    "word_count": 250
  },
  {
    "id": "S6",
    "label": "RAG-Grounded Citation",
    "query": "What chunking strategy does this workspace use?",
    "mode": "query",
    "response": "This workspace uses two distinct chunking strategies, depending on the RAG pipeline:\n\n1. **AnythingLLM RAG Pipeline**: Employs **recursive text splitting** with 6,600-character chunks and 1,000-character overlap, preserving paragraph boundaries. Chunks are not aligned to section headers and are used for dense-only semantic search in the `anyloom_docs` collection.\n\n2. **qdrant-rag MCP Server Pipeline**: Uses **deterministic section-header-based splitting** (e.g., `##` headers) with zero overlap. Chunks are aligned to structural document elements, ensuring precise semantic boundaries, and are used for hybrid dense+sparse search in the `anyloom-qdrant` collection.\n\nBoth strategies serve different purposes: AnythingLLM prioritizes contextual continuity for conversational queries, while the MCP server emphasizes structural precision for technical and keyword-based retrieval.",
    "word_count": 108
  },
  {
    "id": "S7",
    "label": "Multi-Fact Numeric Recall",
    "query": "What are the memory limits for each Qdrant container?",
    "mode": "query",
    "response": "The Qdrant container (`anyloom-qdrant`) on port 6333 does not have a hard memory limit defined in the provided context. However, its memory usage is constrained by the system's overall resource allocation and the model's embedding requirements.\n\nThe **BGE-M3 embedding model** used for dense vector computation consumes approximately **0.6 GB of RAM** when running on CPU via the ONNX INT8 backend. This is the primary memory cost for the embedding pipeline.\n\nThe **Qdrant instance itself** manages vector storage and indexing in memory, but its exact memory footprint depends on the number of vectors, dimensionality (1024 for BGE-M3), and indexing strategy. The system is designed to operate within the 32 GB VRAM of the RTX 5090 GPU, with the Q4_K_M GGUF model weights (~18.6 GiB) and KV cache (~5.0 GiB for 131K context) being the dominant VRAM consumers.\n\nSince Qdrant runs on CPU for embedding computation and uses minimal VRAM for indexing, its memory usage is primarily governed by the size of the stored vectors and the number of concurrent queries. The system is optimized to handle large-scale retrieval efficiently, with no explicit memory cap mentioned in the documentation.\n\nFor precise memory limits, monitoring via `docker stats` or `nvidia-smi` is recommended.",
    "word_count": 199
  }
]