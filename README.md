# AnyLoom: AnythingLLM Local AI Agentic Stack

**A fully local, multi-agent AI system powered by Qwen3-30B-A3B MoE, BGE-M3 hybrid embeddings, and dual Qdrant RAG pipelines â€” orchestrated via DyTopo swarm intelligence and MCP tooling.**

---

## ðŸŒ Overview

AnyLoom transformsÂ **AnythingLLM**Â into a dynamic, self-optimizing multi-agent swarm using:

- **Hybrid RAG fusion**Â (dense + sparse retrieval)
- **Dual Qdrant pipelines**:
    - port`6333`Â â†’ AnythingLLM (dense-only RAG)
    - port`6334`Â â†’ LM Studio (hybrid dense+sparse RAG via MCP)
- **12 MCP tools**
- DyTopo swarm routing, memory, and agent coordination
- **Fully local execution**Â â€” no cloud dependencies, no data leakage
- No need to type @agent anymore, the swarm can use mcp tools
  
| Component                                          | Tokens                     |
| -------------------------------------------------- | -------------------------- |
| Total Token Budget                                 | 80k                        |
| System prompt                                      | ~2K                        |
| MCP tool definitions (~10 Docker tools)            | ~3K                        |
| RAG snippets (16 Ã— ~500 tokens)                    | ~8K/8192 embedding         |
| Chat history (30 messages)                         | ~12K                       |
|                             **Overhead Subtotal:** | **~25K**                   |
| **Remaining for chat**                             | **~55K**  ~200k characters |
The entire RAG-prompt set fits comfortably inside the token limit (two qdrants means I could have 16k worth of RAG and never miss a prompt)

> âœ… Runs on a single GPU (optimized for RTX 5090, but functional on smaller RAM pools)

---

## ðŸ› ï¸ Prerequisites

Ensure the following are installed and running:

|COMPONENT|VERSION / REQUIREMENT|
|---|---|
|**LM Studio**|Latest version (local server onÂ `:1234`)|
|**AnythingLLM**|v1.0+ (local UI)|
|**Docker Desktop**|Running with access to containers|
|**Python**|3.12+|
|**GPU**|RTX 5090 (recommended), or any capable GPU|

> ðŸ”Â **Note**: Memory (MCP for secrets, state, and agent persistence) isÂ **not shared across notebooks**Â â€” each workspace maintains its own context.

---

## ðŸš€ Quickstart

### 1. Start Qdrant Containers

```bash

bashCopy block
# AnythingLLM RAG (dense-only)
docker run -d --name anythingllm-qdrant \
  -p 6333:6333 \
  -v qdrant_anythingllm:/qdrant/storage \
  --restart always \
  --memory=4g \
  --cpus=4 \
  qdrant/qdrant:latest

# LM Studio RAG (hybrid dense+sparse via MCP)
docker run -d --name lmstudio-qdrant \
  -p 6334:6333 \
  -v qdrant_lmstudio:/qdrant/storage \
  --restart always \
  --memory=4g \
  --cpus=4 \
  qdrant/qdrant:latest
```

> ðŸ“Œ Access dashboards at:
> 
> - [http://localhost:6333](http://localhost:6333)Â (AnythingLLM)
> - [http://localhost:6334](http://localhost:6334)Â (LM Studio)

---

### 2. Load Models in LM Studio

Download the following GGUF models:

- **LLM**:Â `unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF`Â (Q6_K)
- **Embedding**:Â `ggml-org/bge-m3-Q8_0`
---

### 3. Install Dependencies

```bash

bashCopy block
pip install \
  FlagEmbedding \
  torch --index-url https://download.pytorch.org/whl/cpu \
  qdrant-client>=1.12.0 \
  mcp[cli]>=1.0.0 \
  sentence-transformers>=3.0 \
  networkx>=3.0 \
  openai>=1.40 \
  tenacity>=9.0 \
  json-repair>=0.39
```

---

### 4. Configure MCP

Copy the MCP config file to the correct location:

```bash

bashCopy block
# Replace with your actual path
cp lmstudio-mcp.json "C:\Users\User\.lmstudio\config\mcp.json"
```

> ðŸ”§Â **Important**: Update theÂ `mcp.json`Â path to match your installation directory.

---

### 5. Configure AnythingLLM

In the AnythingLLM UI:

- **LLM Endpoint**:Â `http://127.0.0.1:1234/v1`
- **Vector DB**:Â `http://127.0.0.1:6333`
- **Embedding Model**:Â `bge-m3`Â (viaÂ `http://127.0.0.1:1234/v1/embeddings`)

> ðŸ“„ SeeÂ [anythingllm-settings.md](anythingllm-settings.md)Â for full configuration.

---

### 6. Configure LM Studio

In LM Studio settings:

- **System Prompt**: Copy into the â€œMy Modelsâ€ tab (not Developer) to persist across sessions
- **JS Code Sandbox**: Disable built-in MCP server
- **RAGv1 Embedder**: Disable built-in MCP server

> ðŸ”„ Restart LM Studio after changes to verify tool registration.

---

### 7. Verify Setup

Run these checks:

|CHECK|COMMAND / URL|EXPECTED|
|---|---|---|
|LM Studio|`http://127.0.0.1:1234/v1`|Returns JSON (model loaded)|
|Qdrant (6333)|[http://localhost:6333](http://localhost:6333)|Dashboard accessible|
|Qdrant (6334)|[http://localhost:6334](http://localhost:6334)|Dashboard accessible|
|MCP Tools|Restart LM Studio|**8 tools**Â should appear (5 RAG + 3 DyTopo)|
|AnythingLLM|Create workspace, embed test doc, query|Success|

> âš ï¸ If containers arenâ€™t running, youâ€™ll get connection errors.

---

## ðŸ“š Documentation

Full reference documentation is available in theÂ `docs/`Â directory:

- `docs/01-architecture-reference.md`Â â€” Dual Qdrant, hybrid RAG, DyTopo routing
- `docs/02-mcp-tooling.md`Â â€” 12 MCP tools, memory, agent coordination
- `docs/03-agent-swarm.md`Â â€” Dynamic topology, task routing, self-optimization
- `docs/04-deployment-guide.md`Â â€” Troubleshooting, scaling, GPU tuning

> ðŸ“ŒÂ **All links in this README are verified and accessible via filesystem**.

---

## ðŸ”„ Access & Maintenance

- **Filesystem Access**: All configuration files, logs, and models are stored locally.
- **Container Management**: UseÂ `docker ps`,Â `docker logs`, andÂ `docker stop`Â for diagnostics.
- **Model Updates**: Re-download GGUF models in LM Studio when needed.
- **RAG Re-indexing**: Re-embed documents via AnythingLLM or MCP CLI.

---

## ðŸ§  Why AnyLoom?

- **No cloud dependency**Â â€” all data stays local
- **Hybrid RAG fusion**Â â€” better recall than pure dense or sparse
- **Dynamic agent swarm**Â â€” DyTopo routes tasks to optimal agents
- **MCP-powered memory**Â â€” persistent state, secrets, and agent history
- **100% local**Â â€” ideal for privacy, compliance, and offline use

---

> âœ…Â **Youâ€™re now running a next-gen, fully local AI agentic stack.**  
<<<<<<< HEAD
> ðŸš€ Start creating, querying, and orchestrating with AnyLoom today.
=======
> ðŸš€ Start creating, querying, and orchestrating with AnyLoom today.
>>>>>>> 9942e327ce1dc149abe142416c07aadc36c3deec
