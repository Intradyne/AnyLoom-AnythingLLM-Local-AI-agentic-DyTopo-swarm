# AnythingLLM Agent Tool Reference

This reference document describes the tools available to the AnythingLLM workspace agent and the broader tool ecosystem in the Loom stack, including the Sequential Thinking structured reasoning tool. Each section is self-contained and formatted for dense-only RAG retrieval on port 6333. The agent uses this information for cross-agent awareness and operational decision-making.


## What RAG tools does the qdrant-rag MCP server provide and how do they relate to the AnythingLLM workspace agent?

<!-- Verified: 2026-02-14 -->

The qdrant-rag MCP server is a native Python application (qdrant_mcp_server.py, approximately 1,820 lines) that provides five RAG (Retrieval-Augmented Generation) tools and three DyTopo swarm tools. These tools connect to the Qdrant vector database on port 6334 — the hybrid search instance that serves the LM Studio agent's reference document collection. These tools are configured in LM Studio's MCP tool inventory and are accessible exclusively through the LM Studio interface. The AnythingLLM workspace agent uses a completely separate retrieval mechanism — automatic dense-only search on port 6333 — and has no direct access to the qdrant-rag MCP server's tools. This section documents the RAG tools for cross-agent architectural awareness, helping the workspace agent understand the full system topology when answering questions about the stack.

The primary search tool is rag_search, which accepts a natural language query string, an optional limit parameter (1 to 10 results, defaulting to 5), and an optional source parameter for filtering results to a specific filename. The rag_search tool performs hybrid search combining dense semantic matching (using BGE-M3's 1024-dimensional embedding vectors computed via the FlagEmbedding library on CPU) with sparse lexical matching (keyword-based term frequency weights that catch exact terms like port numbers and tool names) through Reciprocal Rank Fusion (RRF), a technique that merges both ranked result lists into a single unified ranking. This hybrid approach means that queries phrased as complete sentences activate both pathways effectively — the dense component captures semantic meaning while the sparse component catches specific identifiers. Natural language questions retrieve better results than bare keywords because they engage the full embedding model. The source filter parameter is particularly useful when the agent knows which document contains the answer, for example passing source="architecture.md" to scope results to architectural information.

The rag_status tool takes no arguments and returns a health snapshot of the Qdrant collection: the endpoint URL (http://localhost:6334), the collection name (lmstudio_docs), the total point count, the indexing status, all configured document source directories with their file counts, and a staleness check that compares current file hashes against the last indexed state to detect files that have been modified since the last indexing run. The rag_reindex tool triggers a re-index of all configured document sources — by default it performs an incremental sync that only processes files that have changed (using SHA-256 content hashes stored in .rag_state.json), but passing force=True deletes the entire collection and rebuilds from scratch, which takes 30 to 60 seconds while BGE-M3 re-embeds the full document corpus on CPU. The rag_sources tool lists all configured document source directories with labels, filesystem paths, and file counts — the stack indexes documents from two source directories labeled "lmstudio" and "anythingllm" in the same Qdrant collection with per-source filtering via the source_dir payload index. The rag_file_info tool provides per-file details including the content hash used for incremental sync detection and the chunk count stored in Qdrant.

The AnythingLLM workspace agent's own RAG retrieval is fundamentally different from the qdrant-rag server's retrieval. AnythingLLM retrieves document chunks automatically — they appear in the system message after a "Context:" separator on every relevant query, using dense-only cosine similarity search against BGE-M3 Q8_0 GGUF embeddings computed through LM Studio's /v1/embeddings endpoint. The workspace agent uses 6,600-character chunks with 1,000-character overlap, retrieving up to 16 snippets at a low similarity threshold. By contrast, the qdrant-rag server on port 6334 uses deterministic section-header-based chunking with zero overlap, hybrid dense+sparse search with RRF fusion, and requires explicit tool calls from the LM Studio agent. Understanding this distinction helps the workspace agent accurately describe the system architecture when users ask about how document retrieval works across the stack.

<!-- Related: qdrant-rag MCP server, rag_search, rag_status, rag_reindex, rag_sources, rag_file_info, hybrid search, dense plus sparse, Reciprocal Rank Fusion, RRF, BGE-M3, FlagEmbedding, port 6334, lmstudio_docs collection, source filter, incremental sync, document indexing, LM Studio tools, cross-agent awareness, dense-only vs hybrid, automatic retrieval vs explicit search, 1024-dimensional vectors, sparse lexical weights, CPU embedding, SHA-256, rag_state.json, source_dir filtering -->


## What tools does the Docker MCP Gateway provide for system commands, file operations, web access, and structured reasoning?

<!-- Verified: 2026-02-14 -->

The Docker MCP Gateway (configured as MCP_DOCKER in the mcp.json configuration file) provides nine containerized tool servers that handle system-level operations, file management, web access, and structured reasoning. These tools run inside Docker containers and are accessed through LM Studio's MCP transport layer. The AnythingLLM workspace agent accesses these same nine tool servers through its own MCP configuration, making them the shared operational backbone for both agents in the Loom stack.

Desktop Commander executes operating system commands and manages running processes on the host machine through Docker volume mounts that provide read and write access to the host filesystem. This is the primary tool for running shell commands, checking system resource usage, inspecting Docker container status with docker ps and docker logs, starting or stopping services, verifying port availability, and performing any task that requires direct OS-level interaction. In practice, Desktop Commander follows a four-phase operational pattern: inspect current state first (run docker ps to see all containers), modify as needed (docker start to restart a stopped container), verify the change took effect (run docker ps again to confirm the container is up with correct port mappings). This inspect-modify-verify-log discipline ensures the agent reads state before changing it and confirms the result before reporting.

The Filesystem tool provides focused file operations: reading file contents, writing new files, creating directories, searching for files by name or content patterns, and moving or renaming files. When the task is specifically about file content (reading configs, writing documents, searching for patterns), call Filesystem rather than Desktop Commander.

Memory operates the shared knowledge graph — an entity-relation store that persists structured facts as named entities connected by typed relationships. Both the LM Studio agent and the AnythingLLM workspace agent access the same Memory graph through the same Docker-hosted Memory server, making it the coordination layer for persistent cross-session knowledge and cross-agent continuity. The agent creates entities to record discoveries such as port mappings, configuration decisions, and error resolutions, and queries the graph via search_nodes to recall previously established facts. Entity names use PascalCase (QdrantMCP, BGEm3Config, AnythingLLMWorkspace), entity types use snake_case (service_config, architecture_decision), and relation names use snake_case (serves, depends_on, replaced_by). Memory tools include search_nodes, create_entities, add_observations, create_relations, and open_nodes. The search-before-create discipline — calling search_nodes before creating entities to check for existing entries — prevents entity fragmentation that degrades retrieval quality for both agents.

Context7 retrieves up-to-date documentation for software libraries and APIs using a two-step workflow: resolve-library-id to find the library's identifier, then get-library-docs with that identifier and a topic-specific query. Context7 works for npm packages, PyPI packages, and docs.rs crates such as qdrant-client, FastMCP, sentence-transformers, or FlagEmbedding. The agent prefers Context7 over training knowledge for any API-specific question where the answer might have changed between versions. When Context7 returns documentation with a source URL, include it as a markdown link in the response.

Tavily is the mandatory first tool for any query where the answer changes over time: commodity prices, stock quotes, exchange rates, sports scores, weather, current events, and any query involving a tradeable asset or live data. Call Tavily BEFORE writing any answer text — generating an answer from training knowledge first creates an anchor that persists even when the tool returns different data. "Tell me about silver" in a context where pricing is relevant means call Tavily for the current price, then combine with general knowledge — not produce an essay and append a stale number. Tavily returns structured, concise results that answer factual queries in a single call — call Tavily with a concise query like "silver price today" rather than a vague "commodity information." Only cite Tavily as a source when Tavily was actually called in the current turn and the cited data came from its result — writing "per Tavily" for data that came from training knowledge is a hallucinated citation that destroys the user's ability to assess reliability. For general research on an unknown topic, call Tavily first, then call Fetch on promising URLs from the results for deeper analysis. Fetch retrieves content from a known URL and returns clean markdown — call Fetch for documentation pages, API references, and any URL where the content structure is predictable. Fetch returns pages in segments; when a truncated response already contains the answer, the agent delivers the answer immediately rather than requesting more content (requesting more wastes iterations). Three consecutive truncation cycles on the same URL signals the page is too large for Fetch — switch to Tavily or report what was found. Financial data sites (investing.com, bloomberg.com) are JavaScript-heavy and return poorly via Fetch — call Tavily for market data instead. For library and API documentation, the fallback chain is Context7 first, then Tavily, then Fetch on the official docs URL. When tools return URLs — from Tavily, Fetch, Context7, Playwright, or the Web Scraper — the agent includes the primary source URL inline in the response as a markdown link: `[source name](url)`. One link per source is sufficient; when a tool returns multiple URLs, pick the most authoritative. Playwright handles browser automation for JavaScript-heavy SPAs, login-protected dashboards, and forms — evaluate whether the target is static HTML (call Fetch) or requires JS interaction (call Playwright). Playwright consumes iterations rapidly — a typical navigate-login-read sequence uses 6 of 8 AIbitat iterations. The n8n tool handles workflow automation: call n8n_list_workflows, then n8n_execute_workflow by ID, then n8n_get_execution to poll completion.

AnythingLLM also provides a built-in Web Scraper — a native capability independent of the Docker MCP Gateway offering URL-to-text conversion with zero MCP dependency. Fetch returns cleaner markdown, but the Web Scraper works without Docker configuration. When the Web Scraper retrieves content from a URL, include that URL as a markdown link in the response.

<!-- Related: Docker MCP Gateway, Desktop Commander, Filesystem, Memory, Context7, Tavily, Fetch, Playwright, Sequential Thinking, n8n, shell commands, file operations, knowledge graph, web search, browser automation, Web Scraper, resolve-library-id, get-library-docs, search_nodes, PascalCase, snake_case, shared graph, cross-agent, docker ps, Tavily first choice, Tavily mandatory, call before answer, real-time data, time-sensitive query, tool-call-first, citation integrity, hallucinated citation, Fetch truncation, fallback chains, Tavily Fetch Playwright, Context7 Tavily Fetch -->


## What are DyTopo multi-agent swarm tools and how does the swarm system work across the Loom stack?

<!-- Verified: 2026-02-14 -->

The DyTopo multi-agent swarm system (Dynamic Topology, based on the research paper arXiv 2602.06039 titled "Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching" by Lu et al., February 2026) provides three tools for launching, monitoring, and retrieving results from collaborative reasoning tasks that run as background processes on the host machine. DyTopo creates a team of specialized AI agents that communicate through semantically-routed message passing — instead of broadcasting all agent outputs to all agents each round, DyTopo embeds agent descriptors using the MiniLM-L6-v2 sentence embedding model (a compact 22-million-parameter model consuming approximately 80 MB of system RAM on CPU), constructs a directed acyclic graph via cosine similarity thresholding, and executes agents in topological order so each agent receives only the messages that are semantically relevant to its current needs. These three tools — swarm_start, swarm_status, and swarm_result — are part of the qdrant-rag MCP server and are available exclusively through the LM Studio interface, not through AnythingLLM's MCP configuration. This section documents DyTopo for architectural awareness so the workspace agent can accurately describe the system's multi-agent capabilities when asked.

The swarm_start tool launches a new multi-agent swarm and immediately returns a task_id string for progress tracking. It accepts five parameters: the task description in natural language, the domain selecting which agent team to deploy, the tau routing threshold controlling communication density between agents, the k_in parameter capping incoming messages per agent per round, and max_rounds setting the upper bound on reasoning iterations. The domain parameter selects from three pre-configured agent teams. The code domain activates a Manager with Developer, Researcher, Tester, and Designer agents — this team is designed for programming tasks including code generation, debugging, architecture design, and code review, where each agent contributes its specialty (the Developer writes implementations, the Researcher analyzes requirements and algorithms, the Tester designs test cases and traces through code, and the Designer reviews code architecture and quality). The math domain activates a Manager with ProblemParser, Solver, and Verifier agents — designed for mathematical reasoning including proofs, calculations, and multi-step problem solving, where the Solver and Verifier independently work toward and check the answer. The general domain activates a Manager with Analyst, Critic, and Synthesizer agents — designed for open-ended analysis and multi-perspective reasoning, where the Critic challenges the Analyst's findings and the Synthesizer integrates both perspectives into a balanced conclusion.

The tau parameter (the routing threshold, defaulting to 0.3) is the primary control for inter-agent communication density during rounds 2 and beyond (round 1 always uses broadcast mode where all agents see all outputs). Lower tau values such as 0.1 to 0.2 create denser communication graphs where more agents receive each other's messages, producing richer collaboration at higher token cost. Higher tau values such as 0.4 to 0.6 create sparser routing with fewer inter-agent connections, leading to faster convergence and more independent agent work at the cost of potentially missing useful cross-agent insights. The k_in parameter (default 3, range 1 to 5) caps how many incoming messages each agent receives per round, preventing information overload. The max_rounds parameter (default 5, maximum 10) sets the upper bound on reasoning iterations — the Manager agent can terminate the swarm earlier by setting terminate=true when the task is solved satisfactorily.

The swarm_status tool accepts a task_id and returns the current progress: which round is active, which agent is currently executing, elapsed wall-clock time, and overall status (running, completed, or error). Because swarms execute as asynchronous background tasks while the LM Studio agent continues to respond to the user, the recommended polling interval is 15 to 30 seconds between swarm_status checks, as each round involves multiple LLM inference calls through the Qwen3 model at localhost port 1234. The swarm_result tool retrieves the final output once a swarm completes, including the Manager's synthesized final answer and optionally the per-round topology log showing which agents communicated with which and the semantic similarity scores that determined the routing. Swarm execution is fully isolated from the RAG pipeline — a DyTopo crash returns an error string and has zero impact on rag_search, rag_status, or other MCP tools. All DyTopo swarm agent calls route through the same LM Studio endpoint at localhost port 1234, sharing the Qwen3-30B-A3B model and KV cache with interactive chat from both frontends. This means that while a swarm is actively running, inference requests from both the LM Studio agent and the AnythingLLM workspace agent will queue behind swarm agent calls, potentially increasing response latency during active swarm execution.

<!-- Related: DyTopo, Dynamic Topology, multi-agent swarms, swarm_start, swarm_status, swarm_result, arXiv 2602.06039, tau threshold, routing threshold, k_in, max_rounds, code domain, math domain, general domain, Developer, Researcher, Tester, Designer, ProblemParser, Solver, Verifier, Analyst, Critic, Synthesizer, Manager agent, MiniLM-L6-v2, semantic routing, descriptor embedding, directed acyclic graph, topological order, broadcast mode, two-phase routing, background task, polling interval, LM Studio exclusive tools, inference queuing, temperature settings, qdrant-rag MCP server, port 1234, Qwen3 -->


## How Sequential Thinking works as a structured reasoning scratchpad and when to use it for complex planning

<!-- Verified: 2026-02-14 -->

The Sequential Thinking MCP server (accessed through the Docker MCP Gateway as part of the nine containerized tool servers) provides a structured multi-step reasoning scratchpad that the agent uses to decompose complex problems before taking action with other tools. The server exposes a single tool called sequentialthinking that accepts a thought (the content of the current reasoning step), a thought number (which step in the sequence), an estimated total number of thoughts, and a flag indicating whether another thought is needed after the current one. The tool also supports revision of earlier thoughts when new information contradicts a previous conclusion, branching to explore alternative reasoning paths from any previous thought, and extending the sequence beyond the original estimate when the problem proves more complex than initially expected. Each call to sequentialthinking returns the thought formatted with its position in the sequence, creating a visible chain of reasoning that the model and user can follow.

The Sequential Thinking tool enables five distinct reasoning patterns that map to common workspace agent tasks. Linear decomposition breaks a complex task into ordered sub-steps, where each thought builds on the previous one — useful for planning a multi-tool investigation sequence such as diagnosing why a service is unresponsive (step 1: identify the port, step 2: check container status, step 3: inspect logs, step 4: verify config, step 5: synthesize findings). Hypothesis-test reasoning proposes an explanation in one thought, designs a verification approach in the next, evaluates evidence in a third, and revises the hypothesis if evidence contradicts it — effective for debugging where the root cause is uncertain. Root-cause analysis starts with the observed symptom, systematically traces through potential causes, uses branching to explore alternative causal chains when multiple explanations are plausible, and converges on the actual root cause with supporting evidence. Comparison evaluation dedicates thoughts to each candidate option using consistent criteria, then synthesizes the trade-offs in a final thought — useful when the agent must recommend between approaches such as different configuration strategies or tool alternatives. Planning and sequencing defines the goal, identifies required resources and tools, orders the actions by dependency, and produces an execution plan — with the ability to extend the sequence if the plan grows beyond the initial estimate.

The Sequential Thinking tool works alongside other tools in a hybrid pattern that interleaves structured reasoning with concrete action. The most common multi-tool chain involving Sequential Thinking is the three-pass pattern with the Memory knowledge graph: first, Sequential Thinking plans the approach by decomposing the question into sub-parts and identifying a search strategy; second, Memory search_nodes retrieves existing entities and observations matching the planned queries; third, Qwen3 evaluates the Memory results and decides whether to enrich the graph (calling add_observations or create_entities to store new synthesis) or to deliver the answer directly. This three-pass pattern works because Sequential Thinking's output stays in the conversation context as an internal reasoning scratchpad — it is not shown to the user but guides the agent's subsequent tool calls. Beyond the Memory pattern, the agent interleaves Sequential Thinking with any action tools — Desktop Commander for shell commands, Filesystem for file operations, Context7 for documentation lookups — creating a visible reasoning chain where each thinking step references the evidence gathered by the previous action and adapts the plan based on what was revealed.

The agent selects Sequential Thinking when the task requires genuine multi-step reasoning before or between tool calls — tasks where jumping directly to tool execution would risk wasted effort or incorrect conclusions. Strong indicators for Sequential Thinking include debugging sequences spanning three or more components (where each diagnostic step depends on the previous result), architecture analysis requiring systematic evaluation of multiple factors, error diagnosis where the root cause is ambiguous and multiple hypotheses must be tested, comparing three or more approaches where trade-offs need structured evaluation, and planning multi-file changes or migration sequences where ordering matters. The agent skips Sequential Thinking for single-tool lookups (just call the tool directly), straightforward factual questions (use Memory search_nodes or let workspace RAG provide the answer), status checks (run the check via Desktop Commander), and simple file operations (use Filesystem directly) — adding a reasoning scratchpad to these tasks creates overhead without improving the result. Each thought step in a Sequential Thinking sequence costs approximately 100 to 300 tokens (the thought text itself plus the tool call overhead), so a typical 5-step thinking sequence adds roughly 500 to 1,500 tokens to the context. At 80,000 tokens with approximately 55,000 available for the current exchange, this cost is negligible for a single sequence, but the agent conserves budget by keeping individual thoughts concise and reserving revision and branching for situations where earlier reasoning genuinely needs correction rather than using them routinely.

<!-- Related: Sequential Thinking, sequentialthinking tool, structured reasoning, multi-step reasoning scratchpad, Docker MCP Gateway, reasoning patterns, linear decomposition, hypothesis-test, root-cause analysis, comparison evaluation, planning and sequencing, hybrid ReAct pattern, reasoning then acting, interleaved tool use, thought sequence, thought revision, thought branching, when to use Sequential Thinking, when to skip, token budget, debugging workflow, architecture analysis, error diagnosis, multi-tool planning, Desktop Commander, Filesystem, Memory, Context7, visible reasoning chain, structured deliberation, problem decomposition, trade-off analysis, diagnostic sequence, evidence-based reasoning -->
