# AnythingLLM Agent Tool Reference

This reference document describes the tools available to the AnythingLLM workspace agent and the broader tool ecosystem in the Loom stack, including the Sequential Thinking structured reasoning tool. Each section is self-contained and formatted for dense-only RAG retrieval on port 6333. The agent uses this information for cross-agent awareness and operational decision-making.


## What tools does AnythingLLM have versus what belongs to MCP only?

<!-- Verified: 2026-02-14 -->

**AnythingLLM Agent Skills** (available in @agent mode): Web Browsing (web search configured in admin settings), Web Scraper (URL-to-text), RAG (automatic from port 6333), Save File to Browser, Chart Generation, SQL Connector. These are built-in Agent Skills configured through the AnythingLLM admin UI at localhost:3001.

**MCP tools** (NOT available to AnythingLLM): Desktop Commander, Filesystem, Memory (shared read/write but accessed via MCP), Context7, Tavily, Fetch, Playwright, Sequential Thinking, n8n, rag_search, rag_status, rag_reindex, rag_sources, rag_file_info, swarm_start, swarm_status, swarm_result. These are MCP tools from the Docker MCP Gateway and qdrant-rag server, configured in mcp.json.

**Shared between both agents**: The Memory knowledge graph (both agents read and write the same local graph through the Docker MCP Gateway).

When asked what tools AnythingLLM has, list ONLY the Agent Skills above. Desktop Commander, Filesystem, Tavily, DyTopo swarm tools, and rag_search are MCP-exclusive — never attribute them to AnythingLLM.

<!-- Related: AnythingLLM tools, Agent Skills, MCP tools, tool boundary, Desktop Commander, Filesystem, Tavily, DyTopo, Memory shared, what tools does AnythingLLM have, what MCP tools are available, tool confusion, MCP vs Agent Skills -->


## What RAG tools does the qdrant-rag MCP server provide and how do they relate to the AnythingLLM workspace agent?

<!-- Verified: 2026-02-14 -->

The qdrant-rag MCP server is a native Python application (qdrant_mcp_server.py, approximately 1,030 lines) that provides five RAG (Retrieval-Augmented Generation) tools and three DyTopo swarm tools (the swarm tools delegate to the `src/dytopo/` package with 8 core modules and 5 sub-packages). These tools connect to the single Qdrant vector database (`anyloom-qdrant` on port 6333) using the hybrid search pipeline. These tools are configured in the MCP tool inventory and are accessible exclusively through the MCP interface. The AnythingLLM workspace agent uses a completely separate retrieval mechanism — automatic dense-only search on port 6333 using a different collection — and has no direct access to the qdrant-rag MCP server's tools. This section documents the RAG tools for cross-agent architectural awareness, helping the workspace agent understand the full system topology when answering questions about the stack.

The primary search tool is rag_search, which accepts a natural language query string, an optional limit parameter (1 to 10 results, defaulting to 5), and an optional source parameter for filtering results to a specific filename. The rag_search tool performs hybrid search combining dense semantic matching (using BGE-M3's 1024-dimensional embedding vectors computed via ONNX INT8 on CPU) with sparse lexical matching (keyword-based term frequency weights that catch exact terms like port numbers and tool names) through Reciprocal Rank Fusion (RRF), a technique that merges both ranked result lists into a single unified ranking. This hybrid approach means that queries phrased as complete sentences activate both pathways effectively — the dense component captures semantic meaning while the sparse component catches specific identifiers. Natural language questions retrieve better results than bare keywords because they engage the full embedding model. The source filter parameter is particularly useful when the agent knows which document contains the answer, for example passing source="architecture.md" to scope results to architectural information.

The rag_status tool takes no arguments and returns a health snapshot of the Qdrant collection: the endpoint URL (http://localhost:6333), the collection name, the total point count, the indexing status, all configured document source directories with their file counts, and a staleness check that compares current file hashes against the last indexed state to detect files that have been modified since the last indexing run. The rag_reindex tool triggers a re-index of all configured document sources — by default it performs an incremental sync that only processes files that have changed (using SHA-256 content hashes stored in .rag_state.json), but passing force=True deletes the entire collection and rebuilds from scratch, which takes 30 to 60 seconds while BGE-M3 re-embeds the full document corpus on CPU via ONNX INT8. The rag_sources tool lists all configured document source directories with labels, filesystem paths, and file counts — the stack indexes documents from configured source directories in the Qdrant collection with per-source filtering via the source_dir payload index. The rag_file_info tool provides per-file details including the content hash used for incremental sync detection and the chunk count stored in Qdrant.

The AnythingLLM workspace agent's own RAG retrieval is fundamentally different from the qdrant-rag server's retrieval. AnythingLLM retrieves document chunks automatically — they appear in the system message after a "Context:" separator on every relevant query, using dense-only cosine similarity search against BGE-M3 embeddings computed through the llama.cpp /v1/embeddings endpoint. The workspace agent uses 6,600-character chunks with 1,000-character overlap, retrieving up to 16 snippets at a low similarity threshold. By contrast, the qdrant-rag server uses deterministic section-header-based chunking with zero overlap, hybrid dense+sparse search with RRF fusion, and requires explicit MCP tool calls. Both pipelines query the same `anyloom-qdrant` instance on port 6333 but use separate collections. Understanding this distinction helps the workspace agent accurately describe the system architecture when users ask about how document retrieval works across the stack.

<!-- Related: qdrant-rag MCP server, rag_search, rag_status, rag_reindex, rag_sources, rag_file_info, hybrid search, dense plus sparse, Reciprocal Rank Fusion, RRF, BGE-M3, ONNX INT8, port 6333, anyloom-qdrant, anyloom_docs collection, source filter, incremental sync, document indexing, MCP tools, cross-agent awareness, dense-only vs hybrid, automatic retrieval vs explicit search, 1024-dimensional vectors, sparse lexical weights, CPU embedding, SHA-256, rag_state.json, source_dir filtering -->


## What MCP tools does the Docker MCP Gateway provide? (NOT AnythingLLM Agent Skills — for architectural awareness only)

<!-- Verified: 2026-02-14 -->

The Docker MCP Gateway (configured as MCP_DOCKER in the mcp.json configuration file) provides nine containerized MCP tool servers that handle system-level operations, file management, web access, and structured reasoning. These MCP tools run inside Docker containers and are accessed through the MCP transport layer. IMPORTANT: AnythingLLM does NOT use these MCP tools. AnythingLLM has its own built-in Agent Skills configured through the admin UI at localhost:3001, which are a completely different tool system from MCP. The nine MCP tools described below are MCP-exclusive. This section documents them for cross-system architectural awareness so the workspace agent can accurately describe the full stack when asked — but the AnythingLLM workspace agent cannot call these MCP tools.

Desktop Commander executes operating system commands and manages running processes on the host machine through Docker volume mounts that provide read and write access to the host filesystem. This is the primary tool for running shell commands, checking system resource usage, inspecting Docker container status with docker ps and docker logs, starting or stopping services, verifying port availability, and performing any task that requires direct OS-level interaction. In practice, Desktop Commander follows a four-phase operational pattern: inspect current state first (run docker ps to see all containers), modify as needed (docker start to restart a stopped container), verify the change took effect (run docker ps again to confirm the container is up with correct port mappings). This inspect-modify-verify-log discipline ensures the agent reads state before changing it and confirms the result before reporting.

The Filesystem tool (MCP Filesystem tool — for reading, writing, searching, and listing files. NOT Windows Explorer.) provides focused file operations: reading file contents, writing new files, creating directories, searching for files by name or content patterns, and moving or renaming files. When the task is specifically about file content (reading configs, writing documents, searching for patterns), call Filesystem rather than Desktop Commander.

Memory operates the local knowledge graph — an entity-relation store that persists structured facts as named entities connected by typed relationships. This is a local, private resource on this machine — the agent should use Memory eagerly and proactively as the fastest way to store and retrieve structured knowledge. All agents access the same Memory graph through the same Docker-hosted Memory server, making it the coordination layer for persistent cross-session knowledge and cross-agent continuity. The agent creates entities to record discoveries such as port mappings, configuration decisions, and error resolutions, and queries the graph via search_nodes to recall previously established facts. Entity names use PascalCase (QdrantMCP, BGEm3Config, AnythingLLMWorkspace), entity types use snake_case (service_config, architecture_decision), and relation names use snake_case (serves, depends_on, replaced_by). Memory tools include search_nodes, create_entities, add_observations, create_relations, and open_nodes. The search-before-create discipline — calling search_nodes before creating entities to check for existing entries — prevents entity fragmentation that degrades retrieval quality for both agents.

Context7 retrieves up-to-date documentation for software libraries and APIs using a two-step workflow: resolve-library-id to find the library's identifier, then get-library-docs with that identifier and a topic-specific query. Context7 works for npm packages, PyPI packages, and docs.rs crates such as qdrant-client, FastMCP, sentence-transformers, or onnxruntime. The agent prefers Context7 over training knowledge for any API-specific question where the answer might have changed between versions. When Context7 returns documentation with a source URL, include it as a markdown link in the response.

Tavily is the mandatory first tool for any query where the answer changes over time: commodity prices, stock quotes, exchange rates, sports scores, weather, current events, and any query involving a tradeable asset or live data (agent mode only — requires @agent). Call Tavily BEFORE writing any answer text — generating an answer from training knowledge first creates an anchor that persists even when the tool returns different data. In chat mode without @agent, tools are unavailable — when a time-sensitive query arrives in chat mode, state that live data requires @agent mode rather than generating a response from training data that mimics tool output format. In agent mode, present the tool-returned data with an inline source link following this format: "[Asset] is at [tool-returned price] ([Source name](tool-returned-url))." Fill every bracketed value exclusively from the Tavily result — never from training data, prior examples, or these reference documents. Tavily returns structured, concise results that answer factual queries in a single call — call Tavily with a concise query like "[asset] price today" rather than a vague "commodity information." Only cite Tavily as a source when Tavily was actually called in the current turn and the cited data came from its result — writing "per Tavily" for data that came from training knowledge is a hallucinated citation that destroys the user's ability to assess reliability. For general research on an unknown topic, call Tavily first, then call Fetch on promising URLs from the results for deeper analysis. Fetch retrieves content from a known URL and returns clean markdown — call Fetch for documentation pages, API references, and any URL where the content structure is predictable. Fetch returns pages in segments; when a truncated response already contains the answer, the agent delivers the answer immediately rather than requesting more content (requesting more wastes iterations). Three consecutive truncation cycles on the same URL signals the page is too large for Fetch — switch to Tavily or report what was found. Financial data sites (investing.com, bloomberg.com) are JavaScript-heavy and return poorly via Fetch — call Tavily for market data instead. For library and API documentation, the fallback chain is Context7 first, then Tavily, then Fetch on the official docs URL. When tools return URLs — from Tavily, Fetch, Context7, Playwright, or the Web Scraper — the agent includes the primary source URL inline in the response as a markdown link: `[source name](url)`. One link per source is sufficient; when a tool returns multiple URLs, pick the most authoritative. Playwright handles browser automation for JavaScript-heavy SPAs, login-protected dashboards, and forms — evaluate whether the target is static HTML (call Fetch) or requires JS interaction (call Playwright). Playwright consumes iterations rapidly — a typical navigate-login-read sequence uses 6 of 8 AIbitat iterations. The n8n tool handles workflow automation: call n8n_list_workflows, then n8n_execute_workflow by ID, then n8n_get_execution to poll completion.

AnythingLLM's own built-in Web Scraper is an Agent Skill — a native capability independent of the Docker MCP Gateway, offering URL-to-text conversion with zero MCP dependency. This Web Scraper Agent Skill is one of AnythingLLM's actual tools, unlike the nine MCP tools (Desktop Commander, Filesystem, Memory, Context7, Tavily, Fetch, Playwright, Sequential Thinking, n8n) described above which are MCP-exclusive. When the Web Scraper retrieves content from a URL, include that URL as a markdown link in the response.

<!-- Related: Docker MCP Gateway, Desktop Commander, Filesystem, Memory, Context7, Tavily, Fetch, Playwright, Sequential Thinking, n8n, shell commands, file operations, knowledge graph, web search, browser automation, Web Scraper, resolve-library-id, get-library-docs, search_nodes, PascalCase, snake_case, local graph, cross-agent, docker ps, Tavily first choice, Tavily mandatory, call before answer, real-time data, time-sensitive query, tool-call-first, citation integrity, hallucinated citation, Fetch truncation, fallback chains, Tavily Fetch Playwright, Context7 Tavily Fetch -->


## What are DyTopo multi-agent swarm tools and how does the swarm system work across the Loom stack?

<!-- Verified: 2026-02-15 -->

The DyTopo multi-agent swarm system (Dynamic Topology) provides three MCP-exclusive tools for launching, monitoring, and retrieving results from collaborative reasoning tasks that run as background processes on the host machine. DyTopo is NOT available in AnythingLLM — the swarm tools (swarm_start, swarm_status, swarm_result) exist only in the MCP tool inventory via the qdrant-rag MCP server. These tools are thin wrappers that delegate to the `src/dytopo/` package, which contains 8 core modules and 5 sub-packages: core modules include `models.py` (Pydantic v2 data models), `config.py` (YAML configuration with concurrency backend selection), `agents.py` (system prompts, JSON schemas, domain rosters), `router.py` (MiniLM-L6-v2 embedding and similarity routing), `graph.py` (NetworkX DAG construction with topological tier computation), `orchestrator.py` (async parallel swarm loop with backend-agnostic LLM client and semaphore-based concurrency), `governance.py` (convergence/stalling detection, re-delegation), and `audit.py` (JSONL audit logging); sub-packages provide observability (tracing, metrics, profiling), safeguards (rate limiter, token budget, circuit breaker), messaging (typed agent message passing), async routing engine, and delegation (subtask spawning with depth control). DyTopo creates a team of specialized AI agents that communicate through semantically-routed message passing — instead of broadcasting all agent outputs to all agents each round, DyTopo embeds agent descriptors using the MiniLM-L6-v2 sentence embedding model (a compact 22-million-parameter model consuming approximately 80 MB RAM on CPU), constructs a directed acyclic graph via cosine similarity thresholding, and executes agents in topological order so each agent receives only the messages that are semantically relevant to its current needs. This section documents DyTopo for architectural awareness so the workspace agent can accurately describe the system's multi-agent capabilities when asked.

The swarm_start tool creates a `SwarmTask` Pydantic object and launches `run_swarm()` from `orchestrator.py` via `asyncio.create_task()`, immediately returning a task_id string for progress tracking. It accepts five parameters: the task description in natural language, the domain selecting which agent team to deploy, the tau routing threshold controlling communication density between agents, the k_in parameter capping incoming messages per agent per round, and max_rounds setting the upper bound on reasoning iterations. The domain parameter selects from three pre-configured agent teams defined in `agents.py`. The code domain activates a Manager with Developer, Researcher, Tester, and Designer agents — this team is designed for programming tasks including code generation, debugging, architecture design, and code review. The math domain activates a Manager with ProblemParser, Solver, and Verifier agents — designed for mathematical reasoning including proofs, calculations, and multi-step problem solving. The general domain activates a Manager with Analyst, Critic, and Synthesizer agents — designed for open-ended analysis and multi-perspective reasoning.

The tau parameter (the routing threshold, defaulting to 0.3) is the primary control for inter-agent communication density during rounds 2 and beyond (round 1 always uses broadcast mode where all agents see all outputs). Lower tau values such as 0.1 to 0.2 create denser communication graphs, producing richer collaboration at higher token cost. Higher tau values such as 0.4 to 0.6 create sparser routing, leading to faster convergence and more independent agent work. The k_in parameter (default 3, range 1 to 5) caps how many incoming messages each agent receives per round. The max_rounds parameter (default 5, maximum 10) sets the upper bound on reasoning iterations — the Manager agent can terminate earlier, and `detect_convergence()` from `governance.py` can trigger early termination when outputs stabilize (default 90% similarity threshold, configurable via `convergence_threshold` in `dytopo_config.yaml`).

The swarm_status tool accepts a task_id and returns the current progress: round number, LLM call count, elapsed wall-clock time, progress message, and overall status (running, completed, or error). The recommended polling interval is 15 to 30 seconds. The swarm_result tool retrieves the final output once a swarm completes, including the final answer, `SwarmMetrics` (total rounds, tokens, wall time, routing density per round, convergence point, agent failures, re-delegations, per-agent success rates and latencies), and optionally the per-round topology log showing edges, similarity scores, and execution order. Swarm execution is fully isolated from the RAG pipeline — the entire `run_swarm()` is wrapped in try/except/finally, and the MCP tool's `_safe_run()` wrapper catches any exception and stores it as an error status. A DyTopo crash has zero impact on rag_search, rag_status, or other MCP tools. All swarm inference routes through a lazy singleton AsyncOpenAI client (with tenacity retry: 3 attempts, exponential backoff) at http://localhost:8008/v1, sharing the Qwen3-30B-A3B model and KV cache with interactive chat. While a swarm is running, inference requests from both frontends will queue behind swarm agent calls, potentially increasing response latency.

<!-- Related: DyTopo, Dynamic Topology, multi-agent swarms, swarm_start, swarm_status, swarm_result, arXiv 2602.06039, tau threshold, routing threshold, k_in, max_rounds, code domain, math domain, general domain, Developer, Researcher, Tester, Designer, ProblemParser, Solver, Verifier, Analyst, Critic, Synthesizer, Manager agent, MiniLM-L6-v2, semantic routing, descriptor embedding, directed acyclic graph, topological order, broadcast mode, three-phase routing, background task, polling interval, MCP exclusive tools, inference queuing, temperature settings, qdrant-rag MCP server, port 8008, Qwen3, src/dytopo, dytopo_config.yaml, SwarmTask, SwarmMetrics, governance, convergence -->


## How Sequential Thinking works as a structured reasoning scratchpad and when to use it for complex planning

<!-- Verified: 2026-02-14 -->

The Sequential Thinking MCP server (accessed through the Docker MCP Gateway as part of the nine containerized tool servers) provides a structured multi-step reasoning scratchpad that the agent uses to decompose complex problems before taking action with other tools. The server exposes a single tool called sequentialthinking that accepts a thought (the content of the current reasoning step), a thought number (which step in the sequence), an estimated total number of thoughts, and a flag indicating whether another thought is needed after the current one. The tool also supports revision of earlier thoughts when new information contradicts a previous conclusion, branching to explore alternative reasoning paths from any previous thought, and extending the sequence beyond the original estimate when the problem proves more complex than initially expected. Each call to sequentialthinking returns the thought formatted with its position in the sequence, creating a visible chain of reasoning that the model and user can follow.

The Sequential Thinking tool enables five distinct reasoning patterns that map to common workspace agent tasks. Linear decomposition breaks a complex task into ordered sub-steps, where each thought builds on the previous one — useful for planning a multi-tool investigation sequence such as diagnosing why a service is unresponsive (step 1: identify the port, step 2: check container status, step 3: inspect logs, step 4: verify config, step 5: synthesize findings). Hypothesis-test reasoning proposes an explanation in one thought, designs a verification approach in the next, evaluates evidence in a third, and revises the hypothesis if evidence contradicts it — effective for debugging where the root cause is uncertain. Root-cause analysis starts with the observed symptom, systematically traces through potential causes, uses branching to explore alternative causal chains when multiple explanations are plausible, and converges on the actual root cause with supporting evidence. Comparison evaluation dedicates thoughts to each candidate option using consistent criteria, then synthesizes the trade-offs in a final thought — useful when the agent must recommend between approaches such as different configuration strategies or tool alternatives. Planning and sequencing defines the goal, identifies required resources and tools, orders the actions by dependency, and produces an execution plan — with the ability to extend the sequence if the plan grows beyond the initial estimate.

The Sequential Thinking tool works alongside other tools in a hybrid pattern that interleaves structured reasoning with concrete action. The most common multi-tool chain involving Sequential Thinking is the three-pass pattern with the Memory knowledge graph: first, Sequential Thinking plans the approach by decomposing the question into sub-parts and identifying a search strategy; second, Memory search_nodes retrieves existing entities and observations matching the planned queries; third, Qwen3 evaluates the Memory results and decides whether to enrich the graph (calling add_observations or create_entities to store new synthesis) or to deliver the answer directly. This three-pass pattern works because Sequential Thinking's output stays in the conversation context as an internal reasoning scratchpad — it is not shown to the user but guides the agent's subsequent tool calls. Beyond the Memory pattern, the agent interleaves Sequential Thinking with any action tools — Desktop Commander for shell commands, Filesystem (MCP Filesystem tool — for reading, writing, searching, and listing files. NOT Windows Explorer.) for file operations, Context7 for documentation lookups — creating a visible reasoning chain where each thinking step references the evidence gathered by the previous action and adapts the plan based on what was revealed.

The agent selects Sequential Thinking when the task requires genuine multi-step reasoning before or between tool calls — tasks where jumping directly to tool execution would risk wasted effort or incorrect conclusions. Strong indicators for Sequential Thinking include debugging sequences spanning three or more components (where each diagnostic step depends on the previous result), architecture analysis requiring systematic evaluation of multiple factors, error diagnosis where the root cause is ambiguous and multiple hypotheses must be tested, comparing three or more approaches where trade-offs need structured evaluation, and planning multi-file changes or migration sequences where ordering matters. The agent skips Sequential Thinking for single-tool lookups (just call the tool directly), straightforward factual questions (use Memory search_nodes or let workspace RAG provide the answer), status checks (run the check via Desktop Commander), and simple file operations (use Filesystem directly) — adding a reasoning scratchpad to these tasks creates overhead without improving the result. Each thought step in a Sequential Thinking sequence costs approximately 100 to 300 tokens (the thought text itself plus the tool call overhead), so a typical 5-step thinking sequence adds roughly 500 to 1,500 tokens to the context. At 131,072 tokens with approximately 7,000 available for the current exchange, this cost is significant -- a 5-step thinking sequence can consume 20-50% of the remaining token budget. The agent must keep individual thoughts concise and reserve revision and branching for situations where earlier reasoning genuinely needs correction rather than using them routinely.

<!-- Related: Sequential Thinking, sequentialthinking tool, structured reasoning, multi-step reasoning scratchpad, Docker MCP Gateway, reasoning patterns, linear decomposition, hypothesis-test, root-cause analysis, comparison evaluation, planning and sequencing, hybrid ReAct pattern, reasoning then acting, interleaved tool use, thought sequence, thought revision, thought branching, when to use Sequential Thinking, when to skip, token budget, debugging workflow, architecture analysis, error diagnosis, multi-tool planning, Desktop Commander, Filesystem, Memory, Context7, visible reasoning chain, structured deliberation, problem decomposition, trade-off analysis, diagnostic sequence, evidence-based reasoning -->
