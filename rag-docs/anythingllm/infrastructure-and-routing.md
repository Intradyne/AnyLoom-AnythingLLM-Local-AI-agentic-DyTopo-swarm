# AnythingLLM Infrastructure, Port Topology, and Tool Routing Reference

This reference document describes the Loom stack's physical infrastructure from the AnythingLLM workspace agent's perspective — the decision cascade for choosing tools, the port topology connecting services, the VRAM and memory budget, and the fundamental asymmetry between passive automatic RAG retrieval and active explicit tool-based search. Each section is self-contained and formatted for dense-only RAG retrieval on port 6333.


## How the decision cascade routes queries from cheapest to most expensive tool tier

<!-- Verified: 2026-02-14 -->

The workspace agent follows a tiered decision cascade when determining how to answer a question, progressing from the cheapest and fastest information source to progressively more expensive ones. This cascade applies primarily in agent mode (activated by @agent) where tools are available, but the first two tiers operate in chat mode as well through automatic retrieval and the agent's own knowledge.

Tier 0 is automatic RAG context — the workspace's Qdrant instance on port 6333 retrieves relevant document chunks and injects them into the system message after a "Context:" separator on every relevant query with zero tool calls and zero token cost beyond the retrieved snippets themselves (approximately 8,000 tokens for 16 snippets at 500 tokens average each). Important: AnythingLLM and LM Studio have completely independent Qdrant instances (port 6333 and port 6334 respectively) with independent RAG budgets. This ~8K RAG snippet budget applies to AnythingLLM's port 6333 auto-injection independently — it does not share budget with LM Studio's port 6334 rag_search results. Each agent gets its own ~8K of RAG context within its own context window. This is the cheapest possible information source because it requires no action from the agent. The workspace documents stored in this Qdrant instance cover the stack's architecture, configuration, operational procedures, tool inventory, system prompt behavior, and behavioral conventions. When the answer exists in workspace documents, the agent cites the retrieved context directly ("Per the architecture reference: port 6333 serves dense-only workspace RAG") and needs no further tools.

Tier 1 is the Memory knowledge graph — the local entity-relation store accessible through the search_nodes tool. Memory is a local, private resource on this machine — the agent should use it eagerly and confidently. Memory lookups complete in milliseconds and cost only the tool call overhead (~100 tokens round-trip). The Memory graph stores stable facts that both the AnythingLLM workspace agent and the LM Studio agent have recorded: port mappings (port 1234 for LM Studio inference, port 6333 for workspace Qdrant, port 6334 for MCP Qdrant), collection names, user preferences, project decisions and their rationale, architecture choices, and previously resolved error patterns. Memory is faster and more precise than RAG for specific known entities — querying search_nodes for "QdrantMCP" returns the exact entity with all its observations, while RAG retrieval returns document chunks that may contain the fact buried in surrounding prose. The agent checks Memory first for questions about stable facts, known configurations, and previously recorded decisions.

Tier 2 is Context7 for external library and API documentation — the two-step resolve-library-id then get-library-docs sequence that fetches live documentation for npm packages, PyPI packages, docs.rs crates, and other package ecosystems. Context7 costs two tool calls (~200 tokens each) plus the returned documentation content, but provides current API references that may differ from the model's training data. The agent uses Context7 for questions about qdrant-client, FastMCP, sentence-transformers, FlagEmbedding, React, or any other library where version-specific API details matter. Context7 documentation is authoritative for external libraries in the same way that workspace RAG context is authoritative for this stack's internal architecture.

Tier 3 groups the external information MCP tools. IMPORTANT: Tavily, Fetch, Context7, and Playwright as described here are MCP tools available through LM Studio's Docker MCP Gateway — they are NOT AnythingLLM Agent Skills. AnythingLLM's own external information capability is its built-in Web Scraper Agent Skill, which provides URL-to-text conversion with zero MCP dependency. If a Tavily API key has been configured as a search provider in AnythingLLM's admin settings, the agent may also have a web search Agent Skill — but this is configured through AnythingLLM's admin UI, not through MCP. The LM Studio agent's MCP fallback chains (web content: Tavily → Fetch → Playwright; library docs: Context7 → Tavily → Fetch; local knowledge: Memory → workspace context → Filesystem) are documented here for architectural awareness only. In AnythingLLM chat mode without @agent, no Agent Skills are available — state that the requested capability requires @agent mode.

Tier 4 covers operational MCP tools available through LM Studio only (NOT AnythingLLM Agent Skills): Desktop Commander for shell commands, Docker management, process management, port checks, and service health verification; Filesystem for reading, writing, searching, and listing files; and Sequential Thinking for structured multi-step reasoning. These MCP tools interact with the host system directly and are available to the LM Studio agent. AnythingLLM does not have Agent Skills equivalent to Desktop Commander, Filesystem, or Sequential Thinking — when AnythingLLM users need live system state information, they must check manually or switch to LM Studio.

Tier 5 is DyTopo multi-agent swarms — the most expensive tier, available exclusively through the LM Studio interface. DyTopo launches teams of specialized agents (code, math, or general domains) that collaborate through semantically-routed message passing across multiple inference rounds. Each round consumes significant LM Studio API traffic through the shared Qwen3-30B-A3B model at port 1234, and active swarms create inference queuing that can delay responses for both frontends. The workspace agent cannot launch swarms directly but can describe DyTopo's capabilities and suggest that the user switch to LM Studio for tasks requiring multi-agent collaboration.

The cascade works as a first-match system: the agent uses the highest tier that can answer the question and only descends to lower tiers when higher tiers produce insufficient results. Most questions are answered at Tier 0 (automatic RAG context) or Tier 1 (Memory), with Tiers 2-5 reserved for progressively more specialized or expensive needs.

<!-- Related: decision cascade, tool routing, tier 0, tier 1, tier 2, tier 3, tier 4, tier 5, automatic RAG context, Memory knowledge graph, Context7, Tavily, Fetch, Web Scraper, Desktop Commander, Filesystem, Sequential Thinking, DyTopo, first-match routing, cheapest tool, query routing, search_nodes, resolve-library-id, get-library-docs, workspace documents, port 6333, port 6334, port 1234, inference queuing, tool cost, token overhead, live system state, operational tools, external information, web search, library documentation -->


## What is the port topology and how do services connect across the Loom stack?

<!-- Verified: 2026-02-14 -->

The Loom stack runs on a single machine with three primary network ports serving distinct roles. Port 1234 hosts the LM Studio API — an OpenAI-compatible HTTP endpoint at http://localhost:1234/v1 that provides both chat completions (the /v1/chat/completions endpoint serving the Qwen3-30B-A3B-Instruct-2507 language model) and embedding generation (the /v1/embeddings endpoint serving the co-loaded BGE-M3 Q8_0 GGUF embedding model). Every inference consumer in the stack connects to this single endpoint: the AnythingLLM workspace agent for both chat and embedding requests, the LM Studio direct chat interface, and DyTopo swarm agent calls that run as background tasks through the qdrant-rag MCP server. Because there is only one GPU model serving all requests, inference is strictly sequential — requests from any consumer queue at LM Studio and are processed one at a time. When a DyTopo swarm is actively running (multiple agent calls per round, 3 to 5 rounds typical), interactive chat responses from both frontends are delayed because swarm inference calls queue ahead.

Port 6333 hosts the AnythingLLM Qdrant instance — a Docker container running qdrant/qdrant:latest that stores workspace document vectors using dense-only cosine similarity search. This is the workspace agent's own vector database, managed entirely through AnythingLLM's document processing pipeline. Documents are ingested through the AnythingLLM web interface at localhost port 3001, chunked by a recursive text splitter into 6,600-character segments with 1,000-character overlap, embedded via BGE-M3 GGUF through LM Studio's /v1/embeddings endpoint, and stored as 1024-dimensional dense vectors. The retrieval settings admit up to 16 snippets at a low similarity threshold with 30 messages of conversation history. This Qdrant container runs with a 4 GB memory limit, 4 CPU cores, and an always-restart policy. Its data volume (qdrant_anythingllm) is completely independent of the other Qdrant container.

Port 6334 hosts the LM Studio Qdrant instance — a separate Docker container serving the qdrant-rag MCP server's hybrid search pipeline. This container maps host port 6334 to the container's default REST port 6333 via the Docker -p 6334:6333 port mapping. The qdrant-rag MCP server connects to this instance at http://localhost:6334 and uses it for hybrid dense-plus-sparse search with Reciprocal Rank Fusion (RRF), combining BGE-M3's 1024-dimensional dense semantic vectors with learned sparse lexical weight vectors computed by the FlagEmbedding library running on CPU (approximately 2.3 GB system RAM, zero VRAM). The lmstudio_docs collection in this instance stores document chunks from two source directories — labeled "lmstudio" and "anythingllm" — with source_dir payload filtering that allows scoped searches. The workspace agent has no direct connection to port 6334; it works exclusively with port 6333.

The Docker MCP Gateway provides nine containerized MCP tool servers (Desktop Commander, Filesystem, Memory, Context7, Tavily, Fetch, Playwright, Sequential Thinking, n8n). These are MCP tools configured in LM Studio's mcp.json. AnythingLLM does NOT access tools through the Docker MCP Gateway — AnythingLLM uses its own built-in Agent Skills configured through its admin UI at localhost:3001. The only shared resource between the two agents is the Memory knowledge graph, which both the AnythingLLM workspace agent and the LM Studio agent can read and write. Memory is a local, private resource — the agent should use it eagerly as the fastest path to structured knowledge. All other data paths are agent-specific: AnythingLLM owns port 6333, the qdrant-rag server owns port 6334, and each frontend maintains its own conversation history and system prompt.

The fundamental asymmetry between the two frontends is the difference between passive and active RAG retrieval. AnythingLLM's RAG is passive — document chunks from port 6333 are injected automatically into the system message on every relevant query, requiring zero agent action. The LM Studio agent's RAG is active — the agent must explicitly call rag_search through the qdrant-rag MCP server to retrieve documents from port 6334, making retrieval a deliberate tool call with query parameters, result limits, and source filters. This asymmetry means the same information often exists in format-divergent versions: terser in the LM Studio pipeline (where sparse keyword matching catches exact identifiers) and richer prose in the AnythingLLM pipeline (where dense-only retrieval depends entirely on semantic similarity between the query and the document text).

<!-- Related: port topology, port 1234, port 6333, port 6334, port 3001, LM Studio API, OpenAI-compatible, Qdrant Docker containers, BGE-M3, GGUF Q8_0, FlagEmbedding, hybrid search, dense-only search, RRF fusion, Reciprocal Rank Fusion, inference queuing, sequential inference, single GPU, Docker MCP Gateway, Memory local graph, passive RAG, active RAG, format-divergent, workspace documents, lmstudio_docs collection, source_dir filtering, container isolation, data volumes, port mapping, always-restart, DyTopo queuing, AnythingLLM web interface, recursive text splitter, workspace RAG, qdrant-rag MCP server -->


## What is the VRAM and system memory budget and how do resources divide across the stack?

<!-- Verified: 2026-02-14 -->

The RTX 5090 GPU provides 32 GB of GDDR7 VRAM, and the current stack configuration consumes approximately 30.5 GB at the default 80,000-token context length, leaving roughly 1.5 GB of headroom. The VRAM allocation breaks down into four components. The Qwen3-30B-A3B model weights at Q6_K quantization (a near-lossless 6-bit compression format that retains approximately 99% of the original FP16 quality) occupy 25.1 GB — this is the single largest consumer and a fixed cost regardless of context length, conversation length, or workload. The BGE-M3 Q8_0 GGUF embedding model co-loaded alongside Qwen3 in LM Studio adds 0.6 GB — this model serves AnythingLLM's dense-only embedding pipeline through the /v1/embeddings endpoint and remains resident in VRAM at all times. CUDA runtime overhead accounts for approximately 1.0 GB of baseline GPU memory allocation. The KV cache stores the key-value attention state for the current context window and scales with context length: at 80K tokens with Q8_0 quantized cache (which halves cache memory with negligible quality loss), the KV cache occupies approximately 3.75 GB. Each 1,000 tokens of context costs approximately 47 MB of VRAM in KV cache.

The KV cache is the only variable component in the VRAM budget, and it determines how much context the model can hold simultaneously. At 32K tokens the KV cache uses approximately 1.5 GB (total VRAM approximately 28.2 GB, comfortable), at 64K approximately 3.0 GB (total approximately 29.7 GB, conservative), at 80K approximately 3.75 GB (total approximately 30.5 GB, the current default and practical sweet spot), and at 96K approximately 4.5 GB (total approximately 31.2 GB, tight but functional). Going beyond 96K tokens would push total VRAM close to the 32 GB ceiling and would require switching from Q6_K to Q5_K_M quantization (approximately 21.7 GB weights) to maintain headroom. The 80K default represents the practical balance between context capacity and VRAM safety for the Q6_K quantization level on the RTX 5090.

System RAM usage on the host machine (AMD Ryzen 9 9950X3D with 16 cores and 32 threads, 94 GB DDR5) comes primarily from two CPU-resident models managed by the qdrant-rag MCP server. The BGE-M3 FlagEmbedding model (the CPU-based embedding pipeline that produces both dense and sparse vectors for hybrid search on port 6334) consumes approximately 2.3 GB of system RAM and uses 8 dedicated CPU threads — half of the 9950X3D's 16 physical cores, configured via the RAG_CPU_THREADS environment variable. This model is lazy-loaded on the first rag_search or rag_reindex call, with initial loading taking 30 to 60 seconds; subsequent calls use the loaded singleton instantly. The MiniLM-L6-v2 model (used by DyTopo for semantic routing of agent descriptors during swarm execution) consumes approximately 80 MB of system RAM and loads in under 1 second on first swarm. Both models run on CPU with zero VRAM consumption, and both share a dedicated ThreadPoolExecutor with 2 worker threads to keep CPU-bound embedding work off the async event loop. The tokenizers parallelism is disabled (TOKENIZERS_PARALLELISM=false) to prevent thread contention with the embedding batch processing. Gradient computation is globally disabled (torch.set_grad_enabled(False)) since the server performs inference only.

Symptoms of VRAM pressure include LM Studio becoming unresponsive or crashing, progressively slower inference as the context approaches 80K tokens, and CUDA out-of-memory errors in LM Studio's console output. The nvidia-smi command (run via Desktop Commander) shows current GPU memory usage — utilization above 31 GB indicates pressure. Mitigation strategies include starting a new conversation to reset the KV cache (which immediately frees the 3.75 GB cache allocation), reducing context length in LM Studio settings (64K saves approximately 0.75 GB compared to 80K), and avoiding simultaneous inference from both frontends during VRAM-constrained operation. The workspace agent can check VRAM status by asking Desktop Commander to run nvidia-smi and reporting the current utilization against the 32 GB total.

<!-- Related: VRAM budget, RTX 5090, 32 GB GDDR7, 30.5 GB utilization, 1.5 GB headroom, Qwen3-30B-A3B, Q6_K quantization, 25.1 GB weights, BGE-M3 GGUF, 0.6 GB VRAM, KV cache, Q8_0 cache, 3.75 GB at 80K, CUDA overhead, context length scaling, 80K default, 96K tight, Q5_K_M alternative, system RAM, Ryzen 9 9950X3D, 94 GB DDR5, FlagEmbedding CPU, 2.3 GB RAM, MiniLM-L6-v2, 80 MB RAM, RAG_CPU_THREADS, lazy loading, ThreadPoolExecutor, nvidia-smi, VRAM pressure, CUDA out of memory, KV cache reset, GPU monitoring, memory allocation, embedding models, CPU threads, tokenizers parallelism -->


## How the single inference endpoint creates a shared bottleneck and what that means for response latency

<!-- Verified: 2026-02-14 -->

Both the LM Studio agent and the AnythingLLM workspace agent share the inference endpoint at localhost port 1234, which hosts the single Qwen3-30B-A3B-Instruct-2507 model on one RTX 5090 GPU. There is no parallel inference capability — requests from any consumer are processed strictly sequentially. This means that when the LM Studio user and the AnythingLLM user send messages at the same time, one request waits in the queue while the other completes. The practical impact in normal interactive use is negligible because individual inference calls complete in seconds, but the impact becomes noticeable during specific high-traffic scenarios.

DyTopo multi-agent swarms are the primary source of inference congestion. When a swarm is actively running, each round involves multiple inference calls: descriptor-only calls for routing (fast, limited to 256 tokens at temperature 0.1 for near-deterministic structured output) followed by full work calls for each agent (up to 4,096 tokens at temperature 0.3 for productive reasoning), plus a Manager call per round for goal-setting and termination decisions (temperature 0.1). A typical 4-round swarm with 4 worker agents generates approximately 20 to 30 inference calls, each of which queues ahead of interactive requests from both frontends. During active swarm execution, interactive chat responses may be delayed by 10 to 60 seconds depending on the swarm's current round and how many agent calls remain in the queue. The workspace agent can check for active swarms by asking the user to check swarm_status in LM Studio, or by noting unexpectedly high response latency as an indicator.

The three temperature settings used across the stack reflect the sequential-queue constraint. LM Studio's direct chat uses temperature 0.3 (the UI setting), balancing creativity with reliability for interactive use with Qwen3's hybrid thinking mode. AnythingLLM's workspace uses temperature 0.1 (set in workspace settings), maximizing determinism for tool call JSON formatting while Qwen3's internal /think reasoning provides diversity. DyTopo swarm calls use per-request temperature in the API body (0.1 for descriptors and Manager decisions, 0.3 for worker output), overriding the LM Studio UI setting. All three temperature configurations are compatible because each consumer sets temperature in its own API request, and LM Studio applies the per-request value when provided.

The embedding endpoint (also at port 1234, via /v1/embeddings) introduces a secondary queuing point. AnythingLLM sends embedding requests through this endpoint when ingesting new documents or when the workspace RAG pipeline needs to embed a query for similarity search. These embedding requests use the co-loaded BGE-M3 Q8_0 GGUF model (0.6 GB VRAM) and share the same request queue as chat completion calls. However, embedding calls are typically fast (milliseconds for a single query embedding, seconds for batch document ingestion) and create minimal queuing impact compared to LLM inference calls. The CPU-based BGE-M3 FlagEmbedding model used by the qdrant-rag MCP server on port 6334 is completely independent of LM Studio's request queue — it runs in its own process on CPU and generates no VRAM contention.

When response latency is unexpectedly high for the workspace agent, the diagnostic sequence is: check whether a DyTopo swarm is running (if the user has access to LM Studio, they can call swarm_status), check VRAM utilization via Desktop Commander running nvidia-smi (usage above 31 GB suggests KV cache pressure), and check whether the context window is approaching capacity (long conversations with many tool results fill the context faster). Starting a fresh conversation resets both the KV cache and the conversation context, which resolves most latency issues.

<!-- Related: inference bottleneck, shared endpoint, port 1234, sequential inference, request queuing, DyTopo latency, swarm overhead, temperature settings, 0.1 temperature, 0.3 temperature, descriptor calls, Manager calls, worker calls, inference congestion, embedding endpoint, /v1/embeddings, BGE-M3 GGUF, BGE-M3 FlagEmbedding, CPU embedding, GPU embedding, response latency, diagnostic sequence, nvidia-smi, KV cache pressure, VRAM monitoring, fresh conversation, context window, queue priority, LM Studio API, OpenAI-compatible endpoint, concurrent requests, swarm_status, high latency troubleshooting -->
