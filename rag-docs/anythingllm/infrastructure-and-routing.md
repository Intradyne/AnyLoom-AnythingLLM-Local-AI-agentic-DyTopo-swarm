# AnythingLLM Infrastructure, Port Topology, and Tool Routing Reference

This reference document describes the Loom stack's physical infrastructure from the AnythingLLM workspace agent's perspective — the decision cascade for choosing tools, the port topology connecting services, the VRAM and memory budget, and the fundamental asymmetry between passive automatic RAG retrieval and active explicit tool-based search. Each section is self-contained and formatted for dense-only RAG retrieval on port 6333.


## How the decision cascade routes queries from cheapest to most expensive tool tier

<!-- Verified: 2026-02-14 -->

The workspace agent follows a tiered decision cascade when determining how to answer a question, progressing from the cheapest and fastest information source to progressively more expensive ones. This cascade applies primarily in agent mode (activated by @agent) where tools are available, but the first two tiers operate in chat mode as well through automatic retrieval and the agent's own knowledge.

Tier 0 is automatic RAG context — the Qdrant instance (`anyloom-qdrant` on port 6333) retrieves relevant document chunks and injects them into the system message after a "Context:" separator on every relevant query with zero tool calls and zero token cost beyond the retrieved snippets themselves (approximately 8,000 tokens for 16 snippets at 500 tokens average each). AnythingLLM's dense-only auto-injection and the MCP hybrid RAG pipeline both query the same Qdrant instance but use separate collections and retrieval strategies. This is the cheapest possible information source because it requires no action from the agent. The workspace documents stored in this Qdrant instance cover the stack's architecture, configuration, operational procedures, tool inventory, system prompt behavior, and behavioral conventions. When the answer exists in workspace documents, the agent cites the retrieved context directly ("Per the architecture reference: port 6333 serves dense-only workspace RAG") and needs no further tools.

Tier 1 is the Memory knowledge graph — the local entity-relation store accessible through the search_nodes tool. Memory is a local, private resource on this machine — the agent should use it eagerly and confidently. Memory lookups complete in milliseconds and cost only the tool call overhead (~100 tokens round-trip). The Memory graph stores stable facts that agents have recorded: port mappings (port 8008 for llama.cpp inference, port 6333 for Qdrant), collection names, user preferences, project decisions and their rationale, architecture choices, and previously resolved error patterns. Memory is faster and more precise than RAG for specific known entities — querying search_nodes for "QdrantMCP" returns the exact entity with all its observations, while RAG retrieval returns document chunks that may contain the fact buried in surrounding prose. The agent checks Memory first for questions about stable facts, known configurations, and previously recorded decisions.

Tier 2 is Context7 for external library and API documentation — the two-step resolve-library-id then get-library-docs sequence that fetches live documentation for npm packages, PyPI packages, docs.rs crates, and other package ecosystems. Context7 costs two tool calls (~200 tokens each) plus the returned documentation content, but provides current API references that may differ from the model's training data. The agent uses Context7 for questions about qdrant-client, FastMCP, sentence-transformers, onnxruntime, React, or any other library where version-specific API details matter. Context7 documentation is authoritative for external libraries in the same way that workspace RAG context is authoritative for this stack's internal architecture.

Tier 3 groups the external information MCP tools. IMPORTANT: Tavily, Fetch, Context7, and Playwright as described here are MCP tools available through the Docker MCP Gateway — they are NOT AnythingLLM Agent Skills. AnythingLLM's own external information capability is its built-in Web Scraper Agent Skill, which provides URL-to-text conversion with zero MCP dependency. If a Tavily API key has been configured as a search provider in AnythingLLM's admin settings, the agent may also have a web search Agent Skill — but this is configured through AnythingLLM's admin UI, not through MCP. The MCP fallback chains (web content: Tavily → Fetch → Playwright; library docs: Context7 → Tavily → Fetch; local knowledge: Memory → workspace context → Filesystem) are documented here for architectural awareness only. In AnythingLLM chat mode without @agent, no Agent Skills are available — state that the requested capability requires @agent mode.

Tier 4 covers operational MCP tools (NOT AnythingLLM Agent Skills): Desktop Commander for shell commands, Docker management, process management, port checks, and service health verification; Filesystem for reading, writing, searching, and listing files; and Sequential Thinking for structured multi-step reasoning. These MCP tools interact with the host system directly. AnythingLLM does not have Agent Skills equivalent to Desktop Commander, Filesystem, or Sequential Thinking — when AnythingLLM users need live system state information, they must check manually or use the MCP interface.

Tier 5 is DyTopo multi-agent swarms — the most expensive tier, available exclusively through MCP tools. DyTopo launches teams of specialized agents (code, math, or general domains) that collaborate through semantically-routed message passing across multiple inference rounds. Each round consumes significant llama.cpp API traffic through the shared Qwen3-30B-A3B model at port 8008, and active swarms create inference queuing that can delay responses. The workspace agent cannot launch swarms directly but can describe DyTopo's capabilities and suggest using the MCP interface for tasks requiring multi-agent collaboration.

The cascade works as a first-match system: the agent uses the highest tier that can answer the question and only descends to lower tiers when higher tiers produce insufficient results. Most questions are answered at Tier 0 (automatic RAG context) or Tier 1 (Memory), with Tiers 2-5 reserved for progressively more specialized or expensive needs.

<!-- Related: decision cascade, tool routing, tier 0, tier 1, tier 2, tier 3, tier 4, tier 5, automatic RAG context, Memory knowledge graph, Context7, Tavily, Fetch, Web Scraper, Desktop Commander, Filesystem, Sequential Thinking, DyTopo, first-match routing, cheapest tool, query routing, search_nodes, resolve-library-id, get-library-docs, workspace documents, port 6333, port 8008, inference queuing, tool cost, token overhead, live system state, operational tools, external information, web search, library documentation, anyloom-qdrant, single Qdrant instance -->


## What is the port topology and how do services connect across the AnyLoom stack?

<!-- Verified: 2026-02-16 -->

The AnyLoom stack runs on a single machine with three Docker containers and a set of distinct network ports. Port 8008 (host) maps to container port 8080 on the `anyloom-llm` container, which hosts the llama.cpp inference backend — an OpenAI-compatible HTTP endpoint at http://localhost:8008/v1 that provides both chat completions (the /v1/chat/completions endpoint serving the Qwen3-30B-A3B-Instruct-2507 language model) and embedding generation (the /v1/embeddings endpoint serving the BGE-M3 embedding model). Every inference consumer in the stack connects to this endpoint: the AnythingLLM workspace agent for both chat and embedding requests, and DyTopo swarm agent calls that run as background tasks through the qdrant-rag MCP server. From the host or scripts, use http://localhost:8008/v1. From other Docker containers on the `anyloom` network, use http://anyloom-llm:8080/v1. Because there is only one GPU model serving all requests, inference is strictly sequential — requests from any consumer queue at llama.cpp and are processed one at a time. When a DyTopo swarm is actively running (multiple agent calls per round, 3 to 5 rounds typical), interactive chat responses are delayed because swarm inference calls queue ahead.

Port 6333 (REST) and port 6334 (gRPC) host the single Qdrant instance — the `anyloom-qdrant` Docker container running qdrant/qdrant:latest. This single container serves all vector database needs for the stack. AnythingLLM uses it for workspace document vectors with dense-only cosine similarity search (the `anyloom_docs` collection). The qdrant-rag MCP server also connects to this same instance for hybrid dense-plus-sparse search with Reciprocal Rank Fusion (RRF), using its own collection with source_dir payload filtering. Documents are ingested through the AnythingLLM web interface at localhost port 3001, chunked by a recursive text splitter into 6,600-character segments with 1,000-character overlap, embedded via BGE-M3 through the llama.cpp /v1/embeddings endpoint, and stored as 1024-dimensional dense vectors. The retrieval settings admit up to 16 snippets at a low similarity threshold with 30 messages of conversation history. The MCP hybrid RAG pipeline uses the sentence-transformers ONNX INT8 backend running on CPU (approximately 0.6 GB RAM, 0 VRAM) for dense embedding computation, with TF-weighted hash-based sparse vectors for lexical matching.

Port 3001 hosts the AnythingLLM web interface (`anyloom-anythingllm` container). The Docker MCP Gateway provides nine containerized MCP tool servers (Desktop Commander, Filesystem, Memory, Context7, Tavily, Fetch, Playwright, Sequential Thinking, n8n). These are MCP tools configured in the mcp.json. AnythingLLM does NOT access tools through the Docker MCP Gateway — AnythingLLM uses its own built-in Agent Skills configured through its admin UI at localhost:3001. The only cross-agent shared resource is the Memory knowledge graph. Memory is a local, private resource — the agent should use it eagerly as the fastest path to structured knowledge. Each frontend maintains its own conversation history and system prompt.

The fundamental asymmetry between the two RAG pipelines is the difference between passive and active retrieval. AnythingLLM's RAG is passive — document chunks from the `anyloom_docs` collection on port 6333 are injected automatically into the system message on every relevant query, requiring zero agent action. The MCP RAG pipeline is active — the agent must explicitly call rag_search through the qdrant-rag MCP server to retrieve documents, making retrieval a deliberate tool call with query parameters, result limits, and source filters. This asymmetry means the same information often exists in format-divergent versions: terser in the MCP hybrid pipeline (where sparse keyword matching catches exact identifiers) and richer prose in the AnythingLLM pipeline (where dense-only retrieval depends entirely on semantic similarity between the query and the document text).

<!-- Related: port topology, port 8008, port 6333, port 6334, port 3001, llama.cpp API, OpenAI-compatible, anyloom-qdrant, anyloom-llm, anyloom-anythingllm, BGE-M3, ONNX INT8, hybrid search, dense-only search, RRF fusion, Reciprocal Rank Fusion, inference queuing, sequential inference, single GPU, Docker MCP Gateway, Memory local graph, passive RAG, active RAG, format-divergent, workspace documents, anyloom_docs collection, source_dir filtering, single Qdrant instance, DyTopo queuing, AnythingLLM web interface, recursive text splitter, workspace RAG, qdrant-rag MCP server, container-internal port 8080, CPU embedding -->


## What is the VRAM and system memory budget and how do resources divide across the stack?

<!-- Verified: 2026-02-14 -->

The RTX 5090 GPU provides 32 GB of GDDR7 VRAM, and the current stack configuration loads the Qwen3-30B-A3B model as a Q4_K_M GGUF quantization weighing approximately 18.6 GiB — a significant reduction from the former FP8 format, leaving ample headroom for a 131,072-token context window (--ctx-size 131072). The VRAM allocation breaks down into several components. The Q4_K_M GGUF weights at ~18.6 GiB are the single largest consumer and a fixed cost regardless of context length, conversation length, or workload. The BGE-M3 embedding model served through llama.cpp adds approximately 0.6 GB — this model serves AnythingLLM's dense-only embedding pipeline through the /v1/embeddings endpoint. CUDA runtime overhead accounts for approximately 1.0 GB of baseline GPU memory allocation. The KV cache stores the key-value attention state for the current context window and scales with context length; llama.cpp uses quantized KV cache types K:Q8_0/V:Q4_0 (~39 KiB per token), with approximately 12.4 GiB of VRAM available for KV cache after weights and overhead — sufficient to support the full 131K context window.

The KV cache is the only variable component in the VRAM budget, and it determines how much context the model can hold simultaneously. With Q4_K_M GGUF weights at ~18.6 GiB, the quantized KV cache (K:Q8_0/V:Q4_0 at ~39 KiB per token) for the full 131K context uses approximately 5.0 GiB, bringing total VRAM utilization to roughly 25.2 GiB with comfortable headroom on the RTX 5090's 32 GB. The 131K default (--ctx-size 131072) is viable precisely because the Q4_K_M quantization freed substantial VRAM compared to the former FP8 format, and the quantized KV cache types dramatically reduce per-token memory cost.

System RAM usage on the host machine (AMD Ryzen 9 9950X3D with 16 cores and 32 threads, 94 GB DDR5) comes primarily from two GPU-resident models managed by the qdrant-rag MCP server. The BGE-M3 ONNX INT8 model (the CPU-based embedding pipeline that produces dense vectors for the hybrid search pipeline) consumes approximately 0.6 GB RAM with zero VRAM usage. This model is lazy-loaded on the first rag_search or rag_reindex call, with initial loading taking 30 to 60 seconds; subsequent calls use the loaded singleton instantly. The MiniLM-L6-v2 model (used by DyTopo for semantic routing of agent descriptors during swarm execution) consumes approximately 80 MB RAM and loads in under 1 second on first swarm. Both models run on CPU, and both share a dedicated ThreadPoolExecutor with 2 worker threads to keep embedding work off the async event loop. The tokenizers parallelism is disabled (TOKENIZERS_PARALLELISM=false) to prevent thread contention with the embedding batch processing. No PyTorch or CUDA is required for embedding — the pipeline uses sentence-transformers with the ONNX runtime backend.

Symptoms of VRAM pressure include llama.cpp becoming unresponsive or crashing, progressively slower inference as the context approaches 131K tokens, and CUDA out-of-memory errors in llama.cpp's container logs. The nvidia-smi command (run via Desktop Commander) shows current GPU memory usage — utilization significantly above 25 GiB may indicate pressure. With Q4_K_M GGUF weights and quantized KV cache, headroom is ample under normal operation. Mitigation strategies include starting a new conversation to reset the KV cache and avoiding simultaneous inference during VRAM-constrained operation. The workspace agent can check VRAM status by asking Desktop Commander to run nvidia-smi and reporting the current utilization against the 32 GB total.

<!-- Related: VRAM budget, RTX 5090, 32 GB GDDR7, Q4_K_M GGUF, ~18.6 GiB weights, Qwen3-30B-A3B, BGE-M3, 0.6 GB RAM, KV cache, K:Q8_0/V:Q4_0, ~39 KiB per token, CUDA overhead, context length scaling, 131K default, system RAM, Ryzen 9 9950X3D, 94 GB DDR5, ONNX INT8, CPU embedding, 0.6 GB RAM, MiniLM-L6-v2, 80 MB RAM, lazy loading, ThreadPoolExecutor, nvidia-smi, VRAM pressure, CUDA out of memory, KV cache reset, GPU monitoring, memory allocation, embedding models, tokenizers parallelism, llama.cpp, anyloom-llm -->


## How the single inference endpoint creates a shared bottleneck and what that means for response latency

<!-- Verified: 2026-02-16 -->

All inference consumers share the llama.cpp endpoint at localhost port 8008 (http://localhost:8008/v1 from the host, http://anyloom-llm:8080/v1 from other containers), which hosts the single Qwen3-30B-A3B-Instruct-2507 model on one RTX 5090 GPU. There is no parallel inference capability — requests from any consumer are processed strictly sequentially. This means that when multiple consumers send requests at the same time, one request waits in the queue while the other completes. The practical impact in normal interactive use is negligible because individual inference calls complete in seconds, but the impact becomes noticeable during specific high-traffic scenarios.

DyTopo multi-agent swarms are the primary source of inference congestion. When a swarm is actively running, each round involves multiple inference calls: descriptor-only calls for routing (fast, limited to 256 tokens at temperature 0.1 for near-deterministic structured output) followed by full work calls for each agent (up to 4,096 tokens at temperature 0.3 for productive reasoning), plus a Manager call per round for goal-setting and termination decisions (temperature 0.1). A typical 4-round swarm with 4 worker agents generates approximately 20 to 30 inference calls, each of which queues ahead of interactive requests. During active swarm execution, interactive chat responses may be delayed by 10 to 60 seconds depending on the swarm's current round and how many agent calls remain in the queue. The workspace agent can check for active swarms by asking the user to check swarm_status via MCP tools, or by noting unexpectedly high response latency as an indicator.

The temperature settings used across the stack reflect the sequential-queue constraint. AnythingLLM's workspace uses temperature 0.1 (set in workspace settings), maximizing determinism for tool call JSON formatting while Qwen3's internal /think reasoning provides diversity. DyTopo swarm calls use per-request temperature in the API body (0.1 for descriptors and Manager decisions, 0.3 for worker output). All temperature configurations are compatible because each consumer sets temperature in its own API request, and llama.cpp applies the per-request value when provided.

The embedding endpoint (also at port 8008, via /v1/embeddings) introduces a secondary queuing point. AnythingLLM sends embedding requests through this endpoint when ingesting new documents or when the workspace RAG pipeline needs to embed a query for similarity search. These embedding requests use the BGE-M3 model and share the same request queue as chat completion calls. However, embedding calls are typically fast (milliseconds for a single query embedding, seconds for batch document ingestion) and create minimal queuing impact compared to LLM inference calls. The CPU-based BGE-M3 ONNX INT8 model used by the qdrant-rag MCP server is completely independent of llama.cpp's request queue — it runs on CPU with its own RAM allocation and zero VRAM usage.

When response latency is unexpectedly high for the workspace agent, the diagnostic sequence is: check whether a DyTopo swarm is running (check swarm_status via MCP tools), check VRAM utilization via Desktop Commander running nvidia-smi (usage significantly above 25 GiB suggests KV cache pressure), and check whether the context window is approaching capacity (long conversations with many tool results fill the context faster). Starting a fresh conversation resets both the KV cache and the conversation context, which resolves most latency issues.

<!-- Related: inference bottleneck, shared endpoint, port 8008, sequential inference, request queuing, DyTopo latency, swarm overhead, temperature settings, 0.1 temperature, 0.3 temperature, descriptor calls, Manager calls, worker calls, inference congestion, embedding endpoint, /v1/embeddings, BGE-M3, BGE-M3 ONNX INT8, CPU embedding, response latency, diagnostic sequence, nvidia-smi, KV cache pressure, VRAM monitoring, fresh conversation, context window, queue priority, llama.cpp API, OpenAI-compatible endpoint, concurrent requests, swarm_status, high latency troubleshooting, anyloom-llm -->
