# AnythingLLM Agent System Prompt and Behavioral Reference

This reference document describes the system prompt that governs the AnythingLLM workspace agent — how AnythingLLM processes the prompt, how automatic context retrieval works, how the AIbitat framework manages tool execution, and how routing, memory, and compression interact. This document is formatted for dense-only RAG retrieval on the AnythingLLM Qdrant instance at port 6333. Each section is self-contained and can be understood independently.


## What the workspace agent is, how AnythingLLM processes the system prompt, and how automatically retrieved context works

<!-- Verified: 2026-02-14 -->

The AnythingLLM workspace agent runs Qwen3-30B-A3B-Instruct-2507 (quantized to Q6_K, a near-lossless 6-bit compression format) through LM Studio at localhost port 1234. The host machine is an RTX 5090 with 32 GB of VRAM, and the model operates within an 80,000-token context window. The workspace temperature is set to 0.1, which is deliberately lower than LM Studio's own 0.3 setting — this tighter temperature maximizes determinism for tool call JSON formatting while Qwen3's hybrid thinking mode (the /think and /no_think toggle that controls whether the model uses extended internal chain-of-thought reasoning) provides internal reasoning diversity even at low temperature.

<<<<<<< HEAD
AnythingLLM processes the workspace system prompt through its chatPrompt() function, which uses JavaScript's nullish coalescing operator (??) to select the prompt text. If the workspace system prompt field is null or undefined (a workspace created without any system prompt configured), the platform falls back to a hardcoded default prompt. If the field contains any string — including an empty string — that string is used verbatim as the system message. This means an empty system prompt field does not trigger the default; it silences all system-level instructions entirely. The chatPrompt() function also interpolates three placeholder variables before sending the prompt to the model: {{date}} inserts the current date, {{time}} inserts the current time, and {{workspace.id}} inserts the workspace identifier. These are AnythingLLM's internal template interpolation mechanics — the model never sees the raw template tokens. AnythingLLM replaces them before the prompt reaches the model: for example, {{date}} becomes the actual current date string (such as "2026-02-14"), {{time}} becomes the actual current time string, and {{workspace.id}} becomes the workspace's slug or ID. The model receives only the interpolated values and should use them directly when asked about the current date or time — the date in the system prompt IS today's date, not an example or a placeholder. The model should never output "{{date}}" or "{{workspace.id}}" literally in its responses, and should never substitute a date from training data when the interpolated date is available in the system prompt. This interpolation gives the model awareness of when and where it is operating without hardcoding temporal values. Starting in version 1.9.x, AnythingLLM passes the workspace system prompt through to the model without modification in both chat mode and agent mode — the same prompt text serves both modes identically, unlike earlier versions that prepended framework-specific wrapper instructions when agent mode activated. This pass-through behavior is why the Loom system prompt is designed as a single unified document with U-curve attention placement rather than separate chat and agent prompts.

The agent operates in two modes using this single system prompt text. Regular chat mode handles the majority of interactions where the user types normally — in chat mode the model has NO tool access and cannot call Tavily, Desktop Commander, Memory, or any other tool. Agent mode (invoked with the @agent command) activates the AIbitat tool-calling framework that enables live tool execution. Only agent mode can call tools; chat mode is limited to workspace RAG context and training knowledge. The system prompt is designed so that the most important instructions for chat mode occupy the very beginning and very end of the prompt, exploiting the U-shaped attention curve where transformers pay strongest attention to the first and last content in a prompt. The agent's prime directive appears at both positions as an identical bookend — the opening and closing lines both read "Execute tools directly. Present results, not plans. Cite your sources." This action-forward bookend anchors the agent toward execution rather than narration, reinforcing in both the highest-attention start and end positions that the user expects completed work with citations, not suggestions or descriptions of what tools could be used.
=======
AnythingLLM processes the workspace system prompt through its chatPrompt() function, which uses JavaScript's nullish coalescing operator (??) to select the prompt text. If the workspace system prompt field is null or undefined (a workspace created without any system prompt configured), the platform falls back to a hardcoded default prompt. If the field contains any string — including an empty string — that string is used verbatim as the system message. This means an empty system prompt field does not trigger the default; it silences all system-level instructions entirely. The chatPrompt() function also interpolates three placeholder variables before sending the prompt to the model: {{date}} inserts the current date, {{time}} inserts the current time, and {{workspace}} inserts the workspace identifier, giving the model awareness of when and where it is operating without hardcoding temporal values. Starting in version 1.9.x, AnythingLLM passes the workspace system prompt through to the model without modification in both chat mode and agent mode — the same prompt text serves both modes identically, unlike earlier versions that prepended framework-specific wrapper instructions when agent mode activated. This pass-through behavior is why the Loom system prompt is designed as a single unified document with U-curve attention placement rather than separate chat and agent prompts.

The agent operates in two modes using this single system prompt text. Regular chat mode handles the majority of interactions where the user types normally, and agent mode (invoked with the @agent command) activates the AIbitat tool-calling framework that enables live tool execution. The system prompt is designed so that the most important instructions for chat mode occupy the very beginning and very end of the prompt, exploiting the U-shaped attention curve where transformers pay strongest attention to the first and last content in a prompt. The agent's prime directive appears at both positions as an identical bookend — the opening and closing lines both read "Execute tools directly. Present results, not plans. Cite your sources." This action-forward bookend anchors the agent toward execution rather than narration, reinforcing in both the highest-attention start and end positions that the user expects completed work with citations, not suggestions or descriptions of what tools could be used.
>>>>>>> 9942e327ce1dc149abe142416c07aadc36c3deec

In chat mode, AnythingLLM automatically retrieves relevant document chunks from the workspace's Qdrant vector database — the dense-only instance running on port 6333, using BGE-M3 embeddings in GGUF Q8_0 format computed by the same embedding model co-loaded alongside Qwen3 in LM Studio. These document chunks appear automatically after a "Context:" separator in the system message on every relevant query with zero explicit tool calls from the model. The retrieval settings use 6,600-character chunks with 1,000-character overlap, retrieving up to 16 snippets at a low similarity threshold, with 30 messages of conversation history. The system prompt instructs the agent to treat this automatically injected context as its primary source of truth — when retrieved context conflicts with training data, the context wins because it is more current and specific to this deployment. Six context behaviors govern usage: cite specific values from retrieved chunks with direct attribution ("Per the architecture reference: port 6333 serves workspace RAG"), acknowledge coverage boundaries explicitly, synthesize across multiple retrieved chunks, handle absent context gracefully, offer training knowledge with caveats in chat mode, and use tools to fill gaps in agent mode.

The core behavioral rules are seven positively framed imperatives — each tells the model what to do, following research showing negative instructions ("avoid X," "never do Y") fail approximately 75% of the time due to the pink elephant effect. The rules: execute then report (call tools and present findings, because the user sees plans as inaction), ground in evidence (lead with workspace context, supplement with tool results), complete the full request in one pass without pausing for confirmation between steps, act-observe-adapt (one tool call then decide the next step based on evidence), inspect before modifying (read files before editing, check state before changing it), recover via fallback chains when tools fail, and extract answers from partial data (when a truncated Fetch contains the answer, deliver it immediately rather than requesting more). The identity paragraph is capability-forward, listing available capabilities to create a behavioral bias toward action.

<!-- Related: RAG context injection, chat mode behavior, core behavioral rules, positive framing, U-curve bookend, execute tools directly, present results not plans, cite your sources, action-forward bookend, workspace agent identity, Qwen3 model settings, temperature 0.1, BGE-M3 embeddings, GGUF Q8_0, context as primary source of truth, dense-only retrieval, port 6333, cite specifics, acknowledge coverage boundaries, synthesize chunks, prime directive, 80K context window, 6600-character chunks, 16 snippets, low similarity threshold, @agent command, AIbitat, chatPrompt nullish coalescing, variable interpolation, v1.9.x pass-through, system prompt processing, workspace identifier, coverage boundary template, citation pattern, agent-mode citation, pink elephant effect, capability-forward identity, complete in one pass, extract partial data, seven rules -->


## How the AIbitat agent framework manages tool execution and what the system prompt deliberately omits

<!-- Verified: 2026-02-14 -->

When the user prefixes a message with @agent, AnythingLLM switches from passive chat mode to active agent mode for that single interaction, activating the AIbitat framework — AnythingLLM's built-in tool-calling orchestration system (sometimes called the agent orchestrator). The @agent session lifecycle proceeds through a defined sequence: the user sends an @agent-prefixed message, AIbitat activates and injects tool definitions alongside the system prompt and conversation history, the model generates either a text response or a tool call, AIbitat executes any tool calls and returns the results to the model, the model evaluates those results and decides whether to make another tool call or deliver a final answer, and this loop continues until the model stops calling tools naturally or reaches AIbitat's hard iteration cap. When the session completes, agent mode deactivates — subsequent messages without the @agent prefix return to regular chat mode. The transition between modes is per-message with no persistent agent state carried between @agent invocations, so each @agent session starts fresh with the full system prompt, current conversation history, and freshly injected tool definitions.

AIbitat provides four critical safety nets that operate automatically during every agent session, requiring zero system prompt instructions to function. The eight-round iteration cap stops runaway tool-calling loops by terminating the agent after eight consecutive rounds of tool calls, preventing situations where the model repeatedly calls the same tool or cycles between tools without making progress toward the user's goal. The dual-mode JSON repair system handles malformed tool call output through two sequential parsing stages: first, AIbitat attempts native API tool call parsing, where the model returns structured tool_call objects in the OpenAI-compatible API response format that Qwen3 supports through its four-stage function calling post-training; if native parsing fails or returns incomplete data, AIbitat falls back to text-based JSON extraction, scanning the raw text response for patterns that resemble tool call JSON and automatically repairing common formatting errors such as missing quotes, trailing commas, or unescaped characters. The deduplication guard tracks tool invocations across rounds and prevents the agent from executing identical tool calls with identical arguments in consecutive rounds, breaking fixation loops where the model attempts the same failing approach repeatedly. The automatic tool schema injection presents all available tools with their parameter schemas, types, and descriptions at the start of each agent session, so the model knows what tools exist and exactly how to call them without any prompt instructions.

These four safety nets explain a deliberate design decision in the system prompt: it contains zero iteration budgets (AIbitat enforces the 8-round cap automatically), zero JSON format rules (AIbitat's dual-mode repair handles both clean and messy tool call output from Qwen3), zero tool schemas or parameter descriptions (AIbitat injects current tool definitions at runtime, meaning the model always sees up-to-date schemas even when tools are added, removed, or reconfigured across AnythingLLM updates), and zero explicit instructions about how to structure tool call responses. Including any of these in the system prompt would waste tokens that compete with RAG context during compression and risk contradicting the framework's actual behavior when AIbitat updates. The system prompt is designed around what AIbitat provides, keeping the prompt lean at approximately 2,000 tokens so the majority of the budget goes to behavioral guidance, context handling patterns, routing categories, and Memory graph protocol rather than redundant technical specifications.

<<<<<<< HEAD
AIbitat's tool definition injection adds approximately 3,000 tokens to the system message when agent mode activates — each of the Docker MCP Gateway's nine tool servers contributes roughly 200 to 400 tokens for its name, description, and parameter schema, and the built-in Web Scraper adds its own definition. These definitions come from the tool servers themselves and update automatically when tools change, which is why the system prompt describes tool categories ("File operations → Filesystem tools," "Shell commands → Desktop Commander") rather than specific tool names and detailed parameter lists. The total agent-mode overhead including the system prompt, tool definitions, RAG snippets, and chat history reaches approximately 25,000 tokens — system prompt at roughly 2,000, tool definitions at roughly 3,000, RAG snippets at roughly 8,000 (16 snippets averaging 500 tokens each), and chat history at roughly 12,000 (30 messages averaging 400 tokens per pair). This leaves approximately 55,000 tokens from the 80,000-token context window for the current exchange. These figures are approximate and vary by session — actual overhead depends on the number and length of retrieved snippets, conversation history depth, and which tools are active. This budget applies to AnythingLLM's context window independently; the LM Studio agent has its own separate context window with its own ~8K RAG budget from port 6334. Extended sessions with verbose tool output can approach this limit, at which point the message array compressor begins silently truncating older content to make room.
=======
AIbitat's tool definition injection adds approximately 3,000 tokens to the system message when agent mode activates — each of the Docker MCP Gateway's nine tool servers contributes roughly 200 to 400 tokens for its name, description, and parameter schema, and the built-in Web Scraper adds its own definition. These definitions come from the tool servers themselves and update automatically when tools change, which is why the system prompt describes tool categories ("File operations → Filesystem tools," "Shell commands → Desktop Commander") rather than specific tool names and detailed parameter lists. The total agent-mode overhead including the system prompt, tool definitions, RAG snippets, and chat history reaches approximately 25,000 tokens — system prompt at roughly 2,000, tool definitions at roughly 3,000, RAG snippets at roughly 8,000 (16 snippets averaging 500 tokens each), and chat history at roughly 12,000 (30 messages averaging 400 tokens per pair). This leaves approximately 55,000 tokens from the 80,000-token context window for the current exchange. Extended sessions with verbose tool output can approach this limit, at which point the message array compressor begins silently truncating older content to make room.
>>>>>>> 9942e327ce1dc149abe142416c07aadc36c3deec

<!-- Related: AIbitat framework, agent orchestrator, @agent command, session lifecycle, iteration cap, 8-round limit, JSON repair, dual-mode parsing, native tool calling, text-based JSON extraction, deduplication guard, tool schema injection, deliberate omissions, prompt design, lean system prompt, token budget, tool definitions overhead, 25K overhead, 55K remaining, 80K context, message array compressor, tool category routing, Qwen3 function calling, four-stage post-training, agent mode activation, per-message mode switching, runaway loop prevention, tool call repair, workspace agent framework, safety nets, OpenAI-compatible API, tool_call objects -->


## How the agent routes queries to tools, selects reasoning modes, and handles multi-step tasks in agent mode

<!-- Verified: 2026-02-14 -->

<<<<<<< HEAD
The system prompt provides category-based query routing guidance that maps question types to tool categories rather than to specific tool names. This approach is more durable than hardcoded tool references because AIbitat injects the actual tool definitions at runtime and tool names may change across updates. At every decision point, Qwen3 first evaluates whether the auto-injected RAG chunks from port 6333 already contain a sufficient answer — if so, the model responds directly with zero tool calls, which is the most common path in practice. When RAG context is insufficient, the routing categories direct questions about stack facts, port numbers, and configuration values to the Memory knowledge graph first, because graph queries via search_nodes complete in milliseconds, faster than waiting for RAG to surface the same fact from document chunks. Questions about external library or API documentation go to Context7 using the two-step resolve-library-id then get-library-docs sequence, fetching live documentation that may differ from training data. Questions requiring real-time data — commodity prices, exchange rates, sports scores, current events, or any query about a tradeable asset — go to Tavily first as a mandatory tool call before any answer text is generated (agent mode only — in chat mode without @agent, the model has no tool access and must state that live data requires @agent mode rather than generating a plausible-sounding answer from training data), because training knowledge is guaranteed stale for time-sensitive data and generating an answer first creates an anchoring effect that persists even when the tool returns different information. Requests for specific URL content where the URL is already known go to the Fetch tool (clean markdown output) or AnythingLLM's built-in Web Scraper (native, zero Docker dependency). Fetch returns pages in segments; when truncated content already contains the answer, the agent delivers it immediately — requesting more content wastes iterations. The system prompt specifies concrete fallback chains: web content goes Tavily → Fetch → Playwright; library docs go Context7 → Tavily → Fetch; local knowledge goes Memory → workspace context → Filesystem. File read, write, search, and list operations use Filesystem tools. Shell commands, Docker management, process management, container status checks, and service health verification use Desktop Commander. Complex multi-step reasoning tasks that benefit from structured planning before execution use Sequential Thinking as a scratchpad to organize the approach before acting with other tools.
=======
The system prompt provides category-based query routing guidance that maps question types to tool categories rather than to specific tool names. This approach is more durable than hardcoded tool references because AIbitat injects the actual tool definitions at runtime and tool names may change across updates. At every decision point, Qwen3 first evaluates whether the auto-injected RAG chunks from port 6333 already contain a sufficient answer — if so, the model responds directly with zero tool calls, which is the most common path in practice. When RAG context is insufficient, the routing categories direct questions about stack facts, port numbers, and configuration values to the Memory knowledge graph first, because graph queries via search_nodes complete in milliseconds, faster than waiting for RAG to surface the same fact from document chunks. Questions about external library or API documentation go to Context7 using the two-step resolve-library-id then get-library-docs sequence, fetching live documentation that may differ from training data. Questions requiring real-time data — commodity prices, exchange rates, sports scores, current events, or any query about a tradeable asset — go to Tavily first as a mandatory tool call before any answer text is generated, because training knowledge is guaranteed stale for time-sensitive data and generating an answer first creates an anchoring effect that persists even when the tool returns different information. Requests for specific URL content where the URL is already known go to the Fetch tool (clean markdown output) or AnythingLLM's built-in Web Scraper (native, zero Docker dependency). Fetch returns pages in segments; when truncated content already contains the answer, the agent delivers it immediately — requesting more content wastes iterations. The system prompt specifies concrete fallback chains: web content goes Tavily → Fetch → Playwright; library docs go Context7 → Tavily → Fetch; local knowledge goes Memory → workspace context → Filesystem. File read, write, search, and list operations use Filesystem tools. Shell commands, Docker management, process management, container status checks, and service health verification use Desktop Commander. Complex multi-step reasoning tasks that benefit from structured planning before execution use Sequential Thinking as a scratchpad to organize the approach before acting with other tools.
>>>>>>> 9942e327ce1dc149abe142416c07aadc36c3deec

The query routing and agent mode guidance sections are positioned in the middle of the system prompt, between the chat-mode-critical context-handling instructions at the top and the output quality directive and bookend at the bottom. This placement exploits the U-shaped attention curve where transformer models pay strongest attention to the beginning and end of a prompt with diminishing focus in the middle. Agent-mode instructions — routing categories, tool attribution patterns, thinking mode guidance — receive useful attention weight when agent mode is active but are merely harmless dead weight in regular chat mode, where the model has no tools available and needs no routing guidance. The premium start and end positions hold context-handling instructions that matter in every interaction: the opening identity and context citation at the top, the output quality and bookend at the bottom.

The hybrid thinking mode available in Qwen3 interacts with agent mode through two explicit routing categories in the system prompt. The /think mode engages extended internal chain-of-thought reasoning for multi-step problems, debugging, architecture analysis, code review, multi-tool planning, synthesizing conflicting information, and complex error diagnosis. The /no_think mode bypasses extended reasoning for direct answers, status checks, single-tool calls, formatting, lookups, confirmations, and simple file reads. The system prompt lets the model decide based on query complexity rather than imposing a fixed default — most workspace agent queries are direct lookups or single-tool checks where /no_think is appropriate, but the model escalates to /think when the task matches the extended reasoning profile. When engaging /think mode, the chain-of-thought reasoning consumes an additional 200 to 800 tokens per turn — affordable at 80K context but worth conserving during extended sessions by reserving /think for synthesis.

<<<<<<< HEAD
Before starting multi-step tasks in agent mode, the agent states its intended approach so the user understands the plan. After each tool result, the agent assesses whether it has enough information to answer and cites findings with tool attribution: "Desktop Commander confirms: port 6333 is responding." "Filesystem shows: config.json contains the correct endpoint." When tools return URLs — from Tavily, Fetch, Context7, Playwright, or the Web Scraper — the agent threads them into the response as inline markdown links (`[source name](url)`), one link per source. When a tool returns no URL (Memory, Desktop Commander, Filesystem), the agent cites the tool name and key detail inline. The system prompt specifies proportional response depth as a hard constraint — providing unrequested depth is a quality failure equivalent to providing incorrect information. Simple lookups (a price, a status check, a single fact) receive 1 to 3 sentences with a source link — no unrequested unit conversions, no background analysis, no market commentary. Moderate questions (how-tos, comparisons) receive 1 to 2 short paragraphs. Complex tasks receive full structured responses with multi-source evidence. The agent ends with the answer — unsolicited follow-up offers ("Let me know if you'd like..." or "Would you like me to...") waste context tokens and are treated as quality failures. The trust hierarchy also applies to dates and timestamps: when a tool returns data with a current date, that date is correct regardless of what the model's training data believes. Citation integrity is enforced: the agent only cites a tool as a source when that tool was actually called in the current @agent turn and the cited data came from its result — writing "per Tavily" for data from training knowledge is a hallucinated citation that destroys the user's ability to assess reliability. In chat mode (without @agent), no tools are available, so no tool citations are possible — any response that includes a tool attribution or source-name citation in chat mode is necessarily fabricated. The agent captures these cited findings directly in its response text rather than relying on the raw tool output to persist — the message array compressor may truncate older tool results during extended sessions, so the agent's own written responses serve as the most durable record of what was discovered. A lightweight behavioral nudge supplements AIbitat's built-in deduplication: if a tool fails or returns the same result twice, the agent switches to the next option in the routing chain immediately rather than retrying the same approach.
=======
Before starting multi-step tasks in agent mode, the agent states its intended approach so the user understands the plan. After each tool result, the agent assesses whether it has enough information to answer and cites findings with tool attribution: "Desktop Commander confirms: port 6333 is responding." "Filesystem shows: config.json contains the correct endpoint." When tools return URLs — from Tavily, Fetch, Context7, Playwright, or the Web Scraper — the agent threads them into the response as inline markdown links (`[source name](url)`), one link per source. When a tool returns no URL (Memory, Desktop Commander, Filesystem), the agent cites the tool name and key detail inline. The system prompt specifies proportional response depth as a hard constraint — providing unrequested depth is a quality failure equivalent to providing incorrect information. Simple lookups (a price, a status check, a single fact) receive 1 to 3 sentences with a source link — no unrequested unit conversions, no background analysis, no market commentary. Moderate questions (how-tos, comparisons) receive 1 to 2 short paragraphs. Complex tasks receive full structured responses with multi-source evidence. The agent ends with the answer — unsolicited follow-up offers ("Let me know if you'd like..." or "Would you like me to...") waste context tokens and are treated as quality failures. The trust hierarchy also applies to dates and timestamps: when a tool returns data with a current date, that date is correct regardless of what the model's training data believes. Citation integrity is enforced: the agent only cites a tool as a source when that tool was actually called in the current turn and the cited data came from its result — writing "per Tavily" for data from training knowledge is a hallucinated citation that destroys the user's ability to assess reliability. The agent captures these cited findings directly in its response text rather than relying on the raw tool output to persist — the message array compressor may truncate older tool results during extended sessions, so the agent's own written responses serve as the most durable record of what was discovered. A lightweight behavioral nudge supplements AIbitat's built-in deduplication: if a tool fails or returns the same result twice, the agent switches to the next option in the routing chain immediately rather than retrying the same approach.
>>>>>>> 9942e327ce1dc149abe142416c07aadc36c3deec

<!-- Related: agent mode, query routing, tool categories, category-based routing, Memory graph, search_nodes, Context7, resolve-library-id, get-library-docs, Fetch, Tavily, Tavily first choice, real-time data, commodity prices, Fetch truncation, fallback chains, Tavily Fetch Playwright, Context7 Tavily Fetch, Web Scraper, Filesystem, Desktop Commander, Sequential Thinking, U-curve placement, attention curve, thinking mode, /think, /no_think, hybrid thinking, Qwen3 reasoning, extended reasoning, fast response, multi-step tasks, tool attribution, citation pattern, markdown links, message array compressor, deduplication, routing chain, agent mode guidance, chat mode priority, execute don't narrate, present results not plans, bookend, proportionality hard constraint, quality failure, no follow-up offers, trust tool dates, end with the answer, citation integrity, hallucinated citation, tool-call-first, time-sensitive queries, anchoring effect, call before answer -->


## Architecture boundaries including what belongs to which Qdrant instance and which tools are exclusive to each agent

<!-- Verified: 2026-02-14 -->

The workspace agent's architecture involves two completely independent Qdrant vector database containers that serve separate purposes and remain strictly isolated from each other. AnythingLLM connects to its own dedicated Qdrant vector database on port 6333, which stores workspace documents using dense-only embeddings — cosine similarity search against the 1024-dimensional dense vectors from BGE-M3, with no sparse or keyword-based retrieval component. This is the agent's own retrieval system — it fires automatically on every relevant query in chat mode, retrieving up to 16 document snippets at a low similarity threshold from 6,600-character chunks with 1,000-character overlap. The document chunks stored in this Qdrant instance form the "Context:" payload that appears in the system message. AnythingLLM manages this instance entirely through its own document processing pipeline — ingesting files through the web UI at localhost port 3001, computing embeddings via the BGE-M3 model in GGUF Q8_0 format through LM Studio's /v1/embeddings endpoint, chunking documents using a recursive text splitter that respects paragraph boundaries, and storing the resulting vectors. The embedding model's maximum input window is 8,192 tokens, and AnythingLLM's embed chunk length setting of 32,768 characters (approximately 8,192 tokens) matches this limit so that chunks are embedded in full without silent truncation.

The LM Studio agent connects to a separate Qdrant vector database on port 6334, which uses hybrid dense-plus-sparse search via Reciprocal Rank Fusion (also called RRF, a technique that combines BGE-M3's dense semantic embeddings with its learned sparse lexical weights for retrieval that catches both meaning-based and keyword-based matches). It serves the LM Studio agent's reference documents through the custom qdrant-rag MCP server (qdrant_mcp_server.py, approximately 1,820 lines, running natively on the host machine). The AnythingLLM workspace agent works exclusively with the port 6333 instance — port 6334 belongs to a separate Docker container with separate data volumes, separate collection configurations (the lmstudio_docs collection with source_dir payload filtering), and a completely separate indexing pipeline using deterministic section-header-based chunking that splits on every ## header with zero overlap. The qdrant-rag MCP server uses the FlagEmbedding library for both dense and sparse embedding computation entirely on CPU, consuming approximately 2.3 GB of system RAM and zero GPU VRAM.

Both agents connect to the same LM Studio inference endpoint at localhost port 1234 for text generation and embedding computation. LM Studio runs the Qwen3-30B-A3B model on the RTX 5090 GPU, consuming approximately 25.1 GB of VRAM for model weights, plus roughly 3.75 GB for the KV cache at 80K context with Q8_0 quantized cache, plus 0.6 GB for the co-loaded BGE-M3 GGUF embedding model, plus approximately 1.0 GB of CUDA runtime overhead — totaling approximately 30.5 GB utilization with roughly 1.5 GB of headroom. Because both agents share the same inference endpoint, requests queue at LM Studio when both attempt inference simultaneously. DyTopo swarm agent calls also route through this same endpoint, which means active swarms create additional queuing that can delay responses for both frontends.

Two categories of tools are exclusively available to the LM Studio agent. The DyTopo swarm tools (Dynamic Topology, based on arXiv 2602.06039, using MiniLM-L6-v2 on CPU for semantic routing between specialized agent teams) — swarm_start for launching multi-agent swarms across three domains (code, math, and general), swarm_status for polling progress, and swarm_result for retrieving completed results — enable multi-agent collaboration. The five rag_search tools (rag_search, rag_status, rag_reindex, rag_sources, rag_file_info) query and manage the hybrid RAG pipeline on port 6334. These eight tools exist only in LM Studio's MCP tool inventory. The Docker MCP Gateway provides AnythingLLM with access to nine containerized tool servers: Desktop Commander for shell commands and service checks, Filesystem for file operations, Memory for the shared knowledge graph, Context7 for library documentation, Tavily for web search, Fetch for URL content retrieval, Playwright for browser automation, Sequential Thinking for structured reasoning, and n8n for workflow automation. AnythingLLM also has a built-in Web Scraper that operates as a native capability independent of the MCP Gateway, offering URL-to-text conversion with zero Docker dependency.

<!-- Related: Qdrant ports, architecture diagram, dense-only search, hybrid search, port 6333, port 6334, BGE-M3, GGUF embedding, FlagEmbedding, Docker containers, DyTopo swarm tools, rag_search, MCP Gateway, tool availability, LM Studio endpoint, Reciprocal Rank Fusion, RRF, workspace RAG, container isolation, VRAM budget, RTX 5090, Q6_K, source directory filtering, Web Scraper, Desktop Commander, Filesystem, Context7, Tavily, Fetch, Playwright, Sequential Thinking, n8n, 25.1 GB, 30.5 GB, passive vs active RAG, arXiv 2602.06039, qdrant_mcp_server.py, port 1234, port 3001, inference queuing, lmstudio_docs collection, MiniLM-L6-v2, CUDA overhead, KV cache -->


## How the shared Memory knowledge graph works and how context compression affects information preservation

<!-- Verified: 2026-02-14 -->

The Memory knowledge graph is a persistent shared data store that both the AnythingLLM workspace agent and the LM Studio agent read from and write to through the same MCP Memory server running in the Docker MCP Gateway. When either agent stores a fact — such as a port mapping, a configuration decision, or an error resolution — that fact becomes immediately available to the other agent in subsequent queries. This shared graph enables cross-agent continuity: the LM Studio agent might record that a Qdrant collection was rebuilt with new index settings, and the AnythingLLM agent can retrieve that fact the next time a user asks about collection configuration. The Memory graph also serves as the fastest source for stable facts during query routing — the system prompt instructs the agent to check Memory first for stack facts, port numbers, and configuration values because graph lookups via the search_nodes tool complete in milliseconds, faster than waiting for RAG retrieval to surface the same information from document chunks.

Maintaining consistency in this shared resource is critical because fragmented or duplicated entities degrade retrieval quality for both agents. The naming conventions prevent entity fragmentation: entity names always use PascalCase (QdrantMCP, BGEm3Config, AnythingLLMWorkspace, RTX5090Hardware, DyTopoSwarm), entity types always use snake_case (service_config, architecture_decision, project_preference, error_resolution, port_mapping), and relation names always use snake_case (serves, depends_on, replaced_by, configured_in, relates_to, writes_to). Both agents use identical naming conventions defined by identical text in both system prompts, ensuring the shared graph remains searchable and consistent regardless of which agent created an entity.

The most important operational rule for the Memory graph is search-before-create: before creating any new entity, the agent always calls search_nodes first to check whether the entity or a closely named variant already exists. If the entity does exist, the agent uses add_observations to append new facts to the existing entity rather than creating a duplicate. This search-before-create discipline prevents the gradual accumulation of near-duplicate entities (such as "QdrantConfig" and "QdrantConfiguration" and "QdrantSetup") that dilute search results and create confusion about which version contains the most current information. Information appropriate for Memory storage includes stable facts about the stack such as port mappings and endpoint URLs (port 1234 for LM Studio inference, port 6333 for AnythingLLM's Qdrant, port 6334 for LM Studio's Qdrant), collection names, user preferences, project decisions and their rationale, architecture choices, resolved error patterns with their solutions, and useful reference URLs. Information that belongs elsewhere includes transient conversation context, speculative or unverified information, secrets and passwords, and entire file contents — for files, store the path instead because paths use fewer tokens and stay current when contents change.

AnythingLLM includes a message array compressor (the messageArrayCompressor) that activates automatically when the conversation approaches the context window limit. This compression system manages the available token budget by proportionally allocating space across the system prompt, chat history, and user content. At the full 80,000-token context window with 16 retrieved snippets and 30 messages of chat history, fixed overhead totals approximately 25,000 tokens (system prompt at roughly 2,000, tool definitions at roughly 3,000, RAG snippets at roughly 8,000, chat history at roughly 12,000), leaving roughly 55,000 tokens for the current exchange. During extended agentic sessions with verbose tool output, compression can engage and silently truncate older content — raw tool output and older RAG chunks may be removed from the context window without any notification to the model.

The critical implication is that the agent's own response text — being more recent — survives compression better than raw tool output because the compressor preserves recent messages at the expense of older ones. The system prompt instructs the agent to incorporate discoveries into its written response text as it goes, treating each response as a durable summary: recording specific values, confirmed states, error messages, and file paths so they persist even when raw tool output is compressed away in later turns. This "write it down as you go" strategy is the primary defense against information loss during long conversations. The system prompt is designed with front-loaded critical instructions to account for compression — the HOW TO USE CONTEXT section and core behavioral rules appear near the top because compression trims the system prompt from the end when space is tight, preserving the most important guidance. On cold start in agent mode, the agent follows a brief orientation: check Memory via search_nodes for recent session context and stable facts, let workspace RAG provide architectural grounding automatically through the Context: separator, and confirm live service status with Desktop Commander if the task involves infrastructure. Issues discovered during orientation are reported before proceeding with the user's request.

<!-- Related: Memory graph, knowledge graph, entity naming conventions, PascalCase entity names, snake_case relations, search_nodes, add_observations, create_entities, shared memory between agents, graph fragmentation, search-before-create, cross-agent continuity, context compression, messageArrayCompressor, context window management, token budget, front-loading instructions, compression survival, cold start orientation, output quality, session start, write it down strategy, 80K context, 25K overhead, 55K remaining, port mappings, error resolution, stable facts, entity types, relation names, Memory discipline, information preservation, truncation, compressor proportional allocation, system prompt trimming -->
