# AnythingLLM Agent Error Recovery and Diagnostic Guide

This reference document provides symptom-first diagnostic flows for the most common operational problems in the Loom stack. Each section starts with the symptom a user would report, describes the likely causes, and walks through the diagnostic and remediation sequence. Formatted for dense-only RAG retrieval on port 6333 — symptom descriptions are written to embed near the queries a user would naturally type when experiencing each problem.


## What to do when the agent stops after only a few tool calls without completing the task or hits the iteration limit

<!-- Verified: 2026-02-14 -->

When the AnythingLLM workspace agent stops responding after only two or three tool calls, or when the agent appears to give up on a multi-step task before reaching a satisfactory conclusion, the cause is almost always AIbitat's eight-round iteration cap. This hard limit terminates the agent's tool-calling loop after eight consecutive rounds of tool execution, regardless of whether the task is complete. The cap exists to prevent runaway loops where the model calls tools indefinitely, but it also means that genuinely complex tasks requiring more than eight tool call rounds will be cut off before completion.

The first diagnostic step is recognizing whether the iteration cap was reached. If the agent's final response ends abruptly without a clear conclusion or summary, the cap likely triggered a forced stop. If the agent explicitly says something like "I've reached the limit of what I can investigate in this session," it is acknowledging the cap. The agent cannot increase or disable the 8-round limit — it is a built-in AIbitat safety net that the system prompt deliberately does not try to override, because the alternative (no limit) risks infinite tool-calling loops that waste tokens and time.

The primary remediation is task decomposition: break the original request into smaller sub-tasks that each require fewer than eight tool calls. Instead of asking "@agent debug why the entire stack is slow and fix everything," ask targeted questions: "@agent check if the Qdrant container on port 6333 is running," then "@agent check VRAM usage with nvidia-smi and report the current memory allocation," then "@agent read the mcp.json configuration and verify all endpoints are correct." Each focused @agent invocation gets its own fresh 8-round budget. The agent can then synthesize findings across multiple @agent sessions.

The second remediation is front-loading efficiency in tool usage. Specific tool workflows consume predictable iteration budgets based on the execution patterns observed in practice. NOTE: The following iteration budgets reference MCP tools (Desktop Commander, Playwright, n8n), not AnythingLLM Agent Skills. AnythingLLM's own Agent Skills may have different iteration costs. For reference, MCP tool budgets are: a system diagnostic with Desktop Commander uses 4 to 5 iterations (inspect → restart → verify → write log → synthesize); a Playwright browser automation sequence uses 6 to 7 iterations (navigate → login fields × 3 → click → read → interpret), leaving almost no room for error recovery; an n8n workflow trigger uses 3 to 4 iterations (list → execute → poll → synthesize). The agent should plan its approach with these budgets in mind, gathering the most diagnostic information per tool call rather than making narrow, single-purpose calls. Running docker ps reveals the status of all containers at once rather than checking each container individually. Running nvidia-smi provides a complete GPU memory snapshot rather than checking individual allocations. Reading an entire configuration file provides all settings at once rather than searching for each value separately. The system prompt's act-observe-adapt rule supports this — each tool call should yield maximum new information.

A specific iteration-burning pattern involves Fetch truncation loops. When Fetch returns truncated content with a continuation prompt ("Content truncated. Call the fetch tool with a start_index of N..."), the agent should first check whether the truncated content already contains the answer — if so, deliver the answer immediately rather than requesting more content. If the answer genuinely requires more data, one additional Fetch call with the start_index is reasonable. After three truncation cycles on the same URL, the page is too large or JavaScript-heavy for Fetch — switch to Tavily for a structured answer or report what was found so far. Financial data sites (investing.com, bloomberg.com) and JavaScript-heavy dashboards consistently trigger this pattern; call Tavily directly for market data, exchange rates, and live prices instead of attempting Fetch.

A related symptom is the agent making the same tool call repeatedly without making progress. AIbitat's built-in deduplication guard catches identical tool calls with identical arguments in consecutive rounds, but the guard does not catch calls with slightly different arguments that produce the same result (such as searching Memory for "Qdrant" then "qdrant" then "QdrantConfig" when the entity uses PascalCase "QdrantMCP"). The system prompt includes a behavioral nudge that supplements the deduplication guard: if a tool fails or returns the same result twice, the agent should switch to the next option in the routing chain immediately. When the user observes the agent repeating the same approach, they can redirect it: "@agent try checking the Filesystem (MCP Filesystem tool — for reading, writing, searching, and listing files. NOT Windows Explorer.) directly instead of searching Memory again."

Another variant of premature termination happens when the agent stops calling tools before reaching the iteration cap — it may have "decided" the task is complete when it is not, or it may have encountered an error it did not recognize as recoverable. The MCP fallback chain behavior (Tavily → Fetch → Playwright for web content, Memory → workspace context → Filesystem for local knowledge, Context7 → Tavily → Fetch for library docs) keeps the agent moving through alternatives when one MCP tool fails. AnythingLLM's Agent Skills have their own fallback behavior based on whichever skills are enabled in the workspace — the MCP fallback chains listed here do not apply to AnythingLLM. If the agent stopped after a tool error without trying fallbacks, the user can prompt it to continue: "@agent the last tool call failed — try using Desktop Commander to check the file system directly." The agent's core rule "recover via fallback chains" means it should try alternative tools before giving up, but complex error scenarios sometimes cause the model to stop prematurely, especially at lower temperatures (0.1) where the model is more conservative in its outputs.

<!-- Related: iteration limit, 8-round cap, AIbitat iteration cap, agent stops early, premature termination, task decomposition, deduplication guard, fallback chain, tool error recovery, front-loading efficiency, iteration budget, @agent sessions, act-observe-adapt, runaway loop prevention, Fetch truncation, truncation loop, start_index, JavaScript-heavy sites, Tavily for market data, three truncation cycles, deliver partial answer -->


## What to do when tool calls produce malformed JSON or the agent fails to call tools correctly

<!-- Verified: 2026-02-14 -->

When the workspace agent produces garbled tool call output, calls a tool with incorrect parameter names, or generates what looks like a tool call in the response text but does not actually trigger tool execution, the cause is typically a mismatch between the model's tool calling format and what AIbitat's parser expects. AIbitat provides a dual-mode JSON repair system — native API parsing followed by text-based extraction as a fallback — but this system has limits, and certain types of malformation can slip through both repair stages.

Every agent flow operates through a three-layer error handling architecture. The first layer is the JSON repair mechanism: AIbitat's dual-mode parser attempts native API tool call parsing first (the primary path where Qwen3-30B-A3B generates tool_call objects in the OpenAI-compatible API response format, supported by its four-stage post-training pipeline optimized for tool-use and structured JSON). If native parsing fails, AIbitat falls back to text-based JSON extraction, scanning the raw text for patterns resembling tool calls and repairing common errors — missing closing quotes, trailing commas, unescaped characters, incomplete JSON structures. This repair layer operates automatically with zero LLM involvement. The second layer handles tool execution errors: when a tool call succeeds in parsing but fails during execution (network timeout, 404, file not found, command error), the error string is fed back to Qwen3 as the tool result, and the model adapts by trying an alternative approach. The third layer is the iteration check: AIbitat verifies whether the 8-round cap has been reached and either allows the loop to continue or forces termination with the best partial result.

Malformation that escapes both repair stages typically involves structural issues: the model generates a tool name that does not match any registered tool (AIbitat has no fuzzy matching — tool names must be exact), the model generates parameters under a different key name than the tool schema specifies, or the model embeds a tool call inside a markdown code fence that prevents AIbitat from recognizing it as an actionable call. These failures are more common during extended sessions where the context window is near capacity, because the model's attention on the tool definition injection (approximately 3,000 tokens in the system message) degrades as the total context grows and the message array compressor begins truncating older content.

The diagnostic approach depends on what the user observes. If the agent's response contains text that looks like a tool call but the tool never executed (the user does not see a tool result in the conversation), the text-based fallback also failed to parse it. The user can try rephrasing their request more specifically: "@agent use Desktop Commander to run docker ps" is more likely to produce a clean tool call than "@agent check the containers" because the explicit tool name primes the model's tool call generation. If the agent calls a tool but with wrong parameters, resulting in an error message, the agent should recognize the error and retry with corrected parameters — the core rule "act-observe-adapt" means reading the error and adjusting. If the agent does not self-correct after a parameter error, the user can instruct it directly: "@agent the previous call failed because the parameter name is 'query' not 'search_query' — try again."

The "chat history impacts output of JSON" drift pattern is a related issue where extended conversation history gradually degrades tool call quality. As the conversation grows, earlier messages containing non-JSON content (natural language responses, user questions, previous tool results) create a growing body of non-structured text that influences the model's generation patterns, making it more likely to produce hybrid text-and-JSON output rather than clean tool calls. This drift is more pronounced at the 0.1 temperature setting because the model has less room to deviate from the accumulated distribution. The remediation is starting a fresh conversation, which resets both the conversation history and the KV cache — the first few messages in a new conversation typically produce clean tool calls because there is minimal non-structured context to influence generation.

Persistent tool calling failures that recur across fresh sessions may indicate a deeper issue: a tool server that is not responding (check container status with docker ps), a tool definition that has changed between updates (AIbitat injects current definitions at runtime, but if the tool server is misconfigured, the definition may be missing or incorrect), or a model quantization issue where the Q4_K_M GGUF weights have subtle attention layer effects that surface with specific tool schemas. For quantization-related issues, verifying that repeat_penalty is set to 1.0 (OFF) is important — any value above 1.0 penalizes repeated structural tokens like curly braces, quotes, and commas, which corrupts JSON output.

<!-- Related: malformed JSON, tool call failure, JSON repair, dual-mode parsing, native API parsing, text-based extraction, AIbitat JSON repair, tool name mismatch, parameter errors, Qwen3 function calling, four-stage post-training, structured JSON output, markdown code fence, tool schema, chat history drift, JSON drift, extended conversation, temperature 0.1, fresh conversation, KV cache reset, repeat penalty, Q4_K_M GGUF quantization, tool definition injection, tool server down, Docker container, parameter correction, act-observe-adapt, self-correction, text-based fallback, OpenAI-compatible, fuzzy matching, context degradation -->


## What to do when connections fail to ports 8008 or 6333 and how to diagnose service outages

<!-- Verified: 2026-02-16 -->

Connection failures to the AnyLoom stack's two primary service ports — 8008 (llama.cpp inference API) and 6333 (Qdrant vector database) — each have distinct causes and remediation paths. When the user reports "connection refused" or "timeout" errors, the diagnostic starts with identifying which port is affected and whether the service behind it is down, unreachable, or responding slowly.

Port 6333 connection refused means the Qdrant container (`anyloom-qdrant`) is not running or not accessible. This directly affects both the workspace agent's automatic RAG retrieval and the MCP hybrid RAG pipeline — without the port 6333 Qdrant instance, AnythingLLM cannot retrieve workspace documents (the Context: section in the system message will be empty or missing), and rag_search calls from MCP tools will also fail. The diagnostic sequence: run docker ps via Desktop Commander to check the container named `anyloom-qdrant`; if it is missing from docker ps output, check docker ps -a for stopped containers with exit codes; if the container exists but is stopped, run `docker start anyloom-qdrant` to restart it; if it does not exist at all, it needs to be recreated using the project's docker-compose.yml. Common causes for container stoppage include host reboot (Docker Desktop may not auto-start all containers), Docker Desktop itself not running (the containers cannot start without the Docker engine), port conflict (another process claimed port 6333 — check with `netstat -ano | findstr 6333` on Windows), and disk space exhaustion on the Docker volume (check with `docker system df`). After restarting the container, verify it is healthy by checking that AnythingLLM's RAG retrieval is working again — the next chat message should include Context: content if workspace documents are properly indexed. The rag_status MCP tool can also verify collection health and confirm both Qdrant connectivity and collection integrity.

Port 8008 connection refused or timeout means the llama.cpp container (`anyloom-llm`) is not running or has crashed. This is the most disruptive failure because it affects all inference in the stack — AnythingLLM loses access to chat completions and embeddings, DyTopo swarms cannot execute, and AnythingLLM cannot embed new documents. The diagnostic sequence: run docker ps to check the container named `anyloom-llm`; if it is stopped, check docker logs for the exit reason; restart with `docker start anyloom-llm` or `docker compose up -d anyloom-llm`. If llama.cpp crashed, the crash may have been caused by VRAM exhaustion — check nvidia-smi for current GPU memory state. At 131K context with Q4_K_M GGUF weights, the stack uses approximately ~18.6 GiB of the RTX 5090's 32 GB VRAM with ample headroom; if another GPU-consuming process is running (a game, another model, a rendering task), there may not be enough VRAM. From the host, verify the API is responding: `curl http://localhost:8008/v1/models` should return the loaded model list. Other containers reach llama.cpp at `http://anyloom-llm:8080/v1` on the Docker network.

Timeout errors (the port accepts connections but responses never arrive) on port 8008 are different from connection refused errors and typically indicate inference congestion rather than a downed service. The most common cause is an active DyTopo swarm generating heavy API traffic — each swarm round produces multiple inference calls that queue ahead of interactive requests. The user can check for active swarms using swarm_status via MCP tools. Another cause is the model being stuck in a very long generation (a runaway response with no stop condition). VRAM pressure at high context lengths can also cause extreme slowness without an outright crash — the model continues functioning but at dramatically reduced speed as it approaches memory limits.

<!-- Related: connection refused, port 8008, port 6333, service outage, Docker container down, Qdrant container, llama.cpp crash, docker ps, docker start, docker logs, container restart, port conflict, netstat, disk space, Docker Desktop, VRAM exhaustion, nvidia-smi, Qwen3-30B-A3B, inference timeout, DyTopo swarm congestion, swarm_status, KV cache, inference queuing, host reboot, anyloom-qdrant, anyloom-llm, container health, RAG retrieval broken, Context separator missing, service diagnostics, API server, docker-compose, single Qdrant instance -->


## What to do when the agent ignores the workspace system prompt or context compression silently drops important information

<!-- Verified: 2026-02-14 -->

When the workspace agent appears to ignore instructions from the system prompt — not citing sources, not using tools in the expected order, not following the Memory naming conventions, or producing responses that contradict the system prompt's behavioral rules — the cause usually falls into one of three categories: context compression has truncated the system prompt, the relevant instruction is in a low-attention position in the prompt, or the model's training data is overriding the prompt instruction.

The message array compressor (messageArrayCompressor) activates automatically when the conversation approaches the 131,072-token context window limit. This compression system proportionally allocates the available budget across the system prompt, chat history, and user content. When compression activates, it trims content starting with the oldest and least recent material. The system prompt is protected — it receives a proportional allocation — but in extreme cases where tool output and conversation history consume the vast majority of the available budget, the system prompt itself can be trimmed from the end. This is why the system prompt's critical instructions are front-loaded: the HOW TO USE CONTEXT section and core behavioral rules appear at the very beginning, while the MEMORY section, OUTPUT AND CITATION section, and bookend appear later. If compression trims the prompt's tail, the Memory naming conventions, citation patterns, and the closing bookend may be lost — explaining why the agent stops following those specific instructions while continuing to follow the context-handling rules at the top.

The diagnostic approach starts with estimating context usage. If the conversation has been running for many turns with extensive tool output (multiple file reads, long Desktop Commander results, large Filesystem (MCP Filesystem tool) search outputs), compression is likely active. The agent's own response quality degradation is itself a symptom — if earlier responses cited sources correctly but recent responses do not, compression probably dropped the citation instructions. The remediation is starting a fresh conversation, which resets both the conversation history and the context budget, restoring the full system prompt. The agent can also proactively manage context by summarizing findings into its response text as it goes (the "write it down as you go" strategy), keeping individual tool calls focused to minimize output size, and avoiding verbose commands that generate large outputs when concise alternatives exist.

The U-shaped attention curve creates a second category of instruction-following failures. Transformer models pay strongest attention to the first and last content in a long prompt, with diminishing attention in the middle. The system prompt places chat-mode-critical instructions (context handling, core rules) at the beginning and end, and agent-mode-specific instructions (routing, Sequential Thinking guidance, thinking mode selection) in the middle. In chat mode, the middle instructions receive the least attention, which is by design — they are irrelevant during chat mode. In agent mode, these middle instructions receive adequate attention because the model is actively processing tool-related content. The failure pattern is: an instruction in the middle of the system prompt works correctly in agent mode but is ignored in chat mode. This is expected behavior, not a bug — the prompt architecture deliberately trades middle-section attention for stronger top-and-bottom attention on the instructions that matter most in chat mode.

The third category is training data override, where the model's pre-training or instruction-tuning produces behaviors that conflict with the system prompt. This is most visible with Memory naming conventions — the model may default to snake_case entity names (qdrant_mcp) instead of the PascalCase convention (QdrantMCP) specified in the prompt, because snake_case is more common in the model's training data. Similarly, the model may default to generic citation patterns ("According to my knowledge...") instead of the specific attribution patterns in the prompt ("Per the architecture reference: ..."). These overrides are more likely at lower temperatures (0.1) where the model generates the highest-probability tokens, which align with training data patterns. The remediation is reinforcement through consistent user feedback — when the agent uses wrong naming, the user corrects it ("use PascalCase for entity names — QdrantMCP, not qdrant_mcp"), and the correction appears in the conversation history as an in-context example that guides subsequent behavior within that session.

When the agent ignores the system prompt entirely — responding as if no system prompt exists, introducing itself with a generic identity, or failing to use workspace context — the most likely cause is the chatPrompt() nullish coalescing behavior. If the workspace system prompt field is null or undefined (not empty string, but actually null), AnythingLLM falls back to its hardcoded default prompt instead of the custom Loom prompt. Verify that the system prompt is actually set in the workspace settings by checking the workspace configuration through the AnythingLLM web interface at localhost port 3001. If the field shows the correct prompt text but the agent still ignores it, the issue may be a v1.9.x pass-through regression — earlier AnythingLLM versions wrapped the system prompt with framework instructions, and some edge cases in the pass-through logic may revert to wrapping behavior.

<!-- Related: agent ignores prompt, system prompt ignored, context compression, messageArrayCompressor, token budget exhaustion, front-loaded instructions, prompt trimming, U-shaped attention curve, attention position, middle section, training data override, PascalCase convention, snake_case naming, Memory conventions, citation patterns, source attribution, fresh conversation, context reset, write it down strategy, chat mode attention, agent mode attention, chatPrompt nullish coalescing, workspace settings, v1.9.x pass-through, default prompt, system prompt processing, behavioral rules, prompt architecture, temperature 0.1, reinforcement through correction, conversation history, context budget, HOW TO USE CONTEXT, bookend, prompt tail trimming, instruction following, tool output size -->
