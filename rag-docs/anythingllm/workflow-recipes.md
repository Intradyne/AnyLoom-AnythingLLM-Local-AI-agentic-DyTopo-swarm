# AnythingLLM Agent Workflow Recipes

This reference document provides self-contained workflow recipes for common multi-step tasks the AnythingLLM workspace agent performs in agent mode. Each recipe is a complete narrative walkthrough showing the tool sequence, decision points, and citation patterns. Formatted for dense-only RAG retrieval on port 6333 — each section embeds near the queries that would trigger it.


## How to research a topic using multiple tools and synthesize findings from different sources

<!-- Verified: 2026-02-14 -->

When a user asks a question that spans multiple information sources — for example "What's the current best practice for configuring Qdrant collections for hybrid search?" — the workspace agent follows the decision cascade from cheapest to most expensive tool, gathering evidence at each tier and synthesizing a comprehensive answer. This recipe applies to any research question where the answer may be distributed across workspace documents, the Memory knowledge graph, external library documentation, and web sources.

<<<<<<< HEAD
For questions about real-time data — commodity prices, exchange rates, sports scores, weather, current events — the agent calls Tavily directly (requires @agent) with a concise factual query such as "[asset] price today" and presents the answer with an inline source link following this format: "[Asset] is at [tool-returned price] ([Source name](tool-returned-url))." The bracketed values are placeholders — the agent fills them exclusively from the Tavily result, never from training data or prior examples. In chat mode without @agent, no tools are available — state that live data requires @agent mode instead of generating a response from training data. This completes in a single tool call with zero cascade in agent mode. Tavily returns structured, concise results optimized for factual queries, making it faster and more reliable than Fetch for search-based questions — Fetch works for known URLs but struggles with JavaScript-heavy financial sites that return massive truncated HTML instead of clean data.
=======
For questions about real-time data — commodity prices, exchange rates, sports scores, weather, current events — the agent calls Tavily directly with a concise factual query such as "silver spot price today" and presents the answer with an inline source link: "Silver is at $32.45/oz ([Kitco](https://www.kitco.com/silver-price-today-usa/))." This completes in a single tool call with zero cascade. Tavily returns structured, concise results optimized for factual queries, making it faster and more reliable than Fetch for search-based questions — Fetch works for known URLs but struggles with JavaScript-heavy financial sites that return massive truncated HTML instead of clean data.
>>>>>>> 9942e327ce1dc149abe142416c07aadc36c3deec

The full research cascade applies to broader questions. The workflow begins with the automatic context that AnythingLLM injects from workspace documents via the dense-only Qdrant instance on port 6333. Before making any tool calls, the agent examines the retrieved chunks in the Context: section of the system message. If workspace documents contain a direct answer — for example, a chunk describing the Qdrant collection schema with dense and sparse vector configuration — the agent cites it immediately: "Per the architecture reference: the lmstudio_docs collection uses named vectors 'dense' (1024-dim cosine) and 'sparse' (inverted index with learned lexical weights), with INT8 scalar quantization and HNSW indexing at m=16, ef_construct=200." If the workspace context fully answers the question, the agent delivers the answer with attribution and stops — no tool calls needed.

When workspace context provides a partial answer, the agent moves to the Memory knowledge graph via search_nodes. Memory often contains stable facts recorded from previous investigations: collection names, configuration decisions, resolved issues, and architecture choices. The agent searches for relevant entities — for example, searching for "Qdrant" or "collection" or "hybrid search" — and incorporates any matching observations: "Memory entity QdrantMCP confirms: hybrid search uses RRF fusion with a minimum score threshold of 0.005." The agent cites Memory findings with entity attribution so the user knows the source: "Per Memory graph: QdrantMCP entity records that source_dir payload filtering enables scoped searches across document sources."

For questions involving external library APIs where the answer may have changed between versions, the agent calls Context7 using the two-step resolve-library-id then get-library-docs sequence. For a Qdrant configuration question, the agent would resolve "qdrant-client" to its Context7 identifier and then fetch the current documentation for collection creation parameters, vector configuration options, and indexing settings. Context7 documentation is authoritative for API-specific details: "Per Context7 docs for qdrant-client: the QdrantClient.create_collection method accepts vectors_config as a dict of VectorParams with size, distance, and optional quantization_config." The agent notes the library version if Context7 provides it, helping the user understand which API surface is being referenced.

<<<<<<< HEAD
If Context7 does not cover the topic (some libraries lack Context7 entries, and Context7 returns a "library not found" response) or the question requires community knowledge beyond official documentation, the agent follows the appropriate fallback chain. For library and API documentation: Context7 first, then Tavily, then Fetch on the official docs URL. For general web research or real-time data: call Tavily first with a concise, specific query — "Qdrant hybrid search configuration best practices 2026" rather than a vague "how to use Qdrant." When Tavily returns promising results with URLs, call Fetch to retrieve the full page content for deeper analysis. The agent includes URLs inline as markdown links, using the actual URL returned by the tool: "[Source title](tool-returned-url)." For example, if Tavily returns a Qdrant documentation URL, the agent links to it directly in the response. When Fetch returns truncated content that already contains the answer, the agent delivers the answer immediately — requesting more content wastes iterations. The built-in Web Scraper is an alternative to Fetch for URL content retrieval, though Fetch returns cleaner markdown.
=======
If Context7 does not cover the topic (some libraries lack Context7 entries, and Context7 returns a "library not found" response) or the question requires community knowledge beyond official documentation, the agent follows the appropriate fallback chain. For library and API documentation: Context7 first, then Tavily, then Fetch on the official docs URL. For general web research or real-time data: call Tavily first with a concise, specific query — "Qdrant hybrid search configuration best practices 2026" rather than a vague "how to use Qdrant." When Tavily returns promising results with URLs, call Fetch to retrieve the full page content for deeper analysis. The agent includes URLs inline as markdown links: "According to [Qdrant's hybrid search tutorial](https://qdrant.tech/documentation/tutorials/hybrid-search/), the recommended approach uses named vectors with separate dense and sparse configurations." When Fetch returns truncated content that already contains the answer, the agent delivers the answer immediately — requesting more content wastes iterations. The built-in Web Scraper is an alternative to Fetch for URL content retrieval, though Fetch returns cleaner markdown.
>>>>>>> 9942e327ce1dc149abe142416c07aadc36c3deec

Throughout the research process, the agent cites each source as findings accumulate and synthesizes the final answer by weaving together evidence from all tiers: "Based on the workspace architecture reference, Memory graph records, and current Qdrant documentation via Context7, the recommended configuration for hybrid search collections is..." The agent captures all key findings in its response text rather than relying on raw tool output to persist, because the message array compressor may truncate older tool results during extended sessions. If any tier produces contradictory information, the agent flags the discrepancy explicitly and states which source it considers most authoritative — workspace documents and Memory for stack-specific configuration, Context7 for current API syntax, web results for community practices.

<!-- Related: research workflow, multi-source synthesis, decision cascade, workspace context, automatic RAG, Memory graph, search_nodes, Context7, resolve-library-id, get-library-docs, Tavily web search, Tavily first choice, real-time data, commodity prices, exchange rates, live data, Fetch URL, Fetch truncation, truncated response, Web Scraper, citation patterns, tool attribution, source synthesis, partial answers, coverage gaps, information gathering, library documentation, community knowledge, markdown links, message array compressor, context preservation, Qdrant configuration, hybrid search, collection schema, evidence tiers, authoritative sources, JavaScript-heavy sites, financial data -->


## How to debug a failing service or tool by checking system state and tracing through logs

<!-- Verified: 2026-02-14 -->

When a user reports that something is broken — "my RAG search isn't returning results" or "the agent can't connect to port 6334" or "LM Studio seems stuck" — the workspace agent follows a systematic diagnostic workflow that starts with verifying the current system state before attempting any fixes. This recipe applies to any infrastructure debugging scenario where the agent needs to check running services, inspect container health, read configuration, and trace through logs to identify the root cause.

<<<<<<< HEAD
The diagnostic sequence follows the inspect-modify-verify-log pattern that Desktop Commander workflows use in practice. First, the agent inspects the broadest system state: Desktop Commander runs docker ps to see which containers are running, checking for the two Qdrant containers (one mapping host port 6333 for AnythingLLM, one mapping host port 6334 for the MCP server) and confirming they show "Up" status with correct port mappings. The agent reports what it finds with tool attribution: "Desktop Commander confirms: docker ps shows both Qdrant containers running — anythingllm-qdrant on port 6333 (Up [uptime]) and lmstudio-qdrant on port 6334 (Up [uptime])." (Example format — fill [uptime] from actual docker ps output.) If a container is missing from docker ps, the agent checks docker ps -a to find containers that exist but are stopped, and reports the exit status. If a container needs restarting (the modify phase), the agent runs docker start followed immediately by another docker ps to verify the change took effect (the verify phase) — confirming the container is now up with correct ports before reporting success. A stopped container is often caused by a host reboot (both containers use the always-restart policy but may not restart if Docker Desktop itself hasn't started), a port conflict (another process has claimed port 6333 or 6334), or disk space exhaustion on the Docker volume.
=======
The diagnostic sequence follows the inspect-modify-verify-log pattern that Desktop Commander workflows use in practice. First, the agent inspects the broadest system state: Desktop Commander runs docker ps to see which containers are running, checking for the two Qdrant containers (one mapping host port 6333 for AnythingLLM, one mapping host port 6334 for the MCP server) and confirming they show "Up" status with correct port mappings. The agent reports what it finds with tool attribution: "Desktop Commander confirms: docker ps shows both Qdrant containers running — anythingllm-qdrant on port 6333 (Up 3 days) and lmstudio-qdrant on port 6334 (Up 3 days)." If a container is missing from docker ps, the agent checks docker ps -a to find containers that exist but are stopped, and reports the exit status. If a container needs restarting (the modify phase), the agent runs docker start followed immediately by another docker ps to verify the change took effect (the verify phase) — confirming the container is now up with correct ports before reporting success. A stopped container is often caused by a host reboot (both containers use the always-restart policy but may not restart if Docker Desktop itself hasn't started), a port conflict (another process has claimed port 6333 or 6334), or disk space exhaustion on the Docker volume.
>>>>>>> 9942e327ce1dc149abe142416c07aadc36c3deec

For port-specific issues, the agent uses Desktop Commander to check whether the expected process is actually listening. On Windows, the command netstat -ano | findstr 6333 reveals whether the port is in use and which process holds it. If the port is occupied by an unexpected process, the agent reports the process ID so the user can investigate. If the port is free but the container claims to be running, the container may have an internal error — docker logs followed by the container name reveals the container's console output, which often contains Qdrant-specific error messages about storage corruption, configuration issues, or resource exhaustion.

When the containers are running but a specific tool or service is not behaving correctly, the agent narrows the investigation. For RAG search returning empty results: the agent suggests checking document freshness through the LM Studio agent's rag_status tool (which the workspace agent cannot call directly, but can suggest the user invoke from LM Studio), or checking whether the target document is properly embedded by examining the AnythingLLM workspace's document list in the web interface at localhost port 3001. The agent uses Filesystem tools to read relevant configuration files — for example, reading the mcp.json file to verify that the QDRANT_URL environment variable points to the correct port, or checking that document source paths are valid. The agent cites each file it reads: "Filesystem shows: mcp.json contains QDRANT_URL=http://localhost:6334, confirming the MCP server is configured for the correct Qdrant instance."

<<<<<<< HEAD
For LM Studio performance issues — the model seems unresponsive, responses are extremely slow, or inference appears stuck — the agent checks VRAM utilization by running nvidia-smi via Desktop Commander. Usage above 31 GB of the RTX 5090's 32 GB indicates VRAM pressure, which occurs when the KV cache grows large at high context lengths. The agent reports the current utilization: "Desktop Commander confirms: nvidia-smi shows [current VRAM usage] VRAM in use — [within/above] normal range for 80K context with Q8_0 KV cache." (Example format — fill values from actual nvidia-smi output.) If VRAM is near capacity, the agent suggests starting a fresh conversation (which resets the KV cache), reducing the context length in LM Studio settings, or checking whether a DyTopo swarm is actively running and consuming inference slots. The agent may also check CPU usage and system memory to rule out resource contention from the BGE-M3 FlagEmbedding model (approximately 2.3 GB RAM) or MiniLM-L6-v2 (approximately 80 MB RAM) running on CPU.
=======
For LM Studio performance issues — the model seems unresponsive, responses are extremely slow, or inference appears stuck — the agent checks VRAM utilization by running nvidia-smi via Desktop Commander. Usage above 31 GB of the RTX 5090's 32 GB indicates VRAM pressure, which occurs when the KV cache grows large at high context lengths. The agent reports the current utilization: "Desktop Commander confirms: nvidia-smi shows 30.8 GB VRAM in use — within normal range for 80K context with Q8_0 KV cache." If VRAM is near capacity, the agent suggests starting a fresh conversation (which resets the KV cache), reducing the context length in LM Studio settings, or checking whether a DyTopo swarm is actively running and consuming inference slots. The agent may also check CPU usage and system memory to rule out resource contention from the BGE-M3 FlagEmbedding model (approximately 2.3 GB RAM) or MiniLM-L6-v2 (approximately 80 MB RAM) running on CPU.
>>>>>>> 9942e327ce1dc149abe142416c07aadc36c3deec

Throughout the debugging workflow, the agent uses Sequential Thinking when the diagnostic requires more than three steps or when intermediate results change the investigation direction. The agent states its diagnostic plan before executing ("I'll check container status first, then verify port availability, then inspect the relevant configuration"), cites each finding as it accumulates evidence, and synthesizes a root cause conclusion with recommended remediation. If the root cause is unclear after exhausting the diagnostic chain, the agent states which sources were checked, what was found, and what remains unresolved — giving the user a clear picture of the investigation state rather than a vague "I couldn't figure it out."

<!-- Related: debugging workflow, service diagnostics, failing tool, connection refused, port check, Docker containers, docker ps, docker logs, container health, Desktop Commander, netstat, port conflict, disk space, VRAM pressure, nvidia-smi, KV cache, LM Studio performance, inference latency, Filesystem, mcp.json, configuration verification, Qdrant containers, port 6333, port 6334, port 1234, system state, root cause analysis, Sequential Thinking, diagnostic sequence, container restart, exit status, always-restart policy, BGE-M3, MiniLM, CPU memory, VRAM monitoring, tool attribution, evidence chain, remediation steps -->


## How to store and retrieve structured knowledge in the shared Memory knowledge graph

<!-- Verified: 2026-02-14 -->

The Memory knowledge graph is the shared persistent data store that both the AnythingLLM workspace agent and the LM Studio agent read and write through the Docker MCP Gateway's Memory server. This recipe covers the complete lifecycle of working with Memory: searching for existing knowledge, creating new entities with proper naming conventions, appending facts to existing entities, creating relationships between entities, and maintaining the search-before-create discipline that keeps the graph clean and searchable for both agents.

When the agent discovers a fact worth preserving — a port mapping, a resolved error pattern, a configuration decision, a user preference, or an architecture choice — the first step is always search_nodes to check whether a relevant entity already exists. The agent searches using the likely entity name and common variations: for example, when recording a fact about Qdrant configuration, the agent searches for "Qdrant," "QdrantMCP," "QdrantConfig," and "Qdrant6334" to catch any existing entity regardless of the original creator's exact naming choice. The search-before-create discipline is the most important operational rule for the Memory graph — creating duplicate entities (such as "QdrantConfig" and "QdrantConfiguration" and "QdrantMCPSettings") fragments the knowledge, dilutes search results, and creates confusion about which version contains the most current information. Both agents follow this discipline, so a fact recorded by the LM Studio agent through search_nodes and create_entities will be found by the workspace agent's search_nodes call.

If search_nodes returns an existing entity that matches the topic, the agent uses add_observations to append new facts to it rather than creating a new entity. Observations are individual fact statements attached to an entity: "Port 6334 serves hybrid dense+sparse search with RRF fusion" or "FlagEmbedding runs on CPU with 8 dedicated threads" or "Collection rebuilt on 2026-02-13 after schema change." Each observation should be a self-contained fact that makes sense without reading the other observations on the same entity. The agent cites the Memory operation in its response: "Added observation to Memory entity QdrantMCP: 'Source_dir filtering enables scoped search across lmstudio and anythingllm document sources.'"

When no matching entity exists, the agent creates one using create_entities with the standardized naming conventions that both agents enforce. Entity names use PascalCase: QdrantMCP, BGEm3Config, AnythingLLMWorkspace, RTX5090Hardware, DyTopoSwarm, LMStudioSettings, PortTopology. Entity types use snake_case and describe the category: service_config, architecture_decision, project_preference, error_resolution, port_mapping, hardware_spec, tool_config, deployment_note. The initial observations should capture the core facts about the entity — typically 3 to 5 observations covering the essential attributes. The agent reports the creation: "Created Memory entity 'QdrantMCP' (type: service_config) with observations: port 6334, hybrid search with RRF, BGE-M3 on CPU, lmstudio_docs collection, source_dir filtering."

Relationships between entities are created using create_relations with snake_case relation names that describe the connection: QdrantMCP depends_on BGEm3Config, AnythingLLMWorkspace connects_to QdrantPort6333, DyTopoSwarm uses MiniLMRouting, LMStudioSettings serves QdrantMCP. Relation names should be directional and descriptive — "depends_on" and "serves" are more informative than generic "relates_to." The agent creates relations when the connection between entities is stable and useful for future retrieval: knowing that DyTopo depends on MiniLM helps an agent quickly locate the routing model's configuration when debugging swarm issues.

The types of information appropriate for Memory storage include stable infrastructure facts (port 1234 serves LM Studio API, port 6333 serves AnythingLLM Qdrant, port 6334 serves MCP Qdrant), configuration values that rarely change (collection names, thread counts, quantization settings), user preferences (formatting choices, tool preferences, project conventions), architecture decisions with their rationale (why hybrid search for port 6334 but dense-only for port 6333), resolved error patterns with their solutions (so the same error can be quickly diagnosed if it recurs), and useful reference URLs for documentation or tools. Information that should stay out of Memory includes transient conversation context (the compressor and conversation history handle this), speculative or unverified information (only store confirmed facts), secrets and passwords (security risk), and entire file contents (store the file path instead — paths consume fewer tokens and remain valid when file contents change). When recording a resolved error, the agent stores the symptom, the root cause, and the fix as separate observations on a single entity — this makes the error findable by either the symptom description or the solution approach.

<!-- Related: Memory knowledge graph, shared data store, search_nodes, create_entities, add_observations, create_relations, open_nodes, search-before-create discipline, PascalCase entity names, snake_case entity types, snake_case relation names, entity fragmentation, duplicate prevention, cross-agent continuity, LM Studio agent, AnythingLLM agent, Docker MCP Gateway, Memory server, port mappings, configuration decisions, error resolution, user preferences, architecture choices, stable facts, entity lifecycle, observation format, relation direction, naming conventions, graph consistency, fact preservation, Memory discipline, QdrantMCP, BGEm3Config, DyTopoSwarm -->


## How to plan a complex task with Sequential Thinking before executing with other tools

<!-- Verified: 2026-02-14 -->

When the workspace agent faces a task that requires multiple steps with dependencies between them — where the output of one step determines what the next step should be — using the Sequential Thinking tool to plan before acting produces better results than jumping directly into tool calls. This recipe demonstrates the hybrid pattern of interleaving structured reasoning with concrete tool execution, which is the primary workflow for complex diagnostic, analytical, and planning tasks.

The trigger for using Sequential Thinking is task complexity: the agent recognizes that the question requires three or more tool calls where each call depends on the previous result, or the task involves comparing multiple approaches where structured evaluation prevents overlooking important trade-offs, or the debugging scenario has an ambiguous root cause that requires testing multiple hypotheses systematically. Simple tasks — single-tool lookups, status checks, straightforward file reads, direct factual questions — skip Sequential Thinking entirely and call the appropriate tool directly.

The most common Sequential Thinking multi-tool chain is the three-pass pattern with Memory. When a user asks an analytical question like "What architecture decisions have we made about the embedding pipeline?", the agent executes three passes: first, Sequential Thinking plans the approach by decomposing the question into sub-parts (what decisions exist, what tradeoffs were considered, how they relate) and designing a search strategy; second, Memory search_nodes retrieves existing entities and observations matching the plan; third, the agent evaluates Memory results and decides whether to synthesize a response directly or enrich the graph first by calling add_observations to record the new synthesis for future queries. This three-pass pattern — plan with Sequential Thinking, retrieve with Memory, then evaluate and act — works because Sequential Thinking output stays in the conversation context as an internal reasoning scratchpad that guides subsequent tool calls without being shown to the user.

A diagnostic example illustrates the broader interleaving pattern. The user asks: "My AnythingLLM workspace RAG seems to be returning irrelevant results — can you help figure out why?" The agent recognizes this as a multi-step diagnostic with several possible causes (embedding quality, chunk size mismatch, similarity threshold too permissive, stale documents, incorrect Qdrant configuration) and opens with a Sequential Thinking call to plan the investigation. Thought 1 outlines the diagnostic approach: check the Qdrant container health, verify the embedding model is loaded correctly, examine the chunk configuration, and test whether the issue is query-specific or systemic — estimating 5 total thoughts. The agent then executes the first action: Desktop Commander to run docker ps and verify that the anythingllm-qdrant container on port 6333 is running. Thought 2 analyzes the result ("Container is running, Up 4 days, port mapping correct — the issue is not container availability, so the cause is likely in retrieval configuration or document quality") and decides the next check: examining the workspace's document list to see if recently added documents might have unusual formatting.

The interleaving continues: Thought 3 analyzes what was found and refines the hypothesis, an action tool gathers the next piece of evidence, Thought 4 evaluates whether the accumulated evidence points to a root cause, and so on. Each thinking step explicitly references what the previous tool call revealed, creating a visible reasoning chain. The agent cites both the reasoning and the tool evidence in its response text: "Sequential Thinking analysis: the container is healthy (confirmed via Desktop Commander), documents are properly chunked (verified via Filesystem read of workspace settings), so the issue is likely the low similarity threshold admitting too many marginally relevant chunks. Desktop Commander confirms: AnythingLLM workspace settings show similarity threshold set to 'Low,' which admits maximum candidate chunks."

The planning variant uses Sequential Thinking without interleaving — all thinking steps occur before any action. This pattern applies when the agent needs to construct a complete plan before executing it, such as planning a multi-file configuration change where ordering matters, or designing a migration sequence where one step's failure should prevent subsequent steps. The agent uses Sequential Thinking to define each step, identify dependencies, anticipate failure points, and produce an ordered execution plan. The agent then states the plan briefly and executes all steps, reporting findings at the end rather than pausing between steps.

The agent manages Sequential Thinking's token cost by keeping individual thoughts concise (100 to 300 tokens each), limiting sequences to 3 to 5 thoughts for focused diagnostics and 5 to 8 for complex multi-factor analyses, and reserving the revision and branching features for situations where earlier reasoning is genuinely contradicted by new evidence rather than using them routinely. At 80,000 tokens with approximately 55,000 available for the current exchange, a typical 5-step sequence with interleaved tool calls adds roughly 2,000 to 4,000 tokens total (thinking steps plus tool call overhead), which is well within budget. The agent avoids using Sequential Thinking for tasks that Qwen3's internal /think mode can handle directly — when extended reasoning is needed but no tool interleaving is required, /think provides internal chain-of-thought reasoning at zero visible token cost.

<!-- Related: Sequential Thinking, sequentialthinking tool, structured reasoning, planning before executing, hybrid ReAct pattern, interleaved reasoning, diagnostic workflow, multi-step tasks, hypothesis testing, root cause analysis, tool interleaving, Desktop Commander, Filesystem, Memory, thinking then acting, thought sequence, visible reasoning chain, diagnostic approach, planning variant, migration planning, execution plan, token cost, thought conciseness, revision, branching, /think mode, Qwen3 reasoning, complex diagnostics, evidence chain, reasoning patterns, task complexity trigger, dependency management, multi-factor analysis -->


## How to escalate to DyTopo multi-agent swarms for multi-perspective analysis and when the workspace agent should suggest it

<!-- Verified: 2026-02-14 -->

DyTopo multi-agent swarms — the Dynamic Topology orchestration system based on arXiv 2602.06039 — are the Loom stack's most powerful reasoning tool, launching teams of specialized AI agents that collaborate through semantically-routed message passing across multiple inference rounds. The swarm tools (swarm_start, swarm_status, swarm_result) are available exclusively through the LM Studio interface and cannot be called from the AnythingLLM workspace agent directly. This recipe covers when the workspace agent should recognize that a task would benefit from swarm escalation, how to recommend it to the user, and what the user should expect when launching a swarm from LM Studio.

The workspace agent should suggest DyTopo escalation when a task meets two criteria: the task genuinely benefits from multiple specialist perspectives working collaboratively rather than a single agent working sequentially, and the task is complex enough to justify the overhead of multi-round inference. Strong candidates for swarm escalation include code review and architecture analysis where a Developer, Researcher, Tester, and Designer each evaluate different quality dimensions (the code domain); mathematical proofs or multi-step calculations where independent solution and verification improve confidence (the math domain); and open-ended analysis requiring balanced evaluation of competing arguments where an Analyst, Critic, and Synthesizer produce richer conclusions than a single model reasoning alone (the general domain).

The workspace agent formulates the escalation suggestion with specific guidance for the user: "This task would benefit from DyTopo's code domain swarm — the Developer, Researcher, Tester, and Designer agents would each evaluate your architecture proposal from their specialty. To launch from LM Studio: swarm_start(task='Review the proposed Qdrant collection migration for correctness, test coverage, and architectural quality', domain='code', tau=0.3). Default parameters (tau=0.3, k_in=3, max_rounds=5) work well for most tasks." The agent explains what the swarm will do and approximately how long it will take — typically 3 to 5 minutes depending on the number of rounds and the complexity of each agent's work output.

The workspace agent also explains the practical implications of swarm execution for the user's current session. All DyTopo swarm agent calls route through the same LM Studio inference endpoint at localhost port 1234, sharing the Qwen3-30B-A3B model and KV cache with both frontends. While a swarm is actively running, inference requests from both the LM Studio agent and the AnythingLLM workspace agent queue behind swarm agent calls. The user should expect increased response latency for interactive chat during active swarm execution — responses that normally complete in 2 to 5 seconds may take 10 to 60 seconds while the swarm's round of agent calls processes through the queue. The user can check swarm progress from LM Studio using swarm_status(task_id), which reports the current round, active agent, elapsed time, and overall status (running, completed, or error).

The three domain selections map to different collaboration patterns. The code domain (Manager, Developer, Researcher, Tester, Designer) handles programming tasks where each agent evaluates from a different specialty — the Tester catches edge cases the Developer missed, the Designer identifies structural issues the Researcher overlooked, producing thorough multi-perspective reviews. The math domain (Manager, ProblemParser, Solver, Verifier) handles mathematical reasoning through adversarial verification — the Solver works toward a solution while the Verifier independently checks correctness. The general domain (Manager, Analyst, Critic, Synthesizer) handles open-ended analysis where the Critic challenges the Analyst's findings and the Synthesizer integrates both into a balanced conclusion.

The tau parameter controls how densely agents communicate after the first round (round 1 always uses broadcast where all agents see all outputs). Lower tau values (0.1 to 0.2) create richer inter-agent communication at higher token cost, which is valuable when agents need to build on each other's work closely — detailed code review where the Tester requires the Developer's implementation to write relevant test cases. Higher tau values (0.4 to 0.6) create sparser routing with faster convergence, useful when agents work more independently — a general domain analysis where the Analyst and Critic benefit from independence before the Synthesizer merges their views. The workspace agent recommends default tau=0.3 for most tasks, noting that the user can tune it if the first run produces either too much cross-talk (increase tau) or too little collaboration (decrease tau).

Tasks the workspace agent should handle locally rather than suggesting swarm escalation include single-file debugging (use Desktop Commander and Filesystem directly), factual lookups (use Memory search_nodes or let workspace RAG answer), configuration verification (use Desktop Commander for system state, Filesystem for file contents), simple planning (use Sequential Thinking for the scratchpad), and any task where the overhead of multi-round swarm inference is not justified by the improved quality. The workspace agent handles simple, direct tasks efficiently through its own tool chain — swarms are for collaborative complexity where multiple perspectives produce measurably better results.

<!-- Related: DyTopo, Dynamic Topology, multi-agent swarms, swarm_start, swarm_status, swarm_result, swarm escalation, LM Studio exclusive, code domain, math domain, general domain, Developer, Researcher, Tester, Designer, ProblemParser, Solver, Verifier, Analyst, Critic, Synthesizer, Manager agent, tau threshold, routing threshold, k_in, max_rounds, inference queuing, response latency, port 1234, multi-perspective analysis, collaborative reasoning, specialist agents, code review, architecture analysis, mathematical proofs, balanced evaluation, arXiv 2602.06039, MiniLM-L6-v2, semantic routing, broadcast mode, when to use swarms, when to handle locally, overhead assessment -->
